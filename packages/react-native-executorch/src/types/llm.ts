import { RnExecutorchError } from '../errors/errorUtils';

/**
 * React hook for managing a Large Language Model (LLM) instance.
 */
export interface LLMType {
  /**
   * History containing all messages in conversation. This field is updated after model responds to sendMessage.
   */
  messageHistory: Message[];

  /**
   * State of the generated response. This field is updated with each token generated by the model.
   */
  response: string;

  /**
   * The most recently generated token.
   */
  token: string;

  /**
   * Indicates whether the model is ready.
   */
  isReady: boolean;

  /**
   * Indicates whether the model is currently generating a response.
   */
  isGenerating: boolean;

  /**
   * Represents the download progress as a value between 0 and 1, indicating the extent of the model file retrieval.
   */
  downloadProgress: number;

  /**
   * Contains the error message if the model failed to load.
   */
  error: RnExecutorchError | null;

  /**
   * Configures chat and tool calling.
   * See [Configuring the model](../../03-hooks/01-natural-language-processing/useLLM.md#configuring-the-model) for details.
   *
   * @param configuration - Configuration object containing `chatConfig`, `toolsConfig`, and `generationConfig`.
   */
  configure: ({
    chatConfig,
    toolsConfig,
    generationConfig,
  }: {
    /**
     * Object configuring chat management, contains following properties:
     *
     * `systemPrompt` - Often used to tell the model what is its purpose, for example - "Be a helpful translator".
     *
     * `initialMessageHistory` - An array of `Message` objects that represent the conversation history. This can be used to provide initial context to the model.
     *
     * `contextWindowLength` - The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage.
     */
    chatConfig?: Partial<ChatConfig>;

    /**
     * Object configuring options for enabling and managing tool use. **It will only have effect if your model's chat template support it**. Contains following properties:
     *
     * `tools` - List of objects defining tools.
     *
     * `executeToolCallback` - Function that accepts `ToolCall`, executes tool and returns the string to model.
     *
     * `displayToolCalls` - If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed.
     */
    toolsConfig?: ToolsConfig;

    /**
     * Object configuring generation settings.
     *
     * `outputTokenBatchSize` - Soft upper limit on the number of tokens in each token batch (in certain cases there can be more tokens in given batch, i.e. when the batch would end with special emoji join character).
     *
     * `batchTimeInterval` - Upper limit on the time interval between consecutive token batches.
     *
     * `temperature` - Scales output logits by the inverse of temperature. Controls the randomness / creativity of text generation.
     *
     * `topp` - Only samples from the smallest set of tokens whose cumulative probability exceeds topp.
     */
    generationConfig?: GenerationConfig;
  }) => void;

  /**
   * Returns the number of tokens generated so far in the current generation.
   * @returns The count of generated tokens.
   */
  getGeneratedTokenCount: () => number;

  /**
   * Runs model to complete chat passed in `messages` argument. It doesn't manage conversation context.
   * @param messages - Array of messages representing the chat history.
   * @param tools - Optional array of tools that can be used during generation.
   */
  generate: (messages: Message[], tools?: LLMTool[]) => Promise<void>;

  /**
   * Function to add user message to conversation.
   * After model responds, `messageHistory` will be updated with both user message and model response.
   * @param message - The message string to send.
   */
  sendMessage: (message: string) => Promise<void>;

  /**
   * Deletes all messages starting with message on `index` position. After deletion `messageHistory` will be updated.
   * @param index - The index of the message to delete from history.
   * @returns
   */
  deleteMessage: (index: number) => void;

  /**
   * Function to interrupt the current inference.
   */
  interrupt: () => void;
}

/**
 * Roles that a message sender can have.
 */
export type MessageRole = 'user' | 'assistant' | 'system';

/**
 * Represents a message in the conversation.
 */
export interface Message {
  /**
   * Role of the message sender of type `MessageRole`.
   */
  role: MessageRole;

  /**
   * Content of the message.
   */
  content: string;
}

export interface ToolCall {
  toolName: string;
  arguments: Object;
}

// usually tool is represented with dictionary (Object), but fields depend on the model
// unfortunately there's no one standard so it's hard to type it better
export type LLMTool = Object;

/**
 * Object configuring chat management.
 */
export interface ChatConfig {
  /**
   * An array of `Message` objects that represent the conversation history. This can be used to provide initial context to the model.
   */
  initialMessageHistory: Message[];

  /**
   * The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage.
   */
  contextWindowLength: number;

  /**
   * Often used to tell the model what is its purpose, for example - "Be a helpful translator".
   */
  systemPrompt: string;
}

/**
 * Object configuring options for enabling and managing tool use. **It will only have effect if your model's chat template support it**.
 */
export interface ToolsConfig {
  /**
   * List of objects defining tools.
   */
  tools: LLMTool[];

  /**
   * Function that accepts `ToolCall`, executes tool and returns the string to model.
   */
  executeToolCallback: (call: ToolCall) => Promise<string | null>;

  /**
   * If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed.
   */
  displayToolCalls?: boolean;
}

/**
 * Object configuring generation settings.
 */
export interface GenerationConfig {
  /**
   * Scales output logits by the inverse of temperature. Controls the randomness / creativity of text generation.
   */
  temperature?: number;

  /**
   * Only samples from the smallest set of tokens whose cumulative probability exceeds topp.
   */
  topp?: number;

  /**
   * Soft upper limit on the number of tokens in each token batch (in certain cases there can be more tokens in given batch, i.e. when the batch would end with special emoji join character).
   */
  outputTokenBatchSize?: number;

  /**
   * Upper limit on the time interval between consecutive token batches.
   */
  batchTimeInterval?: number;
}

export const SPECIAL_TOKENS = {
  BOS_TOKEN: 'bos_token',
  EOS_TOKEN: 'eos_token',
  UNK_TOKEN: 'unk_token',
  SEP_TOKEN: 'sep_token',
  PAD_TOKEN: 'pad_token',
  CLS_TOKEN: 'cls_token',
  MASK_TOKEN: 'mask_token',
};

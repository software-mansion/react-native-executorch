import { RnExecutorchError } from '../errors/errorUtils';
import { ResourceSource } from './common';

/**
 * Properties for initializing and configuring a Large Language Model (LLM) instance.
 *
 * @category Types
 */
export interface LLMProps {
  model: {
    /**
     * `ResourceSource` that specifies the location of the model binary.
     */
    modelSource: ResourceSource;
    /**
     * `ResourceSource` pointing to the JSON file which contains the tokenizer.
     */
    tokenizerSource: ResourceSource;
    /**
     * `ResourceSource` pointing to the JSON file which contains the tokenizer config.
     */
    tokenizerConfigSource?: ResourceSource;
  };
  /**
   * Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook.
   */
  preventLoad?: boolean;
}

/**
 * React hook for managing a Large Language Model (LLM) instance.
 *
 * @category Types
 */
export interface LLMType {
  /**
   * History containing all messages in conversation. This field is updated after model responds to sendMessage.
   */
  messageHistory: Message[];

  /**
   * State of the generated response. This field is updated with each token generated by the model.
   */
  response: string;

  /**
   * The most recently generated token.
   */
  token: string;

  /**
   * Indicates whether the model is ready.
   */
  isReady: boolean;

  /**
   * Indicates whether the model is currently generating a response.
   */
  isGenerating: boolean;

  /**
   * Represents the download progress as a value between 0 and 1, indicating the extent of the model file retrieval.
   */
  downloadProgress: number;

  /**
   * Contains the error message if the model failed to load.
   */
  error: RnExecutorchError | null;

  /**
   * Configures chat and tool calling.
   * See [Configuring the model](https://docs.swmansion.com/react-native-executorch/docs/hooks/natural-language-processing/useLLM#configuring-the-model) for details.
   *
   * @param {LLMConfig} configuration - Configuration object containing `chatConfig`, `toolsConfig`, and `generationConfig`.
   */
  configure: ({ chatConfig, toolsConfig, generationConfig }: LLMConfig) => void;

  /**
   * Returns the number of tokens generated so far in the current generation.
   *
   * @returns The count of generated tokens.
   */
  getGeneratedTokenCount: () => number;
  /**
   * Runs model to complete chat passed in `messages` argument. It doesn't manage conversation context.
   *
   * @param messages - Array of messages representing the chat history.
   * @param tools - Optional array of tools that can be used during generation.
   * @returns The generated tokens as `string`.
   */
  generate: (messages: Message[], tools?: LLMTool[]) => Promise<string>;
  /**
   * Returns the number of total tokens from the previous generation.This is a sum of prompt tokens and generated tokens.
   *
   * @returns The count of prompt and generated tokens.
   */
  getTotalTokenCount: () => number;
  /**
   * Returns the number of prompt tokens in the last message.
   *
   * @returns The count of prompt token.
   */
  getPromptTokenCount: () => number;

  /**
   * Function to add user message to conversation.
   * After model responds, `messageHistory` will be updated with both user message and model response.
   *
   * @param message - The message string to send.
   * @returns The model's response as a `string`.
   */
  sendMessage: (message: string) => Promise<string>;

  /**
   * Deletes all messages starting with message on `index` position. After deletion `messageHistory` will be updated.
   *
   * @param index - The index of the message to delete from history.
   */
  deleteMessage: (index: number) => void;

  /**
   * Function to interrupt the current inference.
   */
  interrupt: () => void;
}

/**
 * Configuration object for initializing and customizing a Large Language Model (LLM) instance.
 *
 * @category Types
 */
export interface LLMConfig {
  /**
   * Object configuring chat management, contains following properties:
   *
   * `systemPrompt` - Often used to tell the model what is its purpose, for example - "Be a helpful translator".
   *
   * `initialMessageHistory` - An array of `Message` objects that represent the conversation history. This can be used to provide initial context to the model.
   *
   * `contextWindowLength` - The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage.
   */
  chatConfig?: Partial<ChatConfig>;

  /**
   * Object configuring options for enabling and managing tool use. **It will only have effect if your model's chat template support it**. Contains following properties:
   *
   * `tools` - List of objects defining tools.
   *
   * `executeToolCallback` - Function that accepts `ToolCall`, executes tool and returns the string to model.
   *
   * `displayToolCalls` - If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed.
   */
  toolsConfig?: ToolsConfig;

  /**
   * Object configuring generation settings.
   *
   * `outputTokenBatchSize` - Soft upper limit on the number of tokens in each token batch (in certain cases there can be more tokens in given batch, i.e. when the batch would end with special emoji join character).
   *
   * `batchTimeInterval` - Upper limit on the time interval between consecutive token batches.
   *
   * `temperature` - Scales output logits by the inverse of temperature. Controls the randomness / creativity of text generation.
   *
   * `topp` - Only samples from the smallest set of tokens whose cumulative probability exceeds topp.
   */
  generationConfig?: GenerationConfig;
}

/**
 * Roles that a message sender can have.
 *
 * @category Types
 */
export type MessageRole = 'user' | 'assistant' | 'system';

/**
 * Represents a message in the conversation.
 *
 * @category Types
 * @property {MessageRole} role - Role of the message sender of type `MessageRole`.
 * @property {string} content - Content of the message.
 */
export interface Message {
  role: MessageRole;
  content: string;
}

/**
 * Represents a tool call made by the model.
 *
 * @category Types
 * @property {string} toolName - The name of the tool being called.
 * @property {Object} arguments - The arguments passed to the tool.
 */
export interface ToolCall {
  toolName: string;
  arguments: Object;
}

/**
 * Represents a tool that can be used by the model.
 * Usually tool is represented with dictionary (Object), but fields depend on the model.
 * Unfortunately there's no one standard so it's hard to type it better.
 *
 * @category Types
 */
export type LLMTool = Object;

/**
 * Object configuring chat management.
 *
 * @category Types
 * @property {Message[]} initialMessageHistory - An array of `Message` objects that represent the conversation history. This can be used to provide initial context to the model.
 * @property {string} systemPrompt - Often used to tell the model what is its purpose, for example - "Be a helpful translator".
 * @property {ContextStrategy} contextStrategy - Defines a strategy for managing the conversation context window and message history.
 */
export interface ChatConfig {
  initialMessageHistory: Message[];
  systemPrompt: string;
  contextStrategy: ContextStrategy;
}

/**
 * Object configuring options for enabling and managing tool use. **It will only have effect if your model's chat template support it**.
 *
 * @category Types
 * @property {LLMTool[]} tools - List of objects defining tools.
 * @property {(call: ToolCall) => Promise<string | null>} executeToolCallback - Function that accepts `ToolCall`, executes tool and returns the string to model.
 * @property {boolean} [displayToolCalls] - If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed.
 */
export interface ToolsConfig {
  tools: LLMTool[];
  executeToolCallback: (call: ToolCall) => Promise<string | null>;
  displayToolCalls?: boolean;
}

/**
 * Object configuring generation settings.
 *
 * @category Types
 * @property {number} [temperature] - Scales output logits by the inverse of temperature. Controls the randomness / creativity of text generation.
 * @property {number} [topp] - Only samples from the smallest set of tokens whose cumulative probability exceeds topp.
 * @property {number} [outputTokenBatchSize] - Soft upper limit on the number of tokens in each token batch (in certain cases there can be more tokens in given batch, i.e. when the batch would end with special emoji join character).
 * @property {number} [batchTimeInterval] - Upper limit on the time interval between consecutive token batches.
 */
export interface GenerationConfig {
  temperature?: number;
  topp?: number;
  outputTokenBatchSize?: number;
  batchTimeInterval?: number;
}

/**
 * Defines a strategy for managing the conversation context window and message history.
 *
 * @category Types
 */
export interface ContextStrategy {
  /**
   * Constructs the final array of messages to be sent to the model for the current inference step.
   * * @param systemPrompt - The top-level instructions or persona assigned to the model.
   * @param history - The complete conversation history up to the current point.
   * @param getTokenCount - A callback function provided by the LLM controller that calculates the exact number of tokens a specific array of messages will consume once formatted.
   * @returns The optimized array of messages, ready to be processed by the model.
   */
  buildContext(
    systemPrompt: string,
    history: Message[],
    getTokenCount: (messages: Message[]) => number
  ): Message[];
}

/**
 * Special tokens used in Large Language Models (LLMs).
 *
 * @category Types
 */
export const SPECIAL_TOKENS = {
  BOS_TOKEN: 'bos_token',
  EOS_TOKEN: 'eos_token',
  UNK_TOKEN: 'unk_token',
  SEP_TOKEN: 'sep_token',
  PAD_TOKEN: 'pad_token',
  CLS_TOKEN: 'cls_token',
  MASK_TOKEN: 'mask_token',
};

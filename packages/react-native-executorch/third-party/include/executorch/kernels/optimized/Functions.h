/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/codegen/macros.h> // TORCH_API
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by gen.py from Functions.h

#include "NativeFunctions.h"

namespace torch {
namespace executor {


namespace aten {

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _fft_c2r_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dim, int64_t normalization, int64_t last_dim_size, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_fft_c2r_out(context, self, dim, normalization, last_dim_size, out);
}


// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _fft_r2c_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::ArrayRef<int64_t> dim, int64_t normalization, bool onesided, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_fft_r2c_out(context, self, dim, normalization, onesided, out);
}


// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & _log_softmax_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, int64_t dim, bool half_to_float, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_log_softmax_out(context, self, dim, half_to_float, out);
}


// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_add_out(context, self, other, alpha, out);
}


// aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_add_scalar_out(context, self, other, alpha, out);
}


// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & bmm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mat2, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_bmm_out(context, self, mat2, out);
}


// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_div_out(context, self, other, out);
}


// aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & div_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_div_scalar_out(context, self, other, out);
}


// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & elu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & alpha, const torch::executor::Scalar & scale, const torch::executor::Scalar & input_scale, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_elu_out(context, self, alpha, scale, input_scale, out);
}


// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & exp_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_exp_out(context, self, out);
}


// aten::gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & gelu_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, torch::executor::string_view approximate, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_gelu_out(context, self, approximate, out);
}


// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & le_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_le_scalar_out(context, self, other, out);
}


// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & le_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_le_tensor_out(context, self, other, out);
}


// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & linear_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::optional<torch::executor::Tensor> & bias, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_linear_out(context, input, weight, bias, out);
}


// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & mat2, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_mm_out(context, self, mat2, out);
}


// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mul_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_mul_out(context, self, other, out);
}


// aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mul_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_mul_scalar_out(context, self, other, out);
}


// aten::native_layer_norm.out(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2) -> (Tensor(a!), Tensor(b!), Tensor(c!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &,torch::executor::Tensor &> native_layer_norm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, torch::executor::ArrayRef<int64_t> normalized_shape, const torch::executor::optional<torch::executor::Tensor> & weight, const torch::executor::optional<torch::executor::Tensor> & bias, double eps, torch::executor::Tensor & out0, torch::executor::Tensor & out1, torch::executor::Tensor & out2) {
    return ::torch::executor::native::opt_native_layer_norm_out(context, input, normalized_shape, weight, bias, eps, out0, out1, out2);
}


// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sub_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Tensor & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_sub_out(context, self, other, alpha, out);
}


// aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & sub_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & other, const torch::executor::Scalar & alpha, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_sub_scalar_out(context, self, other, alpha, out);
}


// aten::where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & where_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & condition, const torch::executor::Tensor & self, const torch::executor::Tensor & other, torch::executor::Tensor & out) {
    return ::torch::executor::native::opt_where_out(context, condition, self, other, out);
}

} // namespace aten

} // namespace executor
} // namespace torch

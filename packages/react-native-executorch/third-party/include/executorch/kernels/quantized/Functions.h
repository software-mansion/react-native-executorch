/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/codegen/macros.h> // TORCH_API
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by gen.py from Functions.h

#include "NativeFunctions.h"

namespace torch {
namespace executor {


namespace quantized_decomposed {

// quantized_decomposed::add.out(Tensor a, float a_scale, int a_zero_point, int a_quant_min, int a_quant_max, Tensor b, float b_scale, int b_zero_point, int b_quant_min, int b_quant_max, float out_scale, int out_zero_point, int out_quant_min, int out_quant_max, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & a, double a_scale, int64_t a_zero_point, int64_t a_quant_min, int64_t a_quant_max, const torch::executor::Tensor & b, double b_scale, int64_t b_zero_point, int64_t b_quant_min, int64_t b_quant_max, double out_scale, int64_t out_zero_point, int64_t out_quant_min, int64_t out_quant_max, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_add_out(context, a, a_scale, a_zero_point, a_quant_min, a_quant_max, b, b_scale, b_zero_point, b_quant_min, b_quant_max, out_scale, out_zero_point, out_quant_min, out_quant_max, out);
}


// quantized_decomposed::choose_qparams.Tensor_out(Tensor input, int quant_min, int quant_max, float eps, ScalarType dtype, *, Tensor(a!) scale_out, Tensor(b!) zero_point_out) -> (Tensor(a!), Tensor(b!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, int64_t quant_min, int64_t quant_max, double eps, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out) {
    return ::torch::executor::native::choose_qparams_tensor_out(context, input, quant_min, quant_max, eps, dtype, scale_out, zero_point_out);
}


// quantized_decomposed::dequantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, ScalarType? out_dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::dequantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out_dtype, out);
}


// quantized_decomposed::dequantize_per_tensor.Tensor_out(Tensor input, Tensor scale, Tensor zero_point, int quant_min, int quant_max, ScalarType dtype, *, ScalarType? out_dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::dequantize_per_tensor_tensor_args_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out_dtype, out);
}


// quantized_decomposed::quantize_per_channel.out(Tensor input, Tensor scales, Tensor zero_points, int axis, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_channel_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantize_per_channel_out(context, input, scales, zero_points, axis, quant_min, quant_max, dtype, out);
}


// quantized_decomposed::dequantize_per_channel.out(Tensor input, Tensor scales, Tensor? zero_points, int axis, int quant_min, int quant_max, ScalarType dtype, *, ScalarType? out_dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_channel_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::optional<torch::executor::Tensor> & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::dequantize_per_channel_out(context, input, scales, zero_points, axis, quant_min, quant_max, dtype, out_dtype, out);
}


// quantized_decomposed::embedding_byte.out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_byte_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_byte_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, out);
}


// quantized_decomposed::embedding_byte.dtype_out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_byte_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_byte_dtype_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, dtype, out);
}


// quantized_decomposed::embedding_2bit.out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_2bit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_2bit_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, out);
}


// quantized_decomposed::embedding_2bit.dtype_out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_2bit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_2bit_dtype_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, dtype, out);
}


// quantized_decomposed::embedding_4bit.out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_4bit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_4bit_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, out);
}


// quantized_decomposed::embedding_4bit.dtype_out(Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, int weight_quant_min, int weight_quant_max, Tensor indices, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & embedding_4bit_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_embedding_4bit_dtype_out(context, weight, weight_scales, weight_zero_points, weight_quant_min, weight_quant_max, indices, dtype, out);
}


// quantized_decomposed::mixed_mm.out(Tensor input, Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mixed_mm_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_mixed_mm_out(context, input, weight, weight_scales, weight_zero_points, out);
}


// quantized_decomposed::mixed_linear.out(Tensor input, Tensor weight, Tensor weight_scales, Tensor? weight_zero_points, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & mixed_linear_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantized_mixed_linear_out(context, input, weight, weight_scales, weight_zero_points, dtype, out);
}


// quantized_decomposed::quantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out);
}


// quantized_decomposed::quantize_per_tensor.Tensor_out(Tensor input, Tensor scale, Tensor zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantize_per_tensor_tensor_args_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out);
}


// quantized_decomposed::choose_qparams_per_token_asymmetric.out(Tensor input, ScalarType dtype, *, Tensor(a!) scale_out, Tensor(b!) zero_point_out) -> (Tensor(a!), Tensor(b!))
TORCH_API inline ::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_per_token_asymmetric_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out) {
    return ::torch::executor::native::choose_qparams_per_token_asymmetric_out(context, input, dtype, scale_out, zero_point_out);
}


// quantized_decomposed::quantize_per_token.out(Tensor input, Tensor scales, Tensor zero_points, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_token_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantize_per_token_out(context, input, scales, zero_points, quant_min, quant_max, dtype, out);
}


// quantized_decomposed::dequantize_per_token.out(Tensor input, Tensor scales, Tensor zero_points, int quant_min, int quant_max, ScalarType dtype, ScalarType output_dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_token_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::ScalarType output_dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::dequantize_per_token_out(context, input, scales, zero_points, quant_min, quant_max, dtype, output_dtype, out);
}

} // namespace quantized_decomposed

} // namespace executor
} // namespace torch

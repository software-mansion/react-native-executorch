"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9091],{4886:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"hooks/natural-language-processing/useTextToSpeech","title":"useTextToSpeech","description":"Learn how to use text-to-speech models in your React Native applications with React Native ExecuTorch\'s useTextToSpeech hook.","source":"@site/docs/03-hooks/01-natural-language-processing/useTextToSpeech.md","sourceDirName":"03-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useTextToSpeech","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextToSpeech","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-hooks/01-natural-language-processing/useTextToSpeech.md","tags":[],"version":"current","frontMatter":{"title":"useTextToSpeech","keywords":["text to speech tts","voice synthesizer","transcription","kokoro","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use text-to-speech models in your React Native applications with React Native ExecuTorch\'s useTextToSpeech hook."},"sidebar":"tutorialSidebar","previous":{"title":"useTextEmbeddings","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextEmbeddings"},"next":{"title":"useTokenizer","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTokenizer"}}');var s=t(4848),r=t(8453);const i={title:"useTextToSpeech",keywords:["text to speech tts","voice synthesizer","transcription","kokoro","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use text-to-speech models in your React Native applications with React Native ExecuTorch's useTextToSpeech hook."},c=void 0,a={},d=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Speech Synthesis",id:"speech-synthesis",level:3},{value:"Streaming Synthesis",id:"streaming-synthesis",level:3},{value:"Supported models",id:"supported-models",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Text to speech is a task that allows to transform written text into spoken language. It is commonly used to implement features such as voice assistants, accessibility tools, or audiobooks."}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,s.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-kokoro",children:"Hugging Face repository"}),". You can also use ",(0,s.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsx)(n.p,{children:"You can play the generated waveform in any way most suitable to you; however, in the snippet below we utilize the react-native-audio-api library to play synthesized speech."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import {\n  useTextToSpeech,\n  KOKORO_MEDIUM,\n  KOKORO_VOICE_AF_HEART,\n} from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\n\nconst model = useTextToSpeech({\n  model: KOKORO_MEDIUM,\n  voice: KOKORO_VOICE_AF_HEART,\n});\n\nconst audioContext = new AudioContext({ sampleRate: 24000 });\n\nconst handleSpeech = async (text: string) => {\n  const speed = 1.0;\n  const waveform = await model.forward(text, speed);\n\n  const audioBuffer = audioContext.createBuffer(1, waveform.length, 24000);\n  audioBuffer.getChannelData(0).set(waveform);\n\n  const source = audioContext.createBufferSource();\n  source.buffer = audioBuffer;\n  source.connect(audioContext.destination);\n  source.start();\n};\n"})}),"\n",(0,s.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"model"})})," (",(0,s.jsx)(n.code,{children:"KokoroConfig"}),") - Object specifying the source files for the Kokoro TTS model (duration predictor, synthesizer)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"voice"})})," (",(0,s.jsx)(n.code,{children:"VoiceConfig"}),") - Object specifying the voice data and phonemizer assets (tagger and lexicon)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading after running the hook."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,s.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,s.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Field"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"forward"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(text: string, speed?: number) => Promise<Float32Array>"})}),(0,s.jsxs)(n.td,{children:["Synthesizes a full text into speech. Returns a promise resolving to the full audio waveform as a ",(0,s.jsx)(n.code,{children:"Float32Array"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"stream"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(input: TextToSpeechStreamingInput) => Promise<void>"})}),(0,s.jsx)(n.td,{children:'Starts a streaming synthesis session. Takes a text input and callbacks to handle audio chunks as they are generated. Ideal for reducing the "time to first audio" for long sentences'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"streamStop"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(): void"})}),(0,s.jsx)(n.td,{children:"Stops the streaming process if there is any ongoing."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"error"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"RnExecutorchError | null"})}),(0,s.jsx)(n.td,{children:"Contains the error message if the model failed to load or synthesis failed."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isGenerating"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model is currently processing a synthesis."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isReady"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for synthesis."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"downloadProgress"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"number"})}),(0,s.jsx)(n.td,{children:"Tracks the progress of the model and voice assets download process."})]})]})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"Type definitions"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"interface TextToSpeechStreamingInput {\n  text: string;\n  speed?: number;\n  onBegin?: () => void | Promise<void>;\n  onNext?: (chunk: Float32Array) => Promise<void> | void;\n  onEnd?: () => Promise<void> | void;\n}\n\ninterface KokoroConfig {\n  durationSource: ResourceSource;\n  synthesizerSource: ResourceSource;\n}\n\ninterface VoiceConfig {\n  voiceSource: ResourceSource;\n  extra: {\n    taggerSource: ResourceSource;\n    lexiconSource: ResourceSource;\n  };\n}\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,s.jsx)(n.p,{children:"The module provides two ways to generate speech:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"forward(text, speed)"})}),": Generates the complete audio waveform at once. Returns a promise resolving to a ",(0,s.jsx)(n.code,{children:"Float32Array"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Since it processes the entire text at once, it might take a significant amount of time to produce an audio for long text inputs."})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"stream({ text, speed })"})}),': An async generator that yields chunks of audio as they are computed.\nThis is ideal for reducing the "time to first audio" for long sentences.']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.h3,{id:"speech-synthesis",children:"Speech Synthesis"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import React from 'react';\nimport { Button, View } from 'react-native';\nimport {\n  useTextToSpeech,\n  KOKORO_MEDIUM,\n  KOKORO_VOICE_AF_HEART,\n} from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\n\nexport default function App() {\n  const tts = useTextToSpeech({\n    model: KOKORO_MEDIUM,\n    voice: KOKORO_VOICE_AF_HEART,\n  });\n\n  const generateAudio = async () => {\n    const audioData = await tts.forward({\n      text: 'Hello world! This is a sample text.',\n    });\n\n    // Playback example\n    const ctx = new AudioContext({ sampleRate: 24000 });\n    const buffer = ctx.createBuffer(1, audioData.length, 24000);\n    buffer.getChannelData(0).set(audioData);\n\n    const source = ctx.createBufferSource();\n    source.buffer = buffer;\n    source.connect(ctx.destination);\n    source.start();\n  };\n\n  return (\n    <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>\n      <Button title=\"Speak\" onPress={generateAudio} disabled={!tts.isReady} />\n    </View>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"streaming-synthesis",children:"Streaming Synthesis"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import React, { useRef } from 'react';\nimport { Button, View } from 'react-native';\nimport {\n  useTextToSpeech,\n  KOKORO_MEDIUM,\n  KOKORO_VOICE_AF_HEART,\n} from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\n\nexport default function App() {\n  const tts = useTextToSpeech({\n    model: KOKORO_MEDIUM,\n    voice: KOKORO_VOICE_AF_HEART,\n  });\n\n  const contextRef = useRef(new AudioContext({ sampleRate: 24000 }));\n\n  const generateStream = async () => {\n    const ctx = contextRef.current;\n\n    await tts.stream({\n      text: \"This is a longer text, which is being streamed chunk by chunk. Let's see how it works!\",\n      onNext: async (chunk) => {\n        return new Promise((resolve) => {\n          const buffer = ctx.createBuffer(1, chunk.length, 24000);\n          buffer.getChannelData(0).set(chunk);\n\n          const source = ctx.createBufferSource();\n          source.buffer = buffer;\n          source.connect(ctx.destination);\n          source.onEnded = () => resolve();\n          source.start();\n        });\n      },\n    });\n  };\n\n  return (\n    <View style={{ flex: 1, justifyContent: 'center', alignItems: 'center' }}>\n      <Button title=\"Stream\" onPress={generateStream} disabled={!tts.isReady} />\n    </View>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Language"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-kokoro",children:"Kokoro"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>c});var o=t(6540);const s={},r=o.createContext(s);function i(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);
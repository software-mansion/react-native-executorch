"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4914],{1332:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"fundamentals/loading-models","title":"Loading models","description":"There are three different methods available for loading model files, depending on their size and location.","source":"@site/versioned_docs/version-0.3.x/fundamentals/loading-models.md","sourceDirName":"fundamentals","slug":"/fundamentals/loading-models","permalink":"/react-native-executorch/docs/0.3.x/fundamentals/loading-models","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.3.x/fundamentals/loading-models.md","tags":[],"version":"0.3.x","sidebarPosition":1,"frontMatter":{"title":"Loading models","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Getting Started","permalink":"/react-native-executorch/docs/0.3.x/fundamentals/getting-started"},"next":{"title":"LLMs","permalink":"/react-native-executorch/docs/0.3.x/category/llms"}}');var s=o(4848),r=o(8453);const a={title:"Loading models",sidebar_position:1},i=void 0,l={},d=[{value:"Example",id:"example",level:2}];function c(e){const n={admonition:"admonition",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"There are three different methods available for loading model files, depending on their size and location."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Load from React Native assets folder (For Files < 512MB)"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"modelSource: require('../assets/llama3_2.pte');\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Load from remote URL:"})}),"\n",(0,s.jsx)(n.p,{children:"For files larger than 512MB or when you want to keep size of the app smaller, you can load the model from a remote URL (e.g. HuggingFace)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"modelSource: 'https://.../llama3_2.pte';\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Load from local file system:"})}),"\n",(0,s.jsx)(n.p,{children:"If you prefer to delegate the process of obtaining and loading model and tokenizer files to the user, you can use the following method:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"modelSource: 'file:///var/mobile/.../llama3_2.pte';\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"The downloaded files are stored in documents directory of your application."})}),"\n",(0,s.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,s.jsxs)(n.p,{children:["The following code snippet demonstrates how to load model and tokenizer files using ",(0,s.jsx)(n.code,{children:"useLLM"})," hook:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { useLLM } from 'react-native-executorch';\n\nconst llama = useLLM({\n  modelSource: 'https://.../llama3_2.pte',\n  tokenizerSource: require('../assets/tokenizer.bin'),\n});\n"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>i});var t=o(6540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);
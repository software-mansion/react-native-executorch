"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[2830],{2042:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"hooks/computer-vision/useStyleTransfer","title":"useStyleTransfer","description":"Style transfer is a technique used in computer graphics and machine learning where the visual style of one image is applied to the content of another. This is achieved using algorithms that manipulate data from both images, typically with the aid of a neural network. The result is a new image that combines the artistic elements of one picture with the structural details of another, effectively merging art with traditional imagery. React Native ExecuTorch offers a dedicated hook useStyleTransfer, for this task. However before you start you\'ll need to obtain ExecuTorch-compatible model binary.","source":"@site/versioned_docs/version-0.6.x/02-hooks/02-computer-vision/useStyleTransfer.md","sourceDirName":"02-hooks/02-computer-vision","slug":"/hooks/computer-vision/useStyleTransfer","permalink":"/react-native-executorch/docs/hooks/computer-vision/useStyleTransfer","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.6.x/02-hooks/02-computer-vision/useStyleTransfer.md","tags":[],"version":"0.6.x","frontMatter":{"title":"useStyleTransfer"},"sidebar":"tutorialSidebar","previous":{"title":"useObjectDetection","permalink":"/react-native-executorch/docs/hooks/computer-vision/useObjectDetection"},"next":{"title":"useTextToImage","permalink":"/react-native-executorch/docs/hooks/computer-vision/useTextToImage"}}');var s=n(4848),i=n(8453);const l={title:"useStyleTransfer"},d=void 0,c={},o=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["Style transfer is a technique used in computer graphics and machine learning where the visual style of one image is applied to the content of another. This is achieved using algorithms that manipulate data from both images, typically with the aid of a neural network. The result is a new image that combines the artistic elements of one picture with the structural details of another, effectively merging art with traditional imagery. React Native ExecuTorch offers a dedicated hook ",(0,s.jsx)(t.code,{children:"useStyleTransfer"}),", for this task. However before you start you'll need to obtain ExecuTorch-compatible model binary."]}),"\n",(0,s.jsx)(t.admonition,{type:"warning",children:(0,s.jsxs)(t.p,{children:["It is recommended to use models provided by us which are available at our ",(0,s.jsx)(t.a,{href:"https://huggingface.co/collections/software-mansion/style-transfer-68d0eab2b0767a20e7efeaf5",children:"Hugging Face repository"}),", you can also use ",(0,s.jsx)(t.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,s.jsx)(t.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-typescript",children:"import {\n  useStyleTransfer,\n  STYLE_TRANSFER_CANDY,\n} from 'react-native-executorch';\n\nconst model = useStyleTransfer({ model: STYLE_TRANSFER_CANDY });\n\nconst imageUri = 'file::///Users/.../cute_cat.png';\n\ntry {\n  const generatedImageUrl = await model.forward(imageUri);\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,s.jsx)(t.h3,{id:"arguments",children:"Arguments"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.code,{children:"modelSource"})})," - A string that specifies the location of the model binary."]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:(0,s.jsx)(t.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,s.jsxs)(t.p,{children:["For more information on loading resources, take a look at ",(0,s.jsx)(t.a,{href:"/react-native-executorch/docs/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,s.jsx)(t.h3,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Field"}),(0,s.jsx)(t.th,{children:"Type"}),(0,s.jsx)(t.th,{children:"Description"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"forward"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"(imageSource: string) => Promise<string>"})}),(0,s.jsxs)(t.td,{children:["Executes the model's forward pass, where ",(0,s.jsx)(t.code,{children:"imageSource"})," can be a fetchable resource or a Base64-encoded string."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"error"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)("code",{children:"string | null"})}),(0,s.jsx)(t.td,{children:"Contains the error message if the model failed to load."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"isGenerating"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"boolean"})}),(0,s.jsx)(t.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"isReady"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"boolean"})}),(0,s.jsx)(t.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"downloadProgress"})}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"number"})}),(0,s.jsx)(t.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,s.jsx)(t.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,s.jsxs)(t.p,{children:["To run the model, you can use ",(0,s.jsx)(t.code,{children:"forward"})," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The function returns a promise which can resolve either to an error or a URL to generated image."]}),"\n",(0,s.jsx)(t.admonition,{type:"info",children:(0,s.jsx)(t.p,{children:"Images from external sources and the generated image are stored in your application's temporary directory."})}),"\n",(0,s.jsx)(t.h2,{id:"example",children:"Example"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-typescript",children:"function App() {\n  const model = useStyleTransfer({ model: STYLE_TRANSFER_CANDY });\n\n  // ...\n  const imageUri = 'file::///Users/.../cute_cat.png';\n\n  try {\n    const generatedImageUrl = await model.forward(imageUri);\n  } catch (error) {\n    console.error(error);\n  }\n  // ...\n}\n"})}),"\n",(0,s.jsx)(t.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/pytorch/examples/tree/main/fast_neural_style",children:"Candy"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/pytorch/examples/tree/main/fast_neural_style",children:"Mosaic"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/pytorch/examples/tree/main/fast_neural_style",children:"Udnie"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/pytorch/examples/tree/main/fast_neural_style",children:"Rain princess"})}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,s.jsx)(t.h3,{id:"model-size",children:"Model size"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Model"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Core ML [MB]"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_CANDY"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"6.78"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"5.22"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_MOSAIC"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"6.78"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"5.22"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_UDNIE"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"6.78"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"5.22"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_RAIN_PRINCESS"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"6.78"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"5.22"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Model"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"iOS (Core ML) [MB]"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_CANDY"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1200"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"380"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_MOSAIC"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1200"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"380"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_UDNIE"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1200"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"380"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_RAIN_PRINCESS"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1200"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"380"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,s.jsx)(t.admonition,{type:"warning",children:(0,s.jsx)(t.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization."})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Model"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone 17 Pro (Core ML) [ms]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (Core ML) [ms]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone SE 3 (Core ML) [ms]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,s.jsx)(t.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_CANDY"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1400"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1485"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"4255"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2510"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2355"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_MOSAIC"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1400"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1485"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"4255"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2510"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2355"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_UDNIE"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1400"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1485"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"4255"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2510"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2355"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"STYLE_TRANSFER_RAIN_PRINCESS"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1400"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"1485"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"4255"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2510"}),(0,s.jsx)(t.td,{style:{textAlign:"center"},children:"2355"})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>d});var r=n(6540);const s={},i=r.createContext(s);function l(e){const t=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);
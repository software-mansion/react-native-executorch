"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9936],{6276:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"natural-language-processing/useSpeechToText","title":"useSpeechToText","description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook.","source":"@site/versioned_docs/version-0.4.x/natural-language-processing/useSpeechToText.md","sourceDirName":"natural-language-processing","slug":"/natural-language-processing/useSpeechToText","permalink":"/react-native-executorch/docs/natural-language-processing/useSpeechToText","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.4.x/natural-language-processing/useSpeechToText.md","tags":[],"version":"0.4.x","frontMatter":{"title":"useSpeechToText","keywords":["speech to text","stt","voice recognition","transcription","whisper","moonshine","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook."},"sidebar":"tutorialSidebar","previous":{"title":"useLLM","permalink":"/react-native-executorch/docs/natural-language-processing/useLLM"},"next":{"title":"useTextEmbeddings","permalink":"/react-native-executorch/docs/natural-language-processing/useTextEmbeddings"}}');var r=t(4848),s=t(8453);const o={title:"useSpeechToText",keywords:["speech to text","stt","voice recognition","transcription","whisper","moonshine","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch's useSpeechToText hook."},a=void 0,c={},d=[{value:"Reference",id:"reference",level:2},{value:"Streaming",id:"streaming",level:3},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:4},{value:"Example",id:"example",level:2},{value:"Live data (microphone) transcription",id:"live-data-microphone-transcription",level:3},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["With the latest ",(0,r.jsx)(n.code,{children:"v0.3.0"})," release we introduce a new hook - ",(0,r.jsx)(n.code,{children:"useSpeechToText"}),". Speech to text is a task that allows to transform spoken language to written text. It is commonly used to implement features such as transcription or voice assistants. As of now, ",(0,r.jsx)(n.a,{href:"#supported-models",children:"all supported STT models"})," run on the XNNPACK backend."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Currently, we do not support direct microphone input streaming to the model. Instead, in v0.3.0, we provide a way to transcribe an audio file."})}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-moonshine-tiny",children:"Hugging Face repository"}),". You can also use ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/release/0.4/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsxs)(n.p,{children:["You can obtain waveform from audio in any way most suitable to you, however in the snippet below we utilize ",(0,r.jsx)(n.code,{children:"react-native-audio-api"})," library to process a mp3 file."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { useSpeechToText } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst { transcribe, error } = useSpeechToText({\n  modelName: 'moonshine',\n});\n\nconst loadAudio = async (url: string) => {\n  const audioContext = new AudioContext({ sampleRate: 16e3 });\n  const audioBuffer = await FileSystem.downloadAsync(\n    url,\n    FileSystem.documentDirectory + '_tmp_transcribe_audio.mp3'\n  ).then(({ uri }) => {\n    return audioContext.decodeAudioDataSource(uri);\n  });\n  return Array.from(audioBuffer?.getChannelData(0));\n};\n\nconst audioUrl = ...; // URL with audio to transcribe\nconst waveform = await loadAudio(audioUrl);\nconst transcription = await transcribe(waveform);\nif (error) {\n  console.log(error);\n} else {\n  console.log(transcription);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,r.jsxs)(n.p,{children:["Given that STT models can process audio no longer than 30 seconds, there is a need to chunk the input audio. Chunking audio may result in cutting speech mid-sentence, which might be hard to understand for the model. To make it work, we employed an algorithm (adapted for mobile devices from ",(0,r.jsx)(n.a,{href:"https://aclanthology.org/2023.ijcnlp-demo.3.pdf",children:"whisper-streaming"}),") that uses overlapping audio chunks. This might introduce some overhead, but allows for processing audio inputs of arbitrary length."]}),"\n",(0,r.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"modelName"})}),"\nA literal of ",(0,r.jsx)(n.code,{children:'"moonshine" | "whisper" | "whisperMultilingual'})," which serves as an identifier for which model should be used."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"encoderSource?"})}),"\nA string that specifies the location of a .pte file for the encoder. For further information on passing model sources, check out ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/fundamentals/loading-models",children:"Loading Models"}),". Defaults to ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/release/0.4/src/constants/modelUrls.ts",children:"constants"})," for given model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"decoderSource?"})}),"\nAnalogous to the encoderSource, this takes in a string which is a source for the decoder part of the model. Defaults to ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/release/0.4/src/constants/modelUrls.ts",children:"constants"})," for given model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerSource?"})}),"\nA string that specifies the location to the tokenizer for the model. This works just as the encoder and decoder do. Defaults to ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/release/0.4/src/constants/modelUrls.ts",children:"constants"})," for given model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"overlapSeconds?"})}),"\nSpecifies the length of overlap between consecutive audio chunks (expressed in seconds). Overrides ",(0,r.jsx)(n.code,{children:"streamingConfig"})," argument."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"windowSize?"})}),"\nSpecifies the size of each audio chunk (expressed in seconds). Overrides ",(0,r.jsx)(n.code,{children:"streamingConfig"})," argument."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"streamingConfig?"})}),"\nSpecifies config for both ",(0,r.jsx)(n.code,{children:"overlapSeconds"})," and ",(0,r.jsx)(n.code,{children:"windowSize"})," values. Three options are available: ",(0,r.jsx)(n.code,{children:"fast"}),", ",(0,r.jsx)(n.code,{children:"balanced"})," and ",(0,r.jsx)(n.code,{children:"quality"}),". We discourage using ",(0,r.jsx)(n.code,{children:"fast"})," config with ",(0,r.jsx)(n.code,{children:"Whisper"})," model which while has the lowest latency to first token has the slowest overall speed."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,r.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"transcribe"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(waveform: number[], audioLanguage?: SpeechToTextLanguage) => Promise<string>"})}),(0,r.jsxs)(n.td,{children:["Starts a transcription process for a given input array, which should be a waveform at 16kHz. Resolves a promise with the output transcription when the model is finished. For multilingual models, you have to specify the audioLanguage flag, which is the language of the spoken language in the audio. Returns error when called when module is in use (i.e. in process of ",(0,r.jsx)(n.code,{children:"streamingTranscribe"})," action)"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"streamingTranscribe"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(streamingAction: STREAMING_ACTION, waveform?: number[], audioLanguage?: SpeechToTextLanguage) => Promise<string>"})}),(0,r.jsxs)(n.td,{children:["This allows for running transcription process on-line, which means where the whole audio is not known beforehand i.e. when transcribing from a live microphone feed. ",(0,r.jsx)(n.code,{children:"streamingAction"})," defines the type of package sent to the model: ",(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:"START"})," - initializes the process, allows for optional ",(0,r.jsx)(n.code,{children:"waveform"})," data"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:"DATA"})," - this package should contain consecutive audio data chunks sampled in 16k Hz"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:"STOP"})," - the last data chunk for this transcription, ends the transcription process and flushes internal buffers"]})," Each call returns most recent transcription. Returns error when called when module is in use (i.e. processing ",(0,r.jsx)(n.code,{children:"transcribe"})," call)"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"error"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"Error | undefined"})}),(0,r.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sequence"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"string"})}),(0,r.jsx)(n.td,{children:"This property is updated with each generated token. If you're looking to obtain tokens as they're generated, you should use this property."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isGenerating"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isReady"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"configureStreaming"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"(overlapSeconds?: number, windowSize?: number, streamingConfig?: 'fast' | 'balanced' | 'quality')"})}),(0,r.jsxs)(n.td,{children:["Configures options for the streaming algorithm: ",(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:"overlapSeconds"})," determines how much adjacent audio chunks overlap (increasing it slows down transcription, decreases probability of weird wording at the chunks intersection, setting it larger than 3 seconds generally is discouraged), "]}),(0,r.jsxs)("li",{children:[(0,r.jsx)(n.code,{children:"windowSize"})," describes size of the audio chunks (increasing it speeds up the end to end transcription time, but increases latency for the first token to be returned),"]}),(0,r.jsxs)("li",{children:[" ",(0,r.jsx)(n.code,{children:"streamingConfig"})," predefined configs for ",(0,r.jsx)(n.code,{children:"windowSize"})," and ",(0,r.jsx)(n.code,{children:"overlapSeconds"})," values."]})]})," Keep ",(0,r.jsx)(n.code,{children:"windowSize + 2 * overlapSeconds <= 30"}),"."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"downloadProgress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"number"})}),(0,r.jsx)(n.td,{children:"Tracks the progress of the model download process."})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"enum STREAMING_ACTION {\n  START,\n  DATA,\n  STOP,\n}\n\nenum SpeechToTextLanguage {\n  Afrikaans = 'af',\n  Albanian = 'sq',\n  Arabic = 'ar',\n  Armenian = 'hy',\n  Azerbaijani = 'az',\n  Basque = 'eu',\n  Belarusian = 'be',\n  Bengali = 'bn',\n  Bosnian = 'bs',\n  Bulgarian = 'bg',\n  Burmese = 'my',\n  Catalan = 'ca',\n  Chinese = 'zh',\n  Croatian = 'hr',\n  Czech = 'cs',\n  Danish = 'da',\n  Dutch = 'nl',\n  Estonian = 'et',\n  English = 'en',\n  Finnish = 'fi',\n  French = 'fr',\n  Galician = 'gl',\n  Georgian = 'ka',\n  German = 'de',\n  Greek = 'el',\n  Gujarati = 'gu',\n  HaitianCreole = 'ht',\n  Hebrew = 'he',\n  Hindi = 'hi',\n  Hungarian = 'hu',\n  Icelandic = 'is',\n  Indonesian = 'id',\n  Italian = 'it',\n  Japanese = 'ja',\n  Kannada = 'kn',\n  Kazakh = 'kk',\n  Khmer = 'km',\n  Korean = 'ko',\n  Lao = 'lo',\n  Latvian = 'lv',\n  Lithuanian = 'lt',\n  Macedonian = 'mk',\n  Malagasy = 'mg',\n  Malay = 'ms',\n  Malayalam = 'ml',\n  Maltese = 'mt',\n  Marathi = 'mr',\n  Nepali = 'ne',\n  Norwegian = 'no',\n  Persian = 'fa',\n  Polish = 'pl',\n  Portuguese = 'pt',\n  Punjabi = 'pa',\n  Romanian = 'ro',\n  Russian = 'ru',\n  Serbian = 'sr',\n  Sinhala = 'si',\n  Slovak = 'sk',\n  Slovenian = 'sl',\n  Spanish = 'es',\n  Sundanese = 'su',\n  Swahili = 'sw',\n  Swedish = 'sv',\n  Tagalog = 'tl',\n  Tajik = 'tg',\n  Tamil = 'ta',\n  Telugu = 'te',\n  Thai = 'th',\n  Turkish = 'tr',\n  Ukrainian = 'uk',\n  Urdu = 'ur',\n  Uzbek = 'uz',\n  Vietnamese = 'vi',\n  Welsh = 'cy',\n  Yiddish = 'yi',\n}\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["Before running the model's ",(0,r.jsx)(n.code,{children:"transcribe"})," method be sure to obtain waveform of the audio You wish to transcribe. You need to obtain the waveform from audio on your own (remember to use sampling rate of 16kHz!), in the snippet above we provide an example how you can do that. In the latter case just pass the obtained waveform as argument to the ",(0,r.jsx)(n.code,{children:"transcribe"})," method which returns a promise resolving to the generated tokens when successful. If the model fails during inference the ",(0,r.jsx)(n.code,{children:"error"})," property contains details of the error. If you want to obtain tokens in a streaming fashion, you can also use the sequence property, which is updated with each generated token, similar to the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/natural-language-processing/useLLM",children:"useLLM"})," hook."]}),"\n",(0,r.jsx)(n.h4,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,r.jsxs)(n.p,{children:["If you aim to obtain a transcription in other languages than English, in v0.4.0 we introduced a new model - ",(0,r.jsx)(n.code,{children:"whisperMultilingual"}),", a multilingual version of Whisper. To obtain the output text in your desired language, make sure pass ",(0,r.jsx)(n.code,{children:"audioLanguage"})," to ",(0,r.jsx)(n.code,{children:"transcribe"}),". You should not pass this flag if you're using a non-multilingual model. For example:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextLanguage } from 'react-native-executorch';\n\n// Rest of your code...\nconst mySpanishAudio = ...;\nawait model.transcribe(mySpanishAudio, SpeechToTextLanguage.Spanish);\n// Rest of your code...\n"})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { Button, Text, View } from 'react-native';\nimport { useSpeechToText } from 'react-native-executorch';\nimport * as FileSystem from 'expo-file-system';\nimport { AudioContext } from 'react-native-audio-api';\n\nfunction App() {\n  const { transcribe, sequence, error } = useSpeechToText({\n    modelName: 'whisper',\n  });\n\n  const loadAudio = async (url: string) => {\n    const audioContext = new AudioContext({ sampleRate: 16e3 });\n    const audioBuffer = await FileSystem.downloadAsync(\n      url,\n      FileSystem.documentDirectory + '_tmp_transcribe_audio.mp3'\n    ).then(({ uri }) => {\n      return audioContext.decodeAudioDataSource(uri);\n    });\n    return Array.from(audioBuffer?.getChannelData(0));\n  };\n\n  const audioUrl = '...;' // URL with audio to transcribe\n\n  return (\n    <View>\n      <Button\n        onPress={async () => {\n          await transcribe(await loadAudio(audioUrl));\n        }}\n        title=\"Transcribe\"\n      />\n      <Text>{error ? error.message : sequence}</Text>\n    </View>\n  );\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"live-data-microphone-transcription",children:"Live data (microphone) transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { STREAMING_ACTION, useSpeechToText } from 'react-native-executorch';\nimport LiveAudioStream from 'react-native-live-audio-stream';\nimport { useState } from 'react';\nimport { Buffer } from 'buffer';\n\nconst audioStreamOptions = {\n  sampleRate: 16000,\n  channels: 1,\n  bitsPerSample: 16,\n  audioSource: 1,\n  bufferSize: 16000,\n};\n\nconst startStreamingAudio = (options: any, onChunk: (data: string) => void) => {\n  LiveAudioStream.init(options);\n  LiveAudioStream.on('data', onChunk);\n  LiveAudioStream.start();\n};\n\nconst float32ArrayFromPCMBinaryBuffer = (b64EncodedBuffer: string) => {\n  const b64DecodedChunk = Buffer.from(b64EncodedBuffer, 'base64');\n  const int16Array = new Int16Array(b64DecodedChunk.buffer);\n\n  const float32Array = new Float32Array(int16Array.length);\n  for (let i = 0; i < int16Array.length; i++) {\n    float32Array[i] = Math.max(\n      -1,\n      Math.min(1, (int16Array[i] / audioStreamOptions.bufferSize) * 8)\n    );\n  }\n  return float32Array;\n};\n\nfunction App() {\n  const [isRecording, setIsRecording] = useState(false);\n  const speechToText = useSpeechToText({\n    modelName: 'moonshine',\n    windowSize: 3,\n    overlapSeconds: 1.2,\n  });\n\n  const onChunk = (data: string) => {\n    const float32Chunk = float32ArrayFromPCMBinaryBuffer(data);\n    speechToText.streamingTranscribe(\n      STREAMING_ACTION.DATA,\n      Array.from(float32Chunk)\n    );\n  };\n\n  const handleRecordPress = async () => {\n    if (isRecording) {\n      setIsRecording(false);\n      LiveAudioStream.stop();\n      messageRecorded.current = true;\n      await speechToText.streamingTranscribe(STREAMING_ACTION.STOP);\n    } else {\n      setIsRecording(true);\n      startStreamingAudio(audioStreamOptions, onChunk);\n      await speechToText.streamingTranscribe(STREAMING_ACTION.START);\n    }\n  };\n\n  return\n    <View>\n      <Text>\n        {speechToText.sequence}\n      </Text>\n      <TouchableOpacity\n        style={\n          !isRecording ? styles.recordTouchable : styles.recordingInfo\n        }\n        onPress={handleRecordPress}\n      >\n        {isRecording ? (\n          <Text>Stop</Text>\n        ) : (\n          <Text>Record</Text>\n        )}\n      </TouchableOpacity>\n    </View>\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Language"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny.en",children:"Whisper tiny.en"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny",children:"Whisper tiny"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/UsefulSensors/moonshine-tiny",children:"Moonshine tiny"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"WHISPER_TINY"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"231.0"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MOONSHINE_TINY"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"148.9"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"WHISPER_TINY"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"900"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"600"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MOONSHINE_TINY"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"650"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"560"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7216],{1392:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"typescript-api/natural-language-processing/LLMModule","title":"LLMModule","description":"TypeScript API implementation of the useLLM hook.","source":"@site/docs/04-typescript-api/01-natural-language-processing/LLMModule.md","sourceDirName":"04-typescript-api/01-natural-language-processing","slug":"/typescript-api/natural-language-processing/LLMModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/LLMModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/04-typescript-api/01-natural-language-processing/LLMModule.md","tags":[],"version":"current","frontMatter":{"title":"LLMModule"},"sidebar":"tutorialSidebar","previous":{"title":"Natural Language Processing","permalink":"/react-native-executorch/docs/next/category/natural-language-processing-1"},"next":{"title":"SpeechToTextModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/SpeechToTextModule"}}');var o=t(4848),r=t(8453);const i={title:"LLMModule"},l=void 0,d={},c=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Listening for download progress",id:"listening-for-download-progress",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Listening for generated tokens",id:"listening-for-generated-tokens",level:2},{value:"Interrupting the model",id:"interrupting-the-model",level:2},{value:"Token Batching",id:"token-batching",level:2},{value:"Configuring the model",id:"configuring-the-model",level:2},{value:"Deleting the model from memory",id:"deleting-the-model-from-memory",level:2}];function a(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useLLM",children:"useLLM"})," hook."]}),"\n",(0,o.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { LLMModule, LLAMA3_2_1B_QLORA } from 'react-native-executorch';\n\n// Creating an instance\nconst llm = new LLMModule({\n  tokenCallback: (token) => console.log(token),\n  messageHistoryCallback: (messages) => console.log(messages),\n});\n\n// Loading the model\nawait llm.load(LLAMA3_2_1B_QLORA, (progress) => console.log(progress));\n\n// Running the model - returns the generated response\nconst response = await llm.sendMessage('Hello, World!');\nconsole.log('Response:', response);\n\n// Interrupting the model (to actually interrupt the generation it has to be called when sendMessage or generate is running)\nllm.interrupt();\n\n// Deleting the model from memory\nllm.delete();\n"})}),"\n",(0,o.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Method"}),(0,o.jsx)(n.th,{children:"Type"}),(0,o.jsx)(n.th,{children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"constructor"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"({tokenCallback?: (token: string) => void, messageHistoryCallback?: (messageHistory: Message[]) => void})"})}),(0,o.jsx)(n.td,{children:"Creates a new instance of LLMModule with optional callbacks."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"load"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(model: { modelSource: ResourceSource; tokenizerSource: ResourceSource; tokenizerConfigSource: ResourceSource }, onDownloadProgressCallback?: (progress: number) => void): Promise<void>"})}),(0,o.jsx)(n.td,{children:"Loads the model."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"setTokenCallback"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"{tokenCallback: (token: string) => void}) => void"})}),(0,o.jsx)(n.td,{children:"Sets new token callback invoked on every token batch."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"generate"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(messages: Message[], tools?: LLMTool[]) => Promise<string>"})}),(0,o.jsxs)(n.td,{children:["Runs model to complete chat passed in ",(0,o.jsx)(n.code,{children:"messages"})," argument. Returns the generated response. It doesn't manage conversation context."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"forward"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(input: string) => Promise<string>"})}),(0,o.jsxs)(n.td,{children:["Runs model inference with raw input string. Returns the generated response. You need to provide entire conversation and prompt (in correct format and with special tokens!) in input string to this method. It doesn't manage conversation context. It is intended for users that need access to the model itself without any wrapper. If you want a simple chat with model the consider using ",(0,o.jsx)(n.code,{children:"sendMessage"})]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"configure"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"({chatConfig?: Partial<ChatConfig>, toolsConfig?: ToolsConfig, generationConfig?: GenerationConfig}) => void"})}),(0,o.jsxs)(n.td,{children:["Configures chat and tool calling and generation settings. See more details in ",(0,o.jsx)(n.a,{href:"#configuring-the-model",children:"configuring the model"}),"."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"sendMessage"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(message: string) => Promise<string>"})}),(0,o.jsxs)(n.td,{children:["Method to add user message to conversation. Returns the generated response. After model responds it will call ",(0,o.jsx)(n.code,{children:"messageHistoryCallback()"})," containing both user message and model response."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"deleteMessage"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(index: number) => void"})}),(0,o.jsxs)(n.td,{children:["Deletes all messages starting with message on ",(0,o.jsx)(n.code,{children:"index"})," position. After deletion it will call ",(0,o.jsx)(n.code,{children:"messageHistoryCallback()"})," containing new history."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"delete"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"() => void"})}),(0,o.jsx)(n.td,{children:"Method to delete the model from memory. Note you cannot delete model while it's generating. You need to interrupt it first and make sure model stopped generation."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"interrupt"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"() => void"})}),(0,o.jsx)(n.td,{children:"Interrupts model generation. It may return one more token after interrupt."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"getGeneratedTokenCount"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"() => number"})}),(0,o.jsx)(n.td,{children:"Returns the number of tokens generated in the last response."})]})]})]}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Type definitions"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"type ResourceSource = string | number | object;\n\ntype MessageRole = 'user' | 'assistant' | 'system';\n\ninterface Message {\n  role: MessageRole;\n  content: string;\n}\ninterface ChatConfig {\n  initialMessageHistory: Message[];\n  contextWindowLength: number;\n  systemPrompt: string;\n}\n\ninterface GenerationConfig {\n  temperature?: number;\n  topp?: number;\n  outputTokenBatchSize?: number;\n  batchTimeInterval?: number;\n}\n\n// tool calling\ninterface ToolsConfig {\n  tools: LLMTool[];\n  executeToolCallback: (call: ToolCall) => Promise<string | null>;\n  displayToolCalls?: boolean;\n}\n\ninterface ToolCall {\n  toolName: string;\n  arguments: Object;\n}\n\ntype LLMTool = Object;\n"})})]}),"\n",(0,o.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,o.jsx)(n.p,{children:"To create a new instance of LLMModule, use the constructor with optional callbacks:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"tokenCallback"})})," - (Optional) A function that will be called on every generated token with that token as its only argument."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"messageHistoryCallback"})})," - (Optional) A function called on every finished message. Returns the entire message history."]}),"\n",(0,o.jsxs)(n.p,{children:["Then, to load the model, use the ",(0,o.jsx)(n.code,{children:"load"})," method. It accepts an object with the following fields:"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"model"})})," - Object containing the model source, tokenizer source, and tokenizer config source."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"modelSource"})})," - ",(0,o.jsx)(n.code,{children:"ResourceSource"})," specifying the location of the model binary."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"tokenizerSource"})})," - ",(0,o.jsx)(n.code,{children:"ResourceSource"})," specifying the location of the tokenizer."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"tokenizerConfigSource"})})," - ",(0,o.jsx)(n.code,{children:"ResourceSource"})," specifying the location of the tokenizer config."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - (Optional) Function called on download progress."]}),"\n",(0,o.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,o.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,o.jsx)(n.h2,{id:"listening-for-download-progress",children:"Listening for download progress"}),"\n",(0,o.jsxs)(n.p,{children:["To subscribe to the download progress event, you can pass the ",(0,o.jsx)(n.code,{children:"onDownloadProgressCallback"})," function to the ",(0,o.jsx)(n.code,{children:"load"})," method. This function is called whenever the download progress changes."]}),"\n",(0,o.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,o.jsxs)(n.p,{children:["To run the model, you can use ",(0,o.jsx)(n.code,{children:"generate"})," method. It allows you to pass chat messages and returns a promise that resolves to the generated response. It doesn't provide any message history management."]}),"\n",(0,o.jsxs)(n.p,{children:["Alternatively in managed chat (see: ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useLLM#functional-vs-managed",children:"Functional vs managed"}),"), you can use the ",(0,o.jsx)(n.code,{children:"sendMessage"})," method. It accepts the user message and returns a promise that resolves to the generated response. Additionally, it will call ",(0,o.jsx)(n.code,{children:"messageHistoryCallback"})," with the updated message history containing both user message and model response."]}),"\n",(0,o.jsxs)(n.p,{children:["If you need raw model access without any wrappers, you can use ",(0,o.jsx)(n.code,{children:"forward"}),". It provides direct access to the model, so the input string is passed straight into the model and returns the generated response. It may be useful to work with models that aren't finetuned for chat completions. If you're not sure what are implications of that (e.g. that you have to include special model tokens), you're better off with ",(0,o.jsx)(n.code,{children:"sendMessage"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"listening-for-generated-tokens",children:"Listening for generated tokens"}),"\n",(0,o.jsxs)(n.p,{children:["To subscribe to the token generation event, you can pass ",(0,o.jsx)(n.code,{children:"tokenCallback"})," or ",(0,o.jsx)(n.code,{children:"messageHistoryCallback"})," functions to the constructor. ",(0,o.jsx)(n.code,{children:"tokenCallback"})," is called on every token and contains only the most recent token. ",(0,o.jsx)(n.code,{children:"messageHistoryCallback"})," is called whenever model finishes generation and contains all message history including user's and model's last messages."]}),"\n",(0,o.jsx)(n.h2,{id:"interrupting-the-model",children:"Interrupting the model"}),"\n",(0,o.jsxs)(n.p,{children:["In order to interrupt the model, you can use the ",(0,o.jsx)(n.code,{children:"interrupt"})," method."]}),"\n",(0,o.jsx)(n.h2,{id:"token-batching",children:"Token Batching"}),"\n",(0,o.jsxs)(n.p,{children:["Depending on selected model and the user's device generation speed can be above 60 tokens per second. If the ",(0,o.jsx)(n.code,{children:"tokenCallback"})," triggers rerenders and is invoked on every single token it can significantly decrease the app's performance. To alleviate this and help improve performance we've implemented token batching. To configure this you need to call ",(0,o.jsx)(n.code,{children:"configure"})," method and pass ",(0,o.jsx)(n.code,{children:"generationConfig"}),". Inside you can set two parameters ",(0,o.jsx)(n.code,{children:"outputTokenBatchSize"})," and ",(0,o.jsx)(n.code,{children:"batchTimeInterval"}),". They set the size of the batch before tokens are emitted and the maximum time interval between consecutive batches respectively. Each batch is emitted if either ",(0,o.jsx)(n.code,{children:"timeInterval"})," elapses since last batch or ",(0,o.jsx)(n.code,{children:"countInterval"})," number of tokens are generated. This allows for smooth generation even if model lags during generation. Default parameters are set to 10 tokens and 80ms for time interval (~12 batches per second)."]}),"\n",(0,o.jsx)(n.h2,{id:"configuring-the-model",children:"Configuring the model"}),"\n",(0,o.jsxs)(n.p,{children:["To configure model (i.e. change system prompt, load initial conversation history or manage tool calling, set generation settings) you can use\n",(0,o.jsx)(n.code,{children:"configure"})," method. ",(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"chatConfig"})})," and ",(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"toolsConfig"})})," is only applied to managed chats i.e. when using ",(0,o.jsx)(n.code,{children:"sendMessage"})," (see: ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useLLM#functional-vs-managed",children:"Functional vs managed"}),") It accepts object with following fields:"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"chatConfig"})})," - Object configuring chat management:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"systemPrompt"})}),' - Often used to tell the model what is its purpose, for example - "Be a helpful translator".']}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"initialMessageHistory"})})," - An array of ",(0,o.jsx)(n.code,{children:"Message"})," objects that represent the conversation history. This can be used to provide initial context to the model."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"contextWindowLength"})})," - The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"toolsConfig"})})," - Object configuring options for enabling and managing tool use. ",(0,o.jsx)(n.strong,{children:"It will only have effect if your model's chat template support it"}),". Contains following properties:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"tools"})})," - List of objects defining tools."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"executeToolCallback"})})," - Function that accepts ",(0,o.jsx)(n.code,{children:"ToolCall"}),", executes tool and returns the string to model."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"displayToolCalls"})})," - If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"generationConfig"})})," - Object configuring generation settings."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"outputTokenBatchSize"})})," - Soft upper limit on the number of tokens in each token batch (in certain cases there can be more tokens in given batch, i.e. when the batch would end with special emoji join character)."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"batchTimeInterval"})})," - Upper limit on the time interval between consecutive token batches."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"temperature"})})," - Scales output logits by the inverse of temperature. Controls the randomness / creativity of text generation."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"topp"})})," - Only samples from the smallest set of tokens whose cumulative probability exceeds topp."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"deleting-the-model-from-memory",children:"Deleting the model from memory"}),"\n",(0,o.jsxs)(n.p,{children:["To delete the model from memory, you can use the ",(0,o.jsx)(n.code,{children:"delete"})," method."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var s=t(6540);const o={},r=s.createContext(o);function i(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[6133],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var o=t(96540);const r={},s=o.createContext(r);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(s.Provider,{value:n},e.children)}},65709:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"hooks/natural-language-processing/useVAD","title":"useVAD","description":"Voice Activity Detection (VAD) is the task of analyzing an audio signal to identify time segments containing human speech, separating them from non-speech sections like silence and background noise.","source":"@site/docs/03-hooks/01-natural-language-processing/useVAD.md","sourceDirName":"03-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useVAD","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useVAD","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-hooks/01-natural-language-processing/useVAD.md","tags":[],"version":"current","frontMatter":{"title":"useVAD"},"sidebar":"tutorialSidebar","previous":{"title":"useTokenizer","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTokenizer"},"next":{"title":"Computer Vision","permalink":"/react-native-executorch/docs/next/category/computer-vision"}}');var r=t(74848),s=t(28453);const a={title:"useVAD"},i=void 0,c={},d=[{value:"API Reference",id:"api-reference",level:2},{value:"High Level Overview",id:"high-level-overview",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Voice Activity Detection (VAD) is the task of analyzing an audio signal to identify time segments containing human speech, separating them from non-speech sections like silence and background noise."}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-fsmn-vad",children:"Hugging Face repository"}),". You can also use ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,r.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For detailed API Reference for ",(0,r.jsx)(n.code,{children:"useVAD"})," see: ",(0,r.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useVAD",children:[(0,r.jsx)(n.code,{children:"useVAD"})," API Reference"]}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For all VAD models available out-of-the-box in React Native ExecuTorch see: ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---voice-activity-detection",children:"VAD Models"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"high-level-overview",children:"High Level Overview"}),"\n",(0,r.jsxs)(n.p,{children:["You can obtain waveform from audio in any way most suitable to you, however in the snippet below we utilize ",(0,r.jsx)(n.a,{href:"https://docs.swmansion.com/react-native-audio-api/",children:(0,r.jsx)(n.code,{children:"react-native-audio-api"})})," library to process a ",(0,r.jsx)(n.code,{children:".mp3"})," file."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { useVAD, FSMN_VAD } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst model = useVAD({\n  model: FSMN_VAD,\n});\n\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioDataSource(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\n\ntry {\n  // NOTE: to obtain segments in seconds, you need to divide\n  // start / end of the segment by the sampling rate (16k)\n\n  const speechSegments = await model.forward(audioBuffer);\n  console.log(speechSegments);\n} catch (error) {\n  console.error('Error during running VAD model', error);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"useVAD"})," takes ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADProps",children:(0,r.jsx)(n.code,{children:"VADProps"})})," that consists of:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model"})," containing ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADProps#modelsource",children:(0,r.jsx)(n.code,{children:"modelSource"})}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["An optional flag ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADProps#preventload",children:(0,r.jsx)(n.code,{children:"preventLoad"})})," which prevents auto-loading of the model."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You need more details? Check the following resources:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For detailed information about ",(0,r.jsx)(n.code,{children:"useVAD"})," arguments check this section: ",(0,r.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useVAD#parameters",children:[(0,r.jsx)(n.code,{children:"useVAD"})," arguments"]}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For all VAD models available out-of-the-box in React Native ExecuTorch see: ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---voice-activity-detection",children:"VAD Models"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"useVAD"})," returns an object called ",(0,r.jsx)(n.code,{children:"VADType"})," containing bunch of functions to interact with VAD models. To get more details please read: ",(0,r.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADType",children:[(0,r.jsx)(n.code,{children:"VADType"})," API Reference"]}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["Before running the model's ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADType#forward",children:(0,r.jsx)(n.code,{children:"forward"})})," method, make sure to extract the audio waveform you want to process. You'll need to handle this step yourself, ensuring the audio is sampled at 16 kHz. Once you have the waveform, pass it as an argument to the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/VADType#forward",children:(0,r.jsx)(n.code,{children:"forward"})})," method. The method returns a promise that resolves to the array of detected speech ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Segment",children:(0,r.jsx)(n.code,{children:"Segment[]"})}),"."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Timestamps in returned speech segments, correspond to indices of input array (waveform)."})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import React from 'react';\nimport { Button, Text, SafeAreaView } from 'react-native';\nimport { useVAD, FSMN_VAD } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nexport default function App() {\n  const model = useVAD({\n    model: FSMN_VAD,\n  });\n\n  const audioURL = 'https://some-audio-url.com/file.mp3';\n\n  const handleAudio = async () => {\n    if (!model) {\n      console.error('VAD model is not loaded yet.');\n      return;\n    }\n\n    console.log('Processing URL:', audioURL);\n\n    try {\n      const { uri } = await FileSystem.downloadAsync(\n        audioURL,\n        FileSystem.cacheDirectory + 'vad_example.tmp'\n      );\n\n      const audioContext = new AudioContext({ sampleRate: 16000 });\n      const originalDecodedBuffer =\n        await audioContext.decodeAudioDataSource(uri);\n      const originalChannelData = originalDecodedBuffer.getChannelData(0);\n\n      const segments = await model.forward(originalChannelData);\n      if (segments.length === 0) {\n        console.log('No speech segments were found.');\n        return;\n      }\n      console.log(`Found ${segments.length} speech segments.`);\n\n      const totalLength = segments.reduce(\n        (sum, seg) => sum + (seg.end - seg.start),\n        0\n      );\n      const newAudioBuffer = audioContext.createBuffer(\n        1, // Mono\n        totalLength,\n        originalDecodedBuffer.sampleRate\n      );\n      const newChannelData = newAudioBuffer.getChannelData(0);\n\n      let offset = 0;\n      for (const segment of segments) {\n        const slice = originalChannelData.subarray(segment.start, segment.end);\n        newChannelData.set(slice, offset);\n        offset += slice.length;\n      }\n\n      //  Play the processed audio\n      const source = audioContext.createBufferSource();\n      source.buffer = newAudioBuffer;\n      source.connect(audioContext.destination);\n      source.start();\n    } catch (error) {\n      console.error('Error processing audio data:', error);\n    }\n  };\n\n  return (\n    <SafeAreaView>\n      <Text>\n        Press the button to process and play speech from a sample file.\n      </Text>\n      <Button onPress={handleAudio} title=\"Run VAD Example\" />\n    </SafeAreaView>\n  );\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/collections/software-mansion/voice-activity-detection",children:"fsmn-vad"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[92889],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(96540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},31285:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"hooks/natural-language-processing/useSpeechToText","title":"useSpeechToText","description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook.","source":"@site/docs/03-hooks/01-natural-language-processing/useSpeechToText.md","sourceDirName":"03-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useSpeechToText","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-hooks/01-natural-language-processing/useSpeechToText.md","tags":[],"version":"current","frontMatter":{"title":"useSpeechToText","keywords":["speech to text","stt","voice recognition","transcription","whisper","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook."},"sidebar":"tutorialSidebar","previous":{"title":"useLLM","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useLLM"},"next":{"title":"useTextEmbeddings","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextEmbeddings"}}');var i=t(74848),s=t(28453);const o={title:"useSpeechToText",keywords:["speech to text","stt","voice recognition","transcription","whisper","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch's useSpeechToText hook."},a=void 0,c={},d=[{value:"API Reference",id:"api-reference",level:2},{value:"High Level Overview",id:"high-level-overview",level:2},{value:"Streaming",id:"streaming",level:3},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Timestamps &amp; Transcription Stat Data",id:"timestamps--transcription-stat-data",level:3},{value:"Example",id:"example",level:2},{value:"Streaming transcription",id:"streaming-transcription",level:3},{value:"Supported models",id:"supported-models",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Speech to text is a task that allows to transform spoken language to written text. It is commonly used to implement features such as transcription or voice assistants."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,i.jsx)(n.a,{href:"https://huggingface.co/collections/software-mansion/speech-to-text-68d0ec99ed794250491b8bbe",children:"Hugging Face repository"}),". You can also use ",(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,i.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For detailed API Reference for ",(0,i.jsx)(n.code,{children:"useSpeechToText"})," see: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useSpeechToText",children:[(0,i.jsx)(n.code,{children:"useSpeechToText"})," API Reference"]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For all speech to text models available out-of-the-box in React Native ExecuTorch see: ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---speech-to-text",children:"STT Models"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"high-level-overview",children:"High Level Overview"}),"\n",(0,i.jsxs)(n.p,{children:["You can obtain waveform from audio in any way most suitable to you, however in the snippet below we utilize ",(0,i.jsx)(n.a,{href:"https://docs.swmansion.com/react-native-audio-api/",children:(0,i.jsx)(n.code,{children:"react-native-audio-api"})})," library to process a ",(0,i.jsx)(n.code,{children:".mp3"})," file."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useSpeechToText, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst model = useSpeechToText({\n  model: WHISPER_TINY_EN,\n});\n\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioData(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\n\ntry {\n  const transcription = await model.transcribe(audioBuffer);\n  console.log(transcription.text);\n} catch (error) {\n  console.error('Error during audio transcription', error);\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,i.jsxs)(n.p,{children:["Since speech-to-text models can only process audio segments up to 30 seconds long, we need to split longer inputs into chunks. However, simple chunking may cut speech mid-sentence, making it harder for the model to understand. To address this, we use the ",(0,i.jsx)(n.a,{href:"https://aclanthology.org/2023.ijcnlp-demo.3.pdf",children:"whisper-streaming"})," algorithm. While this introduces some overhead, it enables accurate processing of audio inputs of arbitrary length."]}),"\n",(0,i.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"useSpeechToText"})," takes ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextProps",children:(0,i.jsx)(n.code,{children:"SpeechToTextProps"})})," that consists of:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"})," of type ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig",children:(0,i.jsx)(n.code,{children:"SpeechToTextConfig"})}),", containing the ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#ismultilingual",children:[(0,i.jsx)(n.code,{children:"isMultilingual"})," flag"]}),", ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#tokenizersource",children:"tokenizer source"}),", ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#encodersource",children:"encoder source"}),", and ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#decodersource",children:"decoder source"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["An optional flag ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextProps#preventload",children:(0,i.jsx)(n.code,{children:"preventLoad"})})," which prevents auto-loading of the model."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You need more details? Check the following resources:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For detailed information about ",(0,i.jsx)(n.code,{children:"useSpeechToText"})," arguments check this section: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useSpeechToText#parameters",children:[(0,i.jsx)(n.code,{children:"useSpeechToText"})," arguments"]})]}),"\n",(0,i.jsxs)(n.li,{children:["For all speech to text models available out-of-the-box in React Native ExecuTorch see: ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---speech-to-text",children:"STT Models"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For more information on loading resources, take a look at ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"useSpeechToText"})," returns an object called ",(0,i.jsx)(n.code,{children:"SpeechToTextType"})," containing bunch of functions to interact with STT."]}),"\n",(0,i.jsxs)(n.p,{children:["Please note, that both ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#transcribe",children:(0,i.jsx)(n.code,{children:"transcribe"})})," and ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#stream",children:(0,i.jsx)(n.code,{children:"stream"})})," functions accept ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions",children:(0,i.jsx)(n.code,{children:"DecodingOptions"})})," type as an argument. It accepts language abbreviation, you can check them out in ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions#language",children:(0,i.jsx)(n.code,{children:"language"})})," property of this config of type ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/type-aliases/SpeechToTextLanguage",children:(0,i.jsx)(n.code,{children:"SpeechToTextLanguage"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["To get more details please read: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType",children:[(0,i.jsx)(n.code,{children:"SpeechToTextType"})," API Reference"]}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,i.jsxs)(n.p,{children:["Before running the model's ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#transcribe",children:(0,i.jsx)(n.code,{children:"transcribe"})})," method, make sure to extract the audio waveform you want to transcribe. You'll need to handle this step yourself, ensuring the audio is sampled at 16 kHz. Once you have the waveform, pass it as an argument to the transcribe method. The method returns a promise that resolves to the generated transcription on success, or an error if inference fails."]}),"\n",(0,i.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,i.jsxs)(n.p,{children:["If you want to transcribe speech in languages other than English, use the multilingual version of Whisper. To generate the output in your desired language, pass the ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions#language",children:(0,i.jsx)(n.code,{children:"language"})})," option to the ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#transcribe",children:(0,i.jsx)(n.code,{children:"transcribe"})})," method."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useSpeechToText, WHISPER_TINY } from 'react-native-executorch';\n\nconst model = useSpeechToText({\n  model: WHISPER_TINY,\n});\n\nconst transcription = await model.transcribe(spanishAudio, { language: 'es' });\n"})}),"\n",(0,i.jsx)(n.h3,{id:"timestamps--transcription-stat-data",children:"Timestamps & Transcription Stat Data"}),"\n",(0,i.jsxs)(n.p,{children:["You can obtain word-level timestamps and other useful parameters from transcription (",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#transcribe",children:(0,i.jsx)(n.code,{children:"transcribe"})})," and ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#stream",children:(0,i.jsx)(n.code,{children:"stream"})})," methods) by setting ",(0,i.jsx)(n.code,{children:"verbose: true"})," in the options. The result mimics the ",(0,i.jsx)(n.em,{children:"verbose_json"})," format from OpenAI Whisper API. For more information please read ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#transcribe",children:(0,i.jsx)(n.code,{children:"transcribe"})}),", ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextType#stream",children:(0,i.jsx)(n.code,{children:"stream"})}),", and ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TranscriptionResult",children:(0,i.jsx)(n.code,{children:"TranscriptionResult"})})," API References."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:'const transcription = await model.transcribe(audioBuffer, { verbose: true });\n// Example result\n//\n// transcription: {\n//   task: "transcription",\n//   text: "Example text for a ...",\n//   duration: 9.05,\n//   language: "en",\n//   segments: [\n//     {\n//       start: 0,\n//       end: 5.4,\n//       text: "Example text for",\n//       words: [\n//         {\n//            word: "Example",\n//            start: 0,\n//            end: 1.4\n//         },\n//         ...\n//       ]\n//       tokens: [1, 32, 45, ...],\n//       temperature: 0.0,\n//       avgLogprob: -1.235,\n//       compressionRatio: 1.632\n//     },\n//     ...\n//   ]\n// }\n'})}),"\n",(0,i.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-tsx",children:"import React, { useState } from 'react';\nimport { Button, Text, View } from 'react-native';\nimport {\n  useSpeechToText,\n  WHISPER_TINY_EN,\n  TranscriptionResult,\n} from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nfunction App() {\n  const model = useSpeechToText({\n    model: WHISPER_TINY_EN,\n  });\n\n  const [transcription, setTranscription] = useState<TranscriptionResult>(null);\n\n  const loadAudio = async () => {\n    const { uri } = await FileSystem.downloadAsync(\n      'https://some-audio-url.com/file.mp3',\n      FileSystem.cacheDirectory + 'audio_file'\n    );\n\n    const audioContext = new AudioContext({ sampleRate: 16000 });\n    const decodedAudioData = await audioContext.decodeAudioDataSource(uri);\n    const audioBuffer = decodedAudioData.getChannelData(0);\n\n    return audioBuffer;\n  };\n\n  const handleTranscribe = async () => {\n    const audio = await loadAudio();\n    // Default text transcription\n    const result = await model.transcribe(audio);\n    setTranscription(result);\n  };\n\n  const handleTranscribeWithTimestamps = async () => {\n    const audio = await loadAudio();\n    // Transcription with timestamps\n    const result = await model.transcribe(audio, { verbose: true });\n    setTranscription(result);\n  };\n\n  // Custom logic for printing transcription\n  // e.g.\n\n  const renderContent = () => {\n    if (!transcription) return <Text>Press a button to transcribe</Text>;\n\n    if (transcription.segments && transcription.segments.length > 0) {\n      return (\n        <Text>\n          {transcription.text +\n            '\\n\\nNum segments: ' +\n            transcription.segments.length.toString()}\n        </Text>\n      );\n    }\n    return <Text>{transcription.text}</Text>;\n  };\n\n  return (\n    <View>\n      {renderContent()}\n      <Button onPress={handleTranscribe} title=\"Transcribe (Text)\" />\n      <Button\n        onPress={handleTranscribeWithTimestamps}\n        title=\"Transcribe (Timestamps)\"\n      />\n    </View>\n  );\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"streaming-transcription",children:"Streaming transcription"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-tsx",children:"import React, { useEffect, useState, useRef } from 'react';\nimport { Text, Button, View, SafeAreaView } from 'react-native';\nimport { useSpeechToText, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioManager, AudioRecorder } from 'react-native-audio-api';\n\nexport default function App() {\n  const model = useSpeechToText({\n    model: WHISPER_TINY_EN,\n  });\n\n  const [transcribedText, setTranscribedText] = useState('');\n\n  const isRecordingRef = useRef(false);\n\n  const [recorder] = useState(() => new AudioRecorder());\n\n  useEffect(() => {\n    AudioManager.setAudioSessionOptions({\n      iosCategory: 'playAndRecord',\n      iosMode: 'spokenAudio',\n      iosOptions: ['allowBluetooth', 'defaultToSpeaker'],\n    });\n    AudioManager.requestRecordingPermissions();\n  }, []);\n\n  const handleStartStreamingTranscribe = async () => {\n    isRecordingRef.current = true;\n    setTranscribedText('');\n\n    const sampleRate = 16000;\n\n    recorder.onAudioReady(\n      {\n        sampleRate,\n        bufferLength: 0.1 * sampleRate,\n        channelCount: 1,\n      },\n      (chunk) => {\n        model.streamInsert(chunk.buffer.getChannelData(0));\n      }\n    );\n\n    try {\n      await recorder.start();\n    } catch (e) {\n      console.error('Recorder failed:', e);\n      return;\n    }\n\n    try {\n      let accumulatedCommitted = '';\n\n      const streamIter = model.stream({ verbose: false });\n\n      for await (const { committed, nonCommitted } of streamIter) {\n        if (!isRecordingRef.current) break;\n\n        if (committed.text) {\n          accumulatedCommitted += committed.text;\n        }\n\n        setTranscribedText(accumulatedCommitted + nonCommitted.text);\n      }\n    } catch (error) {\n      console.error('Error during streaming transcription:', error);\n    }\n  };\n\n  const handleStopStreamingTranscribe = () => {\n    isRecordingRef.current = false;\n    recorder.stop();\n    model.streamStop();\n  };\n\n  return (\n    <SafeAreaView>\n      <View style={{ padding: 20 }}>\n        <Text style={{ marginBottom: 20, fontSize: 18 }}>\n          {transcribedText || 'Press start to speak...'}\n        </Text>\n\n        <Button\n          onPress={handleStartStreamingTranscribe}\n          title=\"Start Streaming\"\n          disabled={model.isGenerating}\n        />\n        <View style={{ height: 10 }} />\n        <Button\n          onPress={handleStopStreamingTranscribe}\n          title=\"Stop Streaming\"\n          color=\"red\"\n        />\n      </View>\n    </SafeAreaView>\n  );\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Language"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny.en",children:"whisper-tiny.en"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny",children:"whisper-tiny"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-base.en",children:"whisper-base.en"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-base",children:"whisper-base"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-small.en",children:"whisper-small.en"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-small",children:"whisper-small"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);
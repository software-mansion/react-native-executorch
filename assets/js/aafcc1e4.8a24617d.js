"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[2331],{10211:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"hooks/natural-language-processing/useTextEmbeddings","title":"useTextEmbeddings","description":"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch\'s useTextEmbeddings hook.","source":"@site/docs/03-hooks/01-natural-language-processing/useTextEmbeddings.md","sourceDirName":"03-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useTextEmbeddings","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextEmbeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-hooks/01-natural-language-processing/useTextEmbeddings.md","tags":[],"version":"current","frontMatter":{"title":"useTextEmbeddings","keywords":["text embedding","text embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch\'s useTextEmbeddings hook."},"sidebar":"tutorialSidebar","previous":{"title":"useSpeechToText","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText"},"next":{"title":"useTextToSpeech","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextToSpeech"}}');var i=t(74848),r=t(28453);const o={title:"useTextEmbeddings",keywords:["text embedding","text embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch's useTextEmbeddings hook."},d=void 0,c={},a=[{value:"API Reference",id:"api-reference",level:2},{value:"High Level Overview",id:"high-level-overview",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Text Embedding is the process of converting text into a numerical representation. This representation can be used for various natural language processing tasks, such as semantic search, text classification, and clustering."}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,i.jsx)(n.a,{href:"https://huggingface.co/collections/software-mansion/text-embeddings-68d0ed42f8ca0200d0283362",children:"Hugging Face repository"}),". You can also use ",(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,i.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For detailed API Reference for ",(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," see: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useTextEmbeddings",children:[(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," API Reference"]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For all text embeddings models available out-of-the-box in React Native ExecuTorch see: ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---text-embeddings",children:"Text Embeddings Models"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"high-level-overview",children:"High Level Overview"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useTextEmbeddings, ALL_MINILM_L6_V2 } from 'react-native-executorch';\n\nconst model = useTextEmbeddings({ model: ALL_MINILM_L6_V2 });\n\ntry {\n  const embedding = await model.forward('Hello World!');\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," takes ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsProps",children:(0,i.jsx)(n.code,{children:"TextEmbeddingsProps"})})," that consists of:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"})," of type ",(0,i.jsx)(n.code,{children:"object"})," containing the ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsProps#modelsource",children:"model source"})," and ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsProps#tokenizersource",children:"tokenizer source"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["An optional flag ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsProps#preventload",children:(0,i.jsx)(n.code,{children:"preventLoad"})})," which prevents auto-loading of the model."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You need more details? Check the following resources:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["For detailed information about ",(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," arguments check this section: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/functions/useTextEmbeddings#parameters",children:[(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," arguments"]}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For all text embeddings models available out-of-the-box in React Native ExecuTorch see: ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---text-embeddings",children:"Text Embeddings Models"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["For more information on loading resources, take a look at ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"useTextEmbeddings"})," returns an object called ",(0,i.jsx)(n.code,{children:"TextEmbeddingsType"})," containing bunch of functions to interact with text embedding. To get more details please read: ",(0,i.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsType",children:[(0,i.jsx)(n.code,{children:"TextEmbeddingsType"})," API Reference"]}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,i.jsxs)(n.p,{children:["To run the model, you can use the ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TextEmbeddingsType#forward",children:(0,i.jsx)(n.code,{children:"forward"})})," method. It accepts one argument, which is a string representing the text you want to embed. The function returns a promise, which can resolve either to an error or an array of numbers representing the embedding."]}),"\n",(0,i.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useTextEmbeddings, ALL_MINILM_L6_V2 } from 'react-native-executorch';\n\nconst dotProduct = (a: number[], b: number[]) =>\n  a.reduce((sum, val, i) => sum + val * b[i], 0);\n\nconst cosineSimilarity = (a: number[], b: number[]) => {\n  const dot = dotProduct(a, b);\n  const normA = Math.sqrt(dotProduct(a, a));\n  const normB = Math.sqrt(dotProduct(b, b));\n  return dot / (normA * normB);\n};\n\nfunction App() {\n  const model = useTextEmbeddings({ model: ALL_MINILM_L6_V2 });\n\n  // ...\n\n  try {\n    const helloWorldEmbedding = await model.forward('Hello World!');\n    const goodMorningEmbedding = await model.forward('Good Morning!');\n\n    const similarity = cosineSimilarity(\n      helloWorldEmbedding,\n      goodMorningEmbedding\n    );\n\n    console.log(`Cosine similarity: ${similarity}`);\n  } catch (error) {\n    console.error(error);\n  }\n\n  // ...\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Language"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Max Tokens"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Embedding Dimensions"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"254"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"384"}),(0,i.jsx)(n.td,{children:"All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-mpnet-base-v2",children:"all-mpnet-base-v2"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"382"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"768"}),(0,i.jsx)(n.td,{children:"All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1",children:"multi-qa-MiniLM-L6-cos-v1"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"509"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"384"}),(0,i.jsx)(n.td,{children:"This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1",children:"multi-qa-mpnet-base-dot-v1"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"510"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"768"}),(0,i.jsx)(n.td,{children:"This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/openai/clip-vit-base-patch32",children:"clip-vit-base-patch32-text"})}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"74"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,i.jsxs)(n.td,{children:["CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. CLIP allows to embed images and text into the same vector space. This allows to find similar images as well as to implement image search. This is the text encoder part of the CLIP model. To embed images checkout ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/computer-vision/useImageEmbeddings#supported-models",children:"clip-vit-base-patch32-image"}),"."]})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"Max Tokens"})})," - The maximum number of tokens that can be processed by the model. If the input text exceeds this limit, it will be truncated."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"Embedding Dimensions"})})," - The size of the output embedding vector. This is the number of dimensions in the vector representation of the input text."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"For the supported models, the returned embedding vector is normalized, meaning that its length is equal to 1. This allows for easier comparison of vectors using cosine similarity, just calculate the dot product of two vectors to get the cosine similarity score."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>d});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);
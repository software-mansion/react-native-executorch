"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[42745],{7865:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"hookless-api/OCRModule","title":"OCRModule","description":"Hookless implementation of the useOCR hook.","source":"@site/versioned_docs/version-0.3.x/hookless-api/OCRModule.md","sourceDirName":"hookless-api","slug":"/hookless-api/OCRModule","permalink":"/react-native-executorch/docs/0.3.x/hookless-api/OCRModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.3.x/hookless-api/OCRModule.md","tags":[],"version":"0.3.x","sidebarPosition":6,"frontMatter":{"title":"OCRModule","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"ObjectDetectionModule","permalink":"/react-native-executorch/docs/0.3.x/hookless-api/ObjectDetectionModule"},"next":{"title":"SpeechToTextModule","permalink":"/react-native-executorch/docs/0.3.x/hookless-api/SpeechToTextModule"}}');var r=o(74848),i=o(28453);const s={title:"OCRModule",sidebar_position:6},c=void 0,d={},a=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Listening for download progress",id:"listening-for-download-progress",level:2},{value:"Running the model",id:"running-the-model",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["Hookless implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.3.x/computer-vision/useOCR",children:"useOCR"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import {\n  OCRModule,\n  CRAFT_800,\n  RECOGNIZER_EN_CRNN_512,\n  RECOGNIZER_EN_CRNN_256,\n  RECOGNIZER_EN_CRNN_128,\n} from 'react-native-executorch';\nconst imageUri = 'path/to/image.png';\n\n// Loading the model\nawait OCRModule.load({\n  detectorSource: CRAFT_800,\n  recognizerSources: {\n    recognizerLarge: RECOGNIZER_EN_CRNN_512,\n    recognizerMedium: RECOGNIZER_EN_CRNN_256,\n    recognizerSmall: RECOGNIZER_EN_CRNN_128,\n  },\n  language: 'en',\n});\n\n// Running the model\nconst ocrDetections = await OCRModule.forward(imageUri);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"load"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(detectorSource: string, recognizerSources: RecognizerSources, language: OCRLanguage): Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Loads the detector and recognizers, which sources are represented by ",(0,r.jsx)(n.code,{children:"RecognizerSources"}),"."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"forward"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: string): Promise<OCRDetections[]>"})}),(0,r.jsxs)(n.td,{children:["Executes the model's forward pass, where ",(0,r.jsx)(n.code,{children:"input"})," can be a fetchable resource or a Base64-encoded string."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"onDownloadProgress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(callback: (downloadProgress: number) => void): any"})}),(0,r.jsx)(n.td,{children:"Subscribe to the download progress event."})]})]})]}),"\n",(0,r.jsxs)(o,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"interface RecognizerSources {\n  recognizerLarge: string | number;\n  recognizerMedium: string | number;\n  recognizerSmall: string | number;\n}\n\ntype OCRLanguage = 'en';\n\ninterface Point {\n  x: number;\n  y: number;\n}\n\ninterface OCRDetection {\n  bbox: Point[];\n  text: string;\n  score: number;\n}\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,r.jsxs)(n.p,{children:["To load the model, use the ",(0,r.jsx)(n.code,{children:"load"})," method. It accepts:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"detectorSource"})})," - A string that specifies the location of the detector binary. For more information, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.3.x/fundamentals/loading-models",children:"loading models"})," section."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"recognizerSources"})})," - An object that specifies locations of the recognizers binary files. Each recognizer is composed of three models tailored to process images of varying widths."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"recognizerLarge"})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 512 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"recognizerMedium"})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 256 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"recognizerSmall"})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 128 pixels."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For more information, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.3.x/fundamentals/loading-models",children:"loading models"})," section."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"language"})})," - A parameter that specifies the language of the text to be recognized by the OCR."]}),"\n",(0,r.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,r.jsx)(n.h2,{id:"listening-for-download-progress",children:"Listening for download progress"}),"\n",(0,r.jsxs)(n.p,{children:["To subscribe to the download progress event, you can use the ",(0,r.jsx)(n.code,{children:"onDownloadProgress"})," method. It accepts a callback function that will be called whenever the download progress changes."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.code,{children:"forward"})," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The method returns a promise, which can resolve either to an error or an array of ",(0,r.jsx)(n.code,{children:"OCRDetection"})," objects. Each object contains coordinates of the bounding box, the label of the detected object, and the confidence score."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>c});var t=o(96540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);
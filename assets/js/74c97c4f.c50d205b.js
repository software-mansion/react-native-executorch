"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1270],{1642:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>d,default:()=>a,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"llms/useLLM","title":"useLLM","description":"Learn how to use Llama models in your React Native applications with React Native ExecuTorch\'s useLLM hook.","source":"@site/versioned_docs/version-0.3.x/llms/useLLM.md","sourceDirName":"llms","slug":"/llms/useLLM","permalink":"/react-native-executorch/docs/0.3.x/llms/useLLM","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.3.x/llms/useLLM.md","tags":[],"version":"0.3.x","sidebarPosition":1,"frontMatter":{"title":"useLLM","sidebar_position":1,"keywords":["llm","large language model","llama","llama 3","react native","executorch","ai","machine learning","on-device","mobile ai","inference","text generation"],"description":"Learn how to use Llama models in your React Native applications with React Native ExecuTorch\'s useLLM hook."},"sidebar":"tutorialSidebar","previous":{"title":"LLMs","permalink":"/react-native-executorch/docs/0.3.x/category/llms"},"next":{"title":"Exporting Llama","permalink":"/react-native-executorch/docs/0.3.x/llms/exporting-llama"}}');var r=n(4848),i=n(8453);const l={title:"useLLM",sidebar_position:1,keywords:["llm","large language model","llama","llama 3","react native","executorch","ai","machine learning","on-device","mobile ai","inference","text generation"],description:"Learn how to use Llama models in your React Native applications with React Native ExecuTorch's useLLM hook."},d=void 0,o={},c=[{value:"Initializing",id:"initializing",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Sending a message",id:"sending-a-message",level:2},{value:"Listening for the response",id:"listening-for-the-response",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:"React Native ExecuTorch supports Llama 3.2 models, including quantized versions. Before getting started, you\u2019ll need to obtain the .pte binary\u2014a serialized model\u2014and the tokenizer. There are various ways to accomplish this:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["For your convienience, it's best if you use models exported by us, you can get them from our ",(0,r.jsx)(t.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-llama-3.2",children:"HuggingFace repository"}),". You can also use ",(0,r.jsx)(t.a,{href:"https://github.com/software-mansion/react-native-executorch/tree/main/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]}),"\n",(0,r.jsxs)(t.li,{children:["If you want to export model by yourself, you can use a Docker image that we've prepared. To see how it works, check out ",(0,r.jsx)(t.a,{href:"./exporting-llama",children:"exporting Llama"})]}),"\n",(0,r.jsxs)(t.li,{children:["Follow the official ",(0,r.jsx)(t.a,{href:"https://github.com/pytorch/executorch/blob/fe20be98c/examples/demo-apps/android/LlamaDemo/docs/delegates/xnnpack_README.md",children:"tutorial"})," made by ExecuTorch team to build the model and tokenizer yourself"]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"initializing",children:"Initializing"}),"\n",(0,r.jsx)(t.p,{children:"In order to load a model into the app, you need to run the following code:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"import { useLLM, LLAMA3_2_1B } from 'react-native-executorch';\n\nconst messageHistory = [\n  { role: 'user', content: 'Hello' },\n  { role: 'assistant', content: 'Hi, how can I help you?' },\n];\n\nconst llama = useLLM({\n  modelSource: LLAMA3_2_1B,\n  tokenizerSource: require('../assets/tokenizer.bin'),\n  systemPrompt: 'Be a helpful assistant',\n  messageHistory: messageHistory,\n  contextWindowLength: 3,\n});\n"})}),"\n",(0,r.jsxs)(n,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"const useLLM: ({\n  modelSource,\n  tokenizerSource,\n  systemPrompt,\n  messageHistory,\n  contextWindowLength,\n}: {\n  modelSource: ResourceSource;\n  tokenizerSource: ResourceSource;\n  systemPrompt?: string;\n  messageHistory?: MessageType[];\n  contextWindowLength?: number;\n}) => Model;\n\ninterface Model {\n  generate: (input: string) => Promise<void>;\n  response: string;\n  downloadProgress: number;\n  error: string | null;\n  isModelGenerating: boolean;\n  isGenerating: boolean;\n  isModelReady: boolean;\n  isReady: boolean;\n  interrupt: () => void;\n}\n\ntype ResourceSource = string | number;\n\ninterface MessageType {\n  role: 'user' | 'assistant';\n  content: string;\n}\n"})})]}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsxs)(t.p,{children:["The code snippet above fetches the model from the specified URL, loads it into memory, and returns an object with various methods and properties for controlling the model. You can monitor the loading progress by checking the ",(0,r.jsx)(t.code,{children:"llama.downloadProgress"})," and ",(0,r.jsx)(t.code,{children:"llama.isReady"})," property, and if anything goes wrong, the ",(0,r.jsx)(t.code,{children:"llama.error"})," property will contain the error message."]}),"\n",(0,r.jsx)(t.admonition,{type:"danger",children:(0,r.jsx)(t.p,{children:"Lower-end devices might not be able to fit LLMs into memory. We recommend using quantized models to reduce the memory footprint."})}),"\n",(0,r.jsx)(t.admonition,{type:"caution",children:(0,r.jsxs)(t.p,{children:["Given computational constraints, our architecture is designed to support only one instance of the model runner at the time. Consequently, this means you can have only one active component leveraging ",(0,r.jsx)(t.code,{children:"useLLM"})," concurrently."]})}),"\n",(0,r.jsx)(t.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.code,{children:"modelSource"})})," - A string that specifies the location of the model binary. For more information, take a look at ",(0,r.jsx)(t.a,{href:"/react-native-executorch/docs/0.3.x/fundamentals/loading-models",children:"loading models"})," section."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.code,{children:"tokenizerSource"})})," - URL to the binary file which contains the tokenizer"]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.code,{children:"systemPrompt"})}),' - Often used to tell the model what is its purpose, for example - "Be a helpful translator"']}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.code,{children:"messageHistory"})})," - An array of ",(0,r.jsx)(t.code,{children:"MessageType"})," objects that represent the conversation history. This can be used to provide context to the model."]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:(0,r.jsx)(t.code,{children:"contextWindowLength"})})," - The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["Make sure that the reference to the ",(0,r.jsx)(t.code,{children:"messageHistory"})," array is stable. Depending on your use case, you might use ",(0,r.jsx)(t.code,{children:"useState"})," or ",(0,r.jsx)(t.code,{children:"useRef"})," to store the message history."]})}),"\n",(0,r.jsx)(t.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Field"}),(0,r.jsx)(t.th,{children:"Type"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"generate"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"(input: string) => Promise<void>"})}),(0,r.jsx)(t.td,{children:"Function to start generating a response with the given input string."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"response"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"string"})}),(0,r.jsx)(t.td,{children:"State of the generated response. This field is updated with each token generated by the model"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"error"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)("code",{children:"string | null"})}),(0,r.jsx)(t.td,{children:"Contains the error message if the model failed to load"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"isGenerating"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"boolean"})}),(0,r.jsx)(t.td,{children:"Indicates whether the model is currently generating a response"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"interrupt"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"() => void"})}),(0,r.jsx)(t.td,{children:"Function to interrupt the current inference"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"isReady"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"boolean"})}),(0,r.jsx)(t.td,{children:"Indicates whether the model is ready"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"downloadProgress"})}),(0,r.jsx)(t.td,{children:(0,r.jsx)(t.code,{children:"number"})}),(0,r.jsx)(t.td,{children:"Represents the download progress as a value between 0 and 1, indicating the extent of the model file retrieval."})]})]})]}),"\n",(0,r.jsx)(t.h2,{id:"sending-a-message",children:"Sending a message"}),"\n",(0,r.jsx)(t.p,{children:"In order to send a message to the model, one can use the following code:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"const llama = useLLM({\n    modelSource: LLAMA3_2_1B,\n    tokenizerSource: require('../assets/tokenizer.bin'),\n});\n\n...\nconst message = 'Hi, who are you?';\nawait llama.generate(message);\n...\n"})}),"\n",(0,r.jsx)(t.h2,{id:"listening-for-the-response",children:"Listening for the response"}),"\n",(0,r.jsxs)(t.p,{children:["As you might've noticed, there is no return value from the ",(0,r.jsx)(t.code,{children:"runInference"})," function. Instead, the ",(0,r.jsx)(t.code,{children:".response"})," field of the model is updated with each token.\nThis is how you can render the response of the model:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"...\nreturn (\n    <Text>{llama.response}</Text>\n)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Behind the scenes, tokens are generated one by one, and the response property is updated with each token as it\u2019s created. This means that the text component will re-render whenever llama.response gets updated."}),"\n",(0,r.jsxs)(t.p,{children:["Sometimes, you might want to stop the model while it\u2019s generating. To do this, you can use ",(0,r.jsx)(t.code,{children:"interrupt()"}),", which will halt the model and append the current response to its internal conversation state."]}),"\n",(0,r.jsxs)(t.p,{children:["There are also cases when you need to check if tokens are being generated, such as to conditionally render a stop button. We\u2019ve made this easy with the ",(0,r.jsx)(t.code,{children:"isTokenBeingGenerated"})," property."]}),"\n",(0,r.jsx)(t.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(t.h3,{id:"model-size",children:"Model size"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Model"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"XNNPACK [GB]"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2.47"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"1.14"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"1.18"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"6.43"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2.55"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2.65"})]})]})]}),"\n",(0,r.jsx)(t.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Model"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [GB]"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [GB]"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"3.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"3.1"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"1.9"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"2.5"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"7.1"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"7.3"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"3.7"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"3.8"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"4"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"4.1"})]})]})]}),"\n",(0,r.jsx)(t.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Model"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [tokens/s]"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone 13 Pro (XNNPACK) [tokens/s]"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [tokens/s]"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [tokens/s]"}),(0,r.jsx)(t.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [tokens/s]"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"16.1"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"11.4"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"15.6"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"19.3"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"40.6"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"16.7"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"16.5"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"40.3"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"48.2"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"31.8"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"11.4"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"11.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"37.3"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"44.4"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"7.1"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"17.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"8.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"16.2"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"19.4"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"14.5"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"14.8"}),(0,r.jsx)(t.td,{style:{textAlign:"center"},children:"18.1"})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"\u274c - Insufficient RAM."})]})}function a(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>d});var s=n(6540);const r={},i=s.createContext(r);function l(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);
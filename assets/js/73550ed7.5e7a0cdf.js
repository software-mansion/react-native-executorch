"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9613],{7824:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"hooks/natural-language-processing/useSpeechToText","title":"useSpeechToText","description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook.","source":"@site/versioned_docs/version-0.5.x/02-hooks/01-natural-language-processing/useSpeechToText.md","sourceDirName":"02-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useSpeechToText","permalink":"/react-native-executorch/docs/hooks/natural-language-processing/useSpeechToText","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.5.x/02-hooks/01-natural-language-processing/useSpeechToText.md","tags":[],"version":"0.5.x","frontMatter":{"title":"useSpeechToText","keywords":["speech to text","stt","voice recognition","transcription","whisper","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch\'s useSpeechToText hook."},"sidebar":"tutorialSidebar","previous":{"title":"useLLM","permalink":"/react-native-executorch/docs/hooks/natural-language-processing/useLLM"},"next":{"title":"useTextEmbeddings","permalink":"/react-native-executorch/docs/hooks/natural-language-processing/useTextEmbeddings"}}');var s=t(4848),i=t(8453);const o={title:"useSpeechToText",keywords:["speech to text","stt","voice recognition","transcription","whisper","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use speech-to-text models in your React Native applications with React Native ExecuTorch's useSpeechToText hook."},c=void 0,a={},d=[{value:"Reference",id:"reference",level:2},{value:"Streaming",id:"streaming",level:3},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Example",id:"example",level:2},{value:"Streaming transcription",id:"streaming-transcription",level:3},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Speech to text is a task that allows to transform spoken language to written text. It is commonly used to implement features such as transcription or voice assistants."}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,s.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-whisper-tiny.en",children:"Hugging Face repository"}),". You can also use ",(0,s.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsxs)(n.p,{children:["You can obtain waveform from audio in any way most suitable to you, however in the snippet below we utilize ",(0,s.jsx)(n.code,{children:"react-native-audio-api"})," library to process a ",(0,s.jsx)(n.code,{children:".mp3"})," file."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { useSpeechToText, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst model = useSpeechToText({\n  model: WHISPER_TINY_EN,\n});\n\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioDataSource(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\nconst audioArray = Array.from(audioBuffer);\n\ntry {\n  const transcription = await model.transcribe(audioArray);\n  console.log(transcription);\n} catch (error) {\n  console.error('Error during audio transcription', error);\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,s.jsxs)(n.p,{children:["Since speech-to-text models can only process audio segments up to 30 seconds long, we need to split longer inputs into chunks. However, simple chunking may cut speech mid-sentence, making it harder for the model to understand. To address this, we use the ",(0,s.jsx)(n.a,{href:"https://aclanthology.org/2023.ijcnlp-demo.3.pdf",children:"whisper-streaming"})," algorithm. While this introduces some overhead, it enables accurate processing of audio inputs of arbitrary length."]}),"\n",(0,s.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"model"})})," - Object containing:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"isMultilingual"})})," - A boolean flag indicating whether the model supports multiple languages."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"encoderSource"})})," - A string that specifies the location of a ",(0,s.jsx)(n.code,{children:".pte"})," file for the encoder."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"decoderSource"})})," - A string that specifies the location of a ",(0,s.jsx)(n.code,{children:".pte"})," file for the decoder."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"tokenizerSource"})})," - A string that specifies the location to the tokenizer for the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,s.jsx)(n.a,{href:"/react-native-executorch/docs/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,s.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Field"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"transcribe"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(waveform: number[], options?: DecodingOptions | undefined) => Promise<string>"})}),(0,s.jsxs)(n.td,{children:["Starts a transcription process for a given input array, which should be a waveform at 16kHz. The second argument is an options object, e.g. ",(0,s.jsx)(n.code,{children:"{ language: 'es' }"})," for multilingual models. Resolves a promise with the output transcription when the model is finished."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"stream"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"() => Promise<string>"})}),(0,s.jsxs)(n.td,{children:["Starts a streaming transcription process. Use in combination with ",(0,s.jsx)(n.code,{children:"streamInsert"})," to feed audio chunks and ",(0,s.jsx)(n.code,{children:"streamStop"})," to end the stream. Updates ",(0,s.jsx)(n.code,{children:"committedTranscription"})," and ",(0,s.jsx)(n.code,{children:"nonCommittedTranscription"})," as transcription progresses."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"streamInsert"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(waveform: number[]) => void"})}),(0,s.jsx)(n.td,{children:"Inserts a chunk of audio data (sampled at 16kHz) into the ongoing streaming transcription. Call this repeatedly as new audio data becomes available."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"streamStop"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"() => void"})}),(0,s.jsx)(n.td,{children:"Stops the ongoing streaming transcription process."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"encode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(waveform: Float32Array) => Promise<void>"})}),(0,s.jsx)(n.td,{children:"Runs the encoding part of the model on the provided waveform. Stores the result internally."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"decode"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(tokens: number[]) => Promise<Float32Array>"})}),(0,s.jsx)(n.td,{children:"Runs the decoder of the model. Returns the decoded waveform as a Float32Array."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"committedTranscription"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Contains the part of the transcription that is finalized and will not change. Useful for displaying stable results during streaming."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"nonCommittedTranscription"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string"})}),(0,s.jsx)(n.td,{children:"Contains the part of the transcription that is still being processed and may change. Useful for displaying live, partial results during streaming."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"error"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"string | null"})}),(0,s.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isGenerating"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isReady"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"downloadProgress"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"number"})}),(0,s.jsx)(n.td,{children:"Tracks the progress of the model download process."})]})]})]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:"Type definitions"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Languages supported by whisper (Multilingual)\ntype SpeechToTextLanguage =\n  | 'af'\n  | 'sq'\n  | 'ar'\n  | 'hy'\n  | 'az'\n  | 'eu'\n  | 'be'\n  | 'bn'\n  | 'bs'\n  | 'bg'\n  | 'my'\n  | 'ca'\n  | 'zh'\n  | 'hr'\n  | 'cs'\n  | 'da'\n  | 'nl'\n  | 'et'\n  | 'en'\n  | 'fi'\n  | 'fr'\n  | 'gl'\n  | 'ka'\n  | 'de'\n  | 'el'\n  | 'gu'\n  | 'ht'\n  | 'he'\n  | 'hi'\n  | 'hu'\n  | 'is'\n  | 'id'\n  | 'it'\n  | 'ja'\n  | 'kn'\n  | 'kk'\n  | 'km'\n  | 'ko'\n  | 'lo'\n  | 'lv'\n  | 'lt'\n  | 'mk'\n  | 'mg'\n  | 'ms'\n  | 'ml'\n  | 'mt'\n  | 'mr'\n  | 'ne'\n  | 'no'\n  | 'fa'\n  | 'pl'\n  | 'pt'\n  | 'pa'\n  | 'ro'\n  | 'ru'\n  | 'sr'\n  | 'si'\n  | 'sk'\n  | 'sl'\n  | 'es'\n  | 'su'\n  | 'sw'\n  | 'sv'\n  | 'tl'\n  | 'tg'\n  | 'ta'\n  | 'te'\n  | 'th'\n  | 'tr'\n  | 'uk'\n  | 'ur'\n  | 'uz'\n  | 'vi'\n  | 'cy'\n  | 'yi';\n\ninterface DecodingOptions {\n  language?: SpeechToTextLanguage;\n}\n\ninterface SpeechToTextModelConfig {\n  isMultilingual: boolean;\n  encoderSource: ResourceSource;\n  decoderSource: ResourceSource;\n  tokenizerSource: ResourceSource;\n}\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,s.jsxs)(n.p,{children:["Before running the model's ",(0,s.jsx)(n.code,{children:"transcribe"})," method, make sure to extract the audio waveform you want to transcribe. You'll need to handle this step yourself, ensuring the audio is sampled at 16 kHz. Once you have the waveform, pass it as an argument to the transcribe method. The method returns a promise that resolves to the generated transcription on success, or an error if inference fails."]}),"\n",(0,s.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,s.jsxs)(n.p,{children:["If you want to transcribe speech in languages other than English, use the multilingual version of Whisper. To generate the output in your desired language, pass the ",(0,s.jsx)(n.code,{children:"language"})," option to the ",(0,s.jsx)(n.code,{children:"transcribe"})," method."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { useSpeechToText, WHISPER_TINY } from 'react-native-executorch';\n\nconst model = useSpeechToText({\n  model: WHISPER_TINY,\n});\n\nconst transcription = await model.transcribe(spanishAudio, { language: 'es' });\n"})}),"\n",(0,s.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import React, { useState } from 'react';\nimport { Button, Text } from 'react-native';\nimport { useSpeechToText, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nfunction App() {\n  const model = useSpeechToText({\n    model: WHISPER_TINY_EN,\n  });\n\n  const [transcription, setTranscription] = useState('');\n\n  const loadAudio = async () => {\n    const { uri } = await FileSystem.downloadAsync(\n      'https://some-audio-url.com/file.mp3',\n      FileSystem.cacheDirectory + 'audio_file'\n    );\n\n    const audioContext = new AudioContext({ sampleRate: 16000 });\n    const decodedAudioData = await audioContext.decodeAudioDataSource(uri);\n    const audioBuffer = decodedAudioData.getChannelData(0);\n\n    return Array.from(audioBuffer);\n  };\n\n  const handleTranscribe = async () => {\n    const audio = await loadAudio();\n    await model.transcribe(audio);\n  };\n\n  return (\n    <>\n      <Text>{transcription}</Text>\n      <Button onPress={handleTranscribe} title=\"Transcribe\" />\n    </>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"streaming-transcription",children:"Streaming transcription"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import React, { useEffect, useState } from 'react';\nimport { Text, Button } from 'react-native';\nimport { useSpeechToText, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioManager, AudioRecorder } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nfunction App() {\n  const model = useSpeechToText({\n    model: WHISPER_TINY_EN,\n  });\n\n  const [recorder] = useState(\n    () =>\n      new AudioRecorder({\n        sampleRate: 16000,\n        bufferLengthInSamples: 1600,\n      })\n  );\n\n  useEffect(() => {\n    AudioManager.setAudioSessionOptions({\n      iosCategory: 'playAndRecord',\n      iosMode: 'spokenAudio',\n      iosOptions: ['allowBluetooth', 'defaultToSpeaker'],\n    });\n    AudioManager.requestRecordingPermissions();\n  }, []);\n\n  const handleStartStreamingTranscribe = async () => {\n    recorder.onAudioReady(async ({ buffer }) => {\n      const bufferArray = Array.from(buffer.getChannelData(0));\n      model.streamInsert(bufferArray);\n    });\n    recorder.start();\n\n    try {\n      await model.stream();\n    } catch (error) {\n      console.error('Error during streaming transcription:', error);\n    }\n  };\n\n  const handleStopStreamingTranscribe = async () => {\n    recorder.stop();\n    model.streamStop();\n  };\n\n  return (\n    <>\n      <Text>\n        {model.committedTranscription}\n        {model.nonCommittedTranscription}\n      </Text>\n      <Button\n        onPress={handleStartStreamingTranscribe}\n        title=\"Start Streaming\"\n      />\n      <Button onPress={handleStopStreamingTranscribe} title=\"Stop Streaming\" />\n    </>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Language"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny.en",children:"whisper-tiny.en"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-tiny",children:"whisper-tiny"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-base.en",children:"whisper-base.en"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-base",children:"whisper-base"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-small.en",children:"whisper-small.en"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"English"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/whisper-small",children:"whisper-small"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Multilingual"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,s.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_TINY_EN"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"151"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_TINY"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"151"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_BASE_EN"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"290.6"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_BASE"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"290.6"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_SMALL_EN"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"968"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_SMALL"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"968"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"WHISPER_TINY"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"900"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"600"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var r=t(6540);const s={},i=r.createContext(s);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7575],{8132:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"hooks/natural-language-processing/useVAD","title":"useVAD","description":"Voice Activity Detection (VAD) is the task of analyzing an audio signal to identify time segments containing human speech, separating them from non-speech sections like silence and background noise.","source":"@site/versioned_docs/version-0.6.0/02-hooks/01-natural-language-processing/useVAD.md","sourceDirName":"02-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useVAD","permalink":"/react-native-executorch/docs/hooks/natural-language-processing/useVAD","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.6.0/02-hooks/01-natural-language-processing/useVAD.md","tags":[],"version":"0.6.0","frontMatter":{"title":"useVAD"},"sidebar":"tutorialSidebar","previous":{"title":"useTokenizer","permalink":"/react-native-executorch/docs/hooks/natural-language-processing/useTokenizer"},"next":{"title":"Computer Vision","permalink":"/react-native-executorch/docs/category/computer-vision"}}');var r=t(4848),o=t(8453);const i={title:"useVAD"},a=void 0,d={},l=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Voice Activity Detection (VAD) is the task of analyzing an audio signal to identify time segments containing human speech, separating them from non-speech sections like silence and background noise."}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/collections/software-mansion/speech-to-text-68d0ec99ed794250491b8bbe",children:"Hugging Face repository"}),". You can also use ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsxs)(n.p,{children:["You can obtain waveform from audio in any way most suitable to you, however in the snippet below we utilize ",(0,r.jsx)(n.code,{children:"react-native-audio-api"})," library to process a ",(0,r.jsx)(n.code,{children:".mp3"})," file."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { useVAD, FSMN_VAD } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst model = useVAD({\n  model: FSMN_VAD,\n});\n\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioDataSource(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\n\ntry {\n  // NOTE: to obtain segments in seconds, you need to divide\n  // start / end of the segment by the sampling rate (16k)\n\n  const speechSegments = await model.forward(audioBuffer);\n  console.log(speechSegments);\n} catch (error) {\n  console.error('Error during running VAD model', error);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"modelSource"})})," - A string that specifies the location of the model binary."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"forward"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(waveform: Float32Array) => Promise<{Segment[]}>"})}),(0,r.jsxs)(n.td,{children:["Executes the model's forward pass, where input array should be a waveform at 16kHz. Returns a promise containing an array of ",(0,r.jsx)(n.code,{children:"Segment"})," objects."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"error"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"string | null"})}),(0,r.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isGenerating"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isReady"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"downloadProgress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"number"})}),(0,r.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"interface Segment {\n  start: number;\n  end: number;\n}\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["Before running the model's ",(0,r.jsx)(n.code,{children:"forward"})," method, make sure to extract the audio waveform you want to process. You'll need to handle this step yourself, ensuring the audio is sampled at 16 kHz. Once you have the waveform, pass it as an argument to the forward method. The method returns a promise that resolves to the array of detected speech segments."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Timestamps in returned speech segments, correspond to indices of input array (waveform)."})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import React from 'react';\nimport { Button, Text, SafeAreaView } from 'react-native';\nimport { useVAD, FSMN_VAD } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nexport default function App() {\n  const model = useVAD({\n    model: FSMN_VAD,\n  });\n\n  const audioURL = 'https://some-audio-url.com/file.mp3';\n\n  const handleAudio = async () => {\n    if (!model) {\n      console.error('VAD model is not loaded yet.');\n      return;\n    }\n\n    console.log('Processing URL:', audioURL);\n\n    try {\n      const { uri } = await FileSystem.downloadAsync(\n        audioURL,\n        FileSystem.cacheDirectory + 'vad_example.tmp'\n      );\n\n      const audioContext = new AudioContext({ sampleRate: 16000 });\n      const originalDecodedBuffer =\n        await audioContext.decodeAudioDataSource(uri);\n      const originalChannelData = originalDecodedBuffer.getChannelData(0);\n\n      const segments = await model.forward(originalChannelData);\n      if (segments.length === 0) {\n        console.log('No speech segments were found.');\n        return;\n      }\n      console.log(`Found ${segments.length} speech segments.`);\n\n      const totalLength = segments.reduce(\n        (sum, seg) => sum + (seg.end - seg.start),\n        0\n      );\n      const newAudioBuffer = audioContext.createBuffer(\n        1, // Mono\n        totalLength,\n        originalDecodedBuffer.sampleRate\n      );\n      const newChannelData = newAudioBuffer.getChannelData(0);\n\n      let offset = 0;\n      for (const segment of segments) {\n        const slice = originalChannelData.subarray(segment.start, segment.end);\n        newChannelData.set(slice, offset);\n        offset += slice.length;\n      }\n\n      //  Play the processed audio\n      const source = audioContext.createBufferSource();\n      source.buffer = newAudioBuffer;\n      source.connect(audioContext.destination);\n      source.start();\n    } catch (error) {\n      console.error('Error processing audio data:', error);\n    }\n  };\n\n  return (\n    <SafeAreaView>\n      <Text>\n        Press the button to process and play speech from a sample file.\n      </Text>\n      <Button onPress={handleAudio} title=\"Run VAD Example\" />\n    </SafeAreaView>\n  );\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/funasr/fsmn-vad",children:"fsmn-vad"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FSMN_VAD"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1.83"})]})})]}),"\n",(0,r.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FSMN_VAD"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"97"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"45,9"})]})})]}),"\n",(0,r.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,r.jsx)(n.admonition,{title:"warning",type:"warning",children:(0,r.jsx)(n.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization."})}),"\n",(0,r.jsxs)(n.p,{children:["Inference time were measured on a 60s audio, that can be found ",(0,r.jsx)(n.a,{href:"https://models.silero.ai/vad_models/en.wav",children:"here"}),"."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 14 Pro Max (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FSMN_VAD"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"151"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"171"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"180"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"109"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(6540);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);
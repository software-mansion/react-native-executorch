"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7131],{28453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>s});var i=t(96540);const r={},o=i.createContext(r);function c(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),i.createElement(o.Provider,{value:n},e.children)}},46919:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"typescript-api/computer-vision/VerticalOCRModule","title":"VerticalOCRModule","description":"TypeScript API implementation of the useVerticalOCR hook.","source":"@site/versioned_docs/version-0.5.x/03-typescript-api/02-computer-vision/VerticalOCRModule.md","sourceDirName":"03-typescript-api/02-computer-vision","slug":"/typescript-api/computer-vision/VerticalOCRModule","permalink":"/react-native-executorch/docs/0.5.x/typescript-api/computer-vision/VerticalOCRModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.5.x/03-typescript-api/02-computer-vision/VerticalOCRModule.md","tags":[],"version":"0.5.x","frontMatter":{"title":"VerticalOCRModule"},"sidebar":"tutorialSidebar","previous":{"title":"StyleTransferModule","permalink":"/react-native-executorch/docs/0.5.x/typescript-api/computer-vision/StyleTransferModule"},"next":{"title":"ExecuTorch Bindings","permalink":"/react-native-executorch/docs/0.5.x/category/executorch-bindings-1"}}');var r=t(74848),o=t(28453);const c={title:"VerticalOCRModule"},s=void 0,a={},d=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.5.x/hooks/computer-vision/useVerticalOCR",children:"useVerticalOCR"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import {\n  VerticalOCRModule,\n  VERTICAL_OCR_ENGLISH,\n} from 'react-native-executorch';\n\nconst imageUri = 'path/to/image.png';\n\n// Creating an instance\nconst verticalOCRModule = new VerticalOCRModule();\n\n// Loading the model\nawait verticalOCRModule.load(VERTICAL_OCR_ENGLISH);\n\n// Running the model\nconst detections = await verticalOCRModule.forward(imageUri);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"load"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(model: { detectorLarge: ResourceSource; detectorNarrow: ResourceSource; recognizerLarge: ResourceSource; recognizerSmall: ResourceSource; language: OCRLanguage }, independentCharacters: boolean, onDownloadProgressCallback?: (progress: number) => void): Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Loads the model, where ",(0,r.jsx)(n.code,{children:"detectorLarge"})," is a string that specifies the location of the recognizer binary file which accepts input images with a width of 1280 pixels, ",(0,r.jsx)(n.code,{children:"detectorNarrow"})," is a string that specifies the location of the detector binary file which accepts input images with a width of 320 pixels, ",(0,r.jsx)(n.code,{children:"recognizerLarge"})," is a string that specifies the location of the recognizer binary file which accepts input images with a width of 512 pixels, ",(0,r.jsx)(n.code,{children:"recognizerSmall"})," is a string that specifies the location of the recognizer binary file which accepts input images with a width of 64 pixels, and ",(0,r.jsx)(n.code,{children:"language"})," is a parameter that specifies the language of the text to be recognized by the OCR."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"forward"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: string): Promise<OCRDetections[]>"})}),(0,r.jsxs)(n.td,{children:["Executes the model's forward pass, where ",(0,r.jsx)(n.code,{children:"input"})," can be a fetchable resource or a Base64-encoded string."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"delete"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(): void"})}),(0,r.jsxs)(n.td,{children:["Release the memory held by the module. Calling ",(0,r.jsx)(n.code,{children:"forward"})," afterwards is invalid. Note that you cannot delete model while it's generating."]})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"interface DetectorSources {\n  detectorLarge: string | number;\n  detectorNarrow: string | number;\n}\n\ninterface RecognizerSources {\n  recognizerLarge: string | number;\n  recognizerSmall: string | number;\n}\n\ntype OCRLanguage =\n  | 'abq'\n  | 'ady'\n  | 'af'\n  | 'ava'\n  | 'az'\n  | 'be'\n  | 'bg'\n  | 'bs'\n  | 'chSim'\n  | 'che'\n  | 'cs'\n  | 'cy'\n  | 'da'\n  | 'dar'\n  | 'de'\n  | 'en'\n  | 'es'\n  | 'et'\n  | 'fr'\n  | 'ga'\n  | 'hr'\n  | 'hu'\n  | 'id'\n  | 'inh'\n  | 'ic'\n  | 'it'\n  | 'ja'\n  | 'kbd'\n  | 'kn'\n  | 'ko'\n  | 'ku'\n  | 'la'\n  | 'lbe'\n  | 'lez'\n  | 'lt'\n  | 'lv'\n  | 'mi'\n  | 'mn'\n  | 'ms'\n  | 'mt'\n  | 'nl'\n  | 'no'\n  | 'oc'\n  | 'pi'\n  | 'pl'\n  | 'pt'\n  | 'ro'\n  | 'ru'\n  | 'rsCyrillic'\n  | 'rsLatin'\n  | 'sk'\n  | 'sl'\n  | 'sq'\n  | 'sv'\n  | 'sw'\n  | 'tab'\n  | 'te'\n  | 'th'\n  | 'tjk'\n  | 'tl'\n  | 'tr'\n  | 'uk'\n  | 'uz'\n  | 'vi';\n\ninterface Point {\n  x: number;\n  y: number;\n}\n\ninterface OCRDetection {\n  bbox: Point[];\n  text: string;\n  score: number;\n}\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,r.jsxs)(n.p,{children:["To load the model, use the ",(0,r.jsx)(n.code,{children:"load"})," method. It accepts:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing the detector sources, recognizer sources, and language."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"detectorLarge"})})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 1280 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"detectorNarrow"})})," - A string that specifies the location of the detector binary file which accepts input images with a width of 320 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"recognizerLarge"})})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 512 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"recognizerSmall"})})," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 64 pixels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"language"})})," - A parameter that specifies the language of the text to be recognized by the OCR."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"independentCharacters"})})," \u2013 A boolean parameter that indicates whether the text in the image consists of a random sequence of characters. If set to true, the algorithm will scan each character individually instead of reading them as continuous text."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - (Optional) Function called on download progress."]}),"\n",(0,r.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.5.x/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.code,{children:"forward"})," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The method returns a promise, which can resolve either to an error or an array of ",(0,r.jsx)(n.code,{children:"OCRDetection"})," objects. Each object contains coordinates of the bounding box, the label of the detected object, and the confidence score."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6975],{5486:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"hooks/executorch-bindings/useExecutorchModule","title":"useExecutorchModule","description":"useExecutorchModule provides React Native bindings to the ExecuTorch Module API directly from JavaScript.","source":"@site/docs/02-hooks/03-executorch-bindings/useExecutorchModule.md","sourceDirName":"02-hooks/03-executorch-bindings","slug":"/hooks/executorch-bindings/useExecutorchModule","permalink":"/react-native-executorch/docs/next/hooks/executorch-bindings/useExecutorchModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/02-hooks/03-executorch-bindings/useExecutorchModule.md","tags":[],"version":"current","frontMatter":{"title":"useExecutorchModule"},"sidebar":"tutorialSidebar","previous":{"title":"ExecuTorch Bindings","permalink":"/react-native-executorch/docs/next/category/executorch-bindings"},"next":{"title":"TypeScript API","permalink":"/react-native-executorch/docs/next/category/typescript-api"}}');var o=t(4848),i=t(8453);const s={title:"useExecutorchModule"},c=void 0,d={},a=[{value:"Initializing ExecuTorch Module",id:"initializing-executorch-module",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"TensorPtr",id:"tensorptr",level:2},{value:"End to end example",id:"end-to-end-example",level:2},{value:"Importing the Module and loading the model",id:"importing-the-module-and-loading-the-model",level:3},{value:"Setting up input parameters",id:"setting-up-input-parameters",level:3},{value:"Performing inference",id:"performing-inference",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["useExecutorchModule provides React Native bindings to the ExecuTorch ",(0,o.jsx)(n.a,{href:"https://pytorch.org/executorch/stable/extension-module.html",children:"Module API"})," directly from JavaScript."]}),"\n",(0,o.jsx)(n.admonition,{type:"caution",children:(0,o.jsx)(n.p,{children:"These bindings are primarily intended for custom model integration where no dedicated hook exists. If you are considering using a provided model, first verify whether a dedicated hook is available. Dedicated hooks simplify the implementation process by managing necessary pre and post-processing automatically. Utilizing these can save you effort and reduce complexity, ensuring you do not implement additional handling that is already covered."})}),"\n",(0,o.jsx)(n.h2,{id:"initializing-executorch-module",children:"Initializing ExecuTorch Module"}),"\n",(0,o.jsxs)(n.p,{children:["You can initialize the ExecuTorch module in your JavaScript application using the ",(0,o.jsx)(n.code,{children:"useExecutorchModule"})," hook. This hook facilitates the loading of models from the specified source and prepares them for use."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { useExecutorchModule } from 'react-native-executorch';\n\nconst executorchModule = useExecutorchModule({\n  modelSource: require('../assets/models/model.pte'),\n});\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"modelSource"})," parameter expects a location string pointing to the model binary."]}),"\n",(0,o.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,o.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"modelSource"})})," - A string that specifies the location of the model binary."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,o.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Field"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Type"}),(0,o.jsx)(n.th,{style:{textAlign:"center"},children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"error"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)("code",{children:"string | null"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"Contains the error message if the model failed to load."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"isGenerating"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"boolean"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"Indicates whether the model is currently processing an inference."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"isReady"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"boolean"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"forward"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"(input: TensorPtr[]) => Promise<TensorPtr[]>"})}),(0,o.jsxs)(n.td,{style:{textAlign:"center"},children:["Executes the model's forward pass, where ",(0,o.jsx)(n.code,{children:"input"})," is an array of TensorPtr objects. If the inference is successful, an array of tensor pointers is returned."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"downloadProgress"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:(0,o.jsx)(n.code,{children:"number"})}),(0,o.jsx)(n.td,{style:{textAlign:"center"},children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"tensorptr",children:"TensorPtr"}),"\n",(0,o.jsxs)(n.p,{children:["TensorPtr is a JS representation of the underlying tensor, which is then passed to the model. You can read more about creating tensors ",(0,o.jsx)(n.a,{href:"https://docs.pytorch.org/executorch/stable/extension-tensor.html",children:"here"}),". On JS side, the TensorPtr holds the following information:"]}),"\n",(0,o.jsxs)(t,{children:[(0,o.jsx)("summary",{children:"Type definitions"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"interface TensorPtr {\n  dataPtr: TensorBuffer;\n  sizes: number[];\n  scalarType: ScalarType;\n}\n\ntype TensorBuffer =\n  | ArrayBuffer\n  | Float32Array\n  | Float64Array\n  | Int8Array\n  | Int16Array\n  | Int32Array\n  | Uint8Array\n  | Uint16Array\n  | Uint32Array\n  | BigInt64Array\n  | BigUint64Array;\n\nenum ScalarType {\n  BYTE = 0,\n  CHAR = 1,\n  SHORT = 2,\n  INT = 3,\n  LONG = 4,\n  HALF = 5,\n  FLOAT = 6,\n  DOUBLE = 7,\n  BOOL = 11,\n  QINT8 = 12,\n  QUINT8 = 13,\n  QINT32 = 14,\n  QUINT4X2 = 16,\n  QUINT2X4 = 17,\n  BITS16 = 22,\n  FLOAT8E5M2 = 23,\n  FLOAT8E4M3FN = 24,\n  FLOAT8E5M2FNUZ = 25,\n  FLOAT8E4M3FNUZ = 26,\n  UINT16 = 27,\n  UINT32 = 28,\n  UINT64 = 29,\n}\n"})})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"dataPtr"})," - Represents a data buffer that will be used to create a tensor on the native side. This can be either an ",(0,o.jsx)(n.code,{children:"ArrayBuffer"})," or a ",(0,o.jsx)(n.code,{children:"TypedArray"}),". If your model takes in a datatype which is not covered by any of the ",(0,o.jsx)(n.code,{children:"TypedArray"})," types, just pass an ",(0,o.jsx)(n.code,{children:"ArrayBuffer"})," here."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"sizes"})," - Represents a shape of a given tensor, i.e. for a 640x640 RGB image with a batch size of 1, you would need to pass ",(0,o.jsx)(n.code,{children:"[1, 3, 640, 640]"})," here."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"scalarType"})," - An enum resembling the ExecuTorch's ",(0,o.jsx)(n.code,{children:"ScalarType"}),". For example, if your model was exported with float32 as an input, you will need to pass ",(0,o.jsx)(n.code,{children:"ScalarType.FLOAT"})," here."]}),"\n",(0,o.jsx)(n.h2,{id:"end-to-end-example",children:"End to end example"}),"\n",(0,o.jsxs)(n.p,{children:["This example demonstrates the integration and usage of the ExecuTorch bindings with a ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/computer-vision/useStyleTransfer",children:"style transfer model"}),". Specifically, we'll be using the ",(0,o.jsx)(n.code,{children:"STYLE_TRANSFER_CANDY"})," model, which applies artistic style transfer to an input image."]}),"\n",(0,o.jsx)(n.h3,{id:"importing-the-module-and-loading-the-model",children:"Importing the Module and loading the model"}),"\n",(0,o.jsxs)(n.p,{children:["First, import the necessary functions from the ",(0,o.jsx)(n.code,{children:"react-native-executorch"})," package and initialize the ExecuTorch module with the specified style transfer model."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import {\n  useExecutorchModule,\n  STYLE_TRANSFER_CANDY,\n  ScalarType,\n} from 'react-native-executorch';\n\n// Initialize the executorch module with the predefined style transfer model.\nconst executorchModule = useExecutorchModule({\n  modelSource: STYLE_TRANSFER_CANDY,\n});\n"})}),"\n",(0,o.jsx)(n.h3,{id:"setting-up-input-parameters",children:"Setting up input parameters"}),"\n",(0,o.jsxs)(n.p,{children:["To prepare the model input, define the tensor shape according to your model's requirements (defined by the model export process). For example, the STYLE_TRANSFER_CANDY model expects a tensor with shape ",(0,o.jsx)(n.code,{children:"[1, 3, 640, 640]"})," \u2014 representing a batch size of 1, 3 color channels (RGB), and 640\xd7640 pixel dimensions."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"const inputTensor = {\n  dataPtr: new Float32Array(1 * 3 * 640 * 640), // or other TypedArray / ArrayBuffer\n  sizes: [1, 3, 640, 640],\n  scalarType: ScalarType.FLOAT,\n};\n"})}),"\n",(0,o.jsx)(n.h3,{id:"performing-inference",children:"Performing inference"}),"\n",(0,o.jsxs)(n.p,{children:["After passing input to the forward function, you'll receive an array of TensorPtr objects. Each TensorPtr contains its ",(0,o.jsx)(n.code,{children:"dataPtr"})," as an ArrayBuffer. Since ArrayBuffer represents raw binary data, you'll need to interpret it according to the tensor's underlying data type (e.g., creating a Float32Array view for float32 tensors, Int32Array for int32 tensors, etc.)."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"try {\n  // Perform the forward operation and receive the stylized image output.\n  const output = await executorchModule.forward([inputTensor]);\n  // Interpret the output ArrayBuffer\n  // foo(output[0].dataPtr);\n} catch (error) {\n  // Log any errors that occur during the forward pass.\n  console.error('Error during model execution:', error);\n}\n"})}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsx)(n.p,{children:"This code assumes that you have handled preprocessing of the input image (scaling, normalization) and postprocessing of the output (interpreting the raw output data) according to the model's requirements. Make sure to adjust these parts depending on your specific data and model outputs."})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var r=t(6540);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);
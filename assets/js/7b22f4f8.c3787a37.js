"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8188],{3066:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"guides/exporting-llama","title":"Exporting Llama","description":"In order to make the process of export as simple as possible for you, we created a script that runs a Docker container and exports the model.","source":"@site/versioned_docs/version-0.1.x/guides/exporting-llama.mdx","sourceDirName":"guides","slug":"/guides/exporting-llama","permalink":"/react-native-executorch/docs/0.1.x/guides/exporting-llama","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.1.x/guides/exporting-llama.mdx","tags":[],"version":"0.1.x","sidebarPosition":2,"frontMatter":{"title":"Exporting Llama","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Running LLMs","permalink":"/react-native-executorch/docs/0.1.x/guides/running-llms"}}');var o=n(4848),i=n(8453);const r={title:"Exporting Llama",sidebar_position:2},l=void 0,s={},c=[{value:"Steps to export Llama",id:"steps-to-export-llama",level:2},{value:"1. Create an Account",id:"1-create-an-account",level:3},{value:"2. Select a Model",id:"2-select-a-model",level:3},{value:"3. Download Files",id:"3-download-files",level:3},{value:"4. Rename the Tokenizer File",id:"4-rename-the-tokenizer-file",level:3},{value:"5. Run the Export Script",id:"5-run-the-export-script",level:3}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"In order to make the process of export as simple as possible for you, we created a script that runs a Docker container and exports the model."}),"\n",(0,o.jsx)(t.h2,{id:"steps-to-export-llama",children:"Steps to export Llama"}),"\n",(0,o.jsx)(t.h3,{id:"1-create-an-account",children:"1. Create an Account"}),"\n",(0,o.jsxs)(t.p,{children:["Get a ",(0,o.jsx)(t.a,{href:"https://huggingface.co/",children:"HuggingFace"})," account. This will allow you to download needed files. You can also use the ",(0,o.jsx)(t.a,{href:"https://www.llama.com/llama-downloads/",children:"official Llama website"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"2-select-a-model",children:"2. Select a Model"}),"\n",(0,o.jsx)(t.p,{children:"Pick the model that suits your needs. Before you download it, you'll need to accept a license. For best performance, we recommend using Spin-Quant or QLoRA versions of the model:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/tree/main/original",children:"Llama 3.2 3B"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/tree/main/original",children:"Llama 3.2 1B"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8/tree/main",children:"Llama 3.2 3B Spin-Quant"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8/tree/main",children:"Llama 3.2 1B Spin-Quant"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8/tree/main",children:"Llama 3.2 3B QLoRA"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8/tree/main",children:"Llama 3.2 1B QLoRA"})}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"3-download-files",children:"3. Download Files"}),"\n",(0,o.jsxs)(t.p,{children:["Download the ",(0,o.jsx)(t.code,{children:"consolidated.00.pth"}),", ",(0,o.jsx)(t.code,{children:"params.json"})," and ",(0,o.jsx)(t.code,{children:"tokenizer.model"})," files. If you can't see them, make sure to check the ",(0,o.jsx)(t.code,{children:"original"})," directory."]}),"\n",(0,o.jsx)(t.h3,{id:"4-rename-the-tokenizer-file",children:"4. Rename the Tokenizer File"}),"\n",(0,o.jsxs)(t.p,{children:["Rename the ",(0,o.jsx)(t.code,{children:"tokenizer.model"})," file to ",(0,o.jsx)(t.code,{children:"tokenizer.bin"})," as required by the library:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"mv tokenizer.model tokenizer.bin\n"})}),"\n",(0,o.jsx)(t.h3,{id:"5-run-the-export-script",children:"5. Run the Export Script"}),"\n",(0,o.jsxs)(t.p,{children:["Navigate to the ",(0,o.jsx)(t.code,{children:"llama_export"})," directory and run the following command:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"./build_llama_binary.sh --model-path /path/to/consolidated.00.pth --params-path /path/to/params.json\n"})}),"\n",(0,o.jsxs)(t.p,{children:["The script will pull a Docker image from docker hub, and then run it to export the model. By default the output (llama3_2.pte file) will be saved in the ",(0,o.jsx)(t.code,{children:"llama-export/outputs"})," directory. However, you can override that behavior with the ",(0,o.jsx)(t.code,{children:"--output-path [path]"})," flag."]}),"\n",(0,o.jsx)(t.admonition,{title:"Note",type:"note",children:(0,o.jsx)(t.p,{children:"This Docker image was tested on MacOS with ARM chip. This might not work in other environments."})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>l});var a=n(6540);const o={},i=a.createContext(o);function r(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);
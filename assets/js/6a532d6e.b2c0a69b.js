"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4969],{7746:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"typescript-api/natural-language-processing/SpeechToTextModule","title":"SpeechToTextModule","description":"TypeScript API implementation of the useSpeechToText hook.","source":"@site/docs/03-typescript-api/01-natural-language-processing/SpeechToTextModule.md","sourceDirName":"03-typescript-api/01-natural-language-processing","slug":"/typescript-api/natural-language-processing/SpeechToTextModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/SpeechToTextModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-typescript-api/01-natural-language-processing/SpeechToTextModule.md","tags":[],"version":"current","frontMatter":{"title":"SpeechToTextModule"},"sidebar":"tutorialSidebar","previous":{"title":"LLMModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/LLMModule"},"next":{"title":"TextEmbeddingsModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/TextEmbeddingsModule"}}');var r=t(4848),i=t(8453);const s={title:"SpeechToTextModule"},a=void 0,c={},d=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Example",id:"example",level:2},{value:"Transcription",id:"transcription",level:3},{value:"Streaming Transcription",id:"streaming-transcription",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText",children:"useSpeechToText"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\nawait model.transcribe(waveform);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"load"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(model: SpeechToTextModelConfig, onDownloadProgressCallback?: (progress: number) => void): Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Loads the model specified by the config object. ",(0,r.jsx)(n.code,{children:"onDownloadProgressCallback"})," allows you to monitor the current progress of the model download."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"encode"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(waveform: Float32Array): Promise<void>"})}),(0,r.jsx)(n.td,{children:"Runs the encoding part of the model on the provided waveform. Stores the result internally."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"decode"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(tokens: number[]): Promise<Float32Array>"})}),(0,r.jsx)(n.td,{children:"Runs the decoder of the model. Returns the decoded waveform as a Float32Array."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"transcribe"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(waveform: number[], options?: DecodingOptions): Promise<string>"})}),(0,r.jsxs)(n.td,{children:["Starts a transcription process for a given input array (16kHz waveform). For multilingual models, specify the language in ",(0,r.jsx)(n.code,{children:"options"}),". Returns the transcription as a string."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"stream"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(options?: DecodingOptions): AsyncGenerator<{ committed: string; nonCommitted: string }>"})}),(0,r.jsxs)(n.td,{children:["Starts a streaming transcription session. Yields objects with ",(0,r.jsx)(n.code,{children:"committed"})," and ",(0,r.jsx)(n.code,{children:"nonCommitted"})," transcriptions. Use with ",(0,r.jsx)(n.code,{children:"streamInsert"})," and ",(0,r.jsx)(n.code,{children:"streamStop"})," to control the stream."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"streamStop"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(): void"})}),(0,r.jsx)(n.td,{children:"Stops the current streaming transcription session."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"streamInsert"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(waveform: number[]): void"})}),(0,r.jsx)(n.td,{children:"Inserts a new audio chunk into the streaming transcription session."})]})]})]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"committed"})," contains the latest part of the transcription that is finalized and will not change. To obtain the full transcription during streaming, concatenate all the ",(0,r.jsx)(n.code,{children:"committed"})," values yielded over time. Useful for displaying stable results during streaming."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nonCommitted"})," contains the part of the transcription that is still being processed and may change. Useful for displaying live, partial results during streaming."]}),"\n"]})}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Languages supported by whisper (Multilingual)\ntype SpeechToTextLanguage =\n  | 'af'\n  | 'sq'\n  | 'ar'\n  | 'hy'\n  | 'az'\n  | 'eu'\n  | 'be'\n  | 'bn'\n  | 'bs'\n  | 'bg'\n  | 'my'\n  | 'ca'\n  | 'zh'\n  | 'hr'\n  | 'cs'\n  | 'da'\n  | 'nl'\n  | 'et'\n  | 'en'\n  | 'fi'\n  | 'fr'\n  | 'gl'\n  | 'ka'\n  | 'de'\n  | 'el'\n  | 'gu'\n  | 'ht'\n  | 'he'\n  | 'hi'\n  | 'hu'\n  | 'is'\n  | 'id'\n  | 'it'\n  | 'ja'\n  | 'kn'\n  | 'kk'\n  | 'km'\n  | 'ko'\n  | 'lo'\n  | 'lv'\n  | 'lt'\n  | 'mk'\n  | 'mg'\n  | 'ms'\n  | 'ml'\n  | 'mt'\n  | 'mr'\n  | 'ne'\n  | 'no'\n  | 'fa'\n  | 'pl'\n  | 'pt'\n  | 'pa'\n  | 'ro'\n  | 'ru'\n  | 'sr'\n  | 'si'\n  | 'sk'\n  | 'sl'\n  | 'es'\n  | 'su'\n  | 'sw'\n  | 'sv'\n  | 'tl'\n  | 'tg'\n  | 'ta'\n  | 'te'\n  | 'th'\n  | 'tr'\n  | 'uk'\n  | 'ur'\n  | 'uz'\n  | 'vi'\n  | 'cy'\n  | 'yi';\n\ninterface DecodingOptions {\n  language?: SpeechToTextLanguage;\n}\n\ninterface SpeechToTextModelConfig {\n  isMultilingual: boolean;\n  encoderSource: ResourceSource;\n  decoderSource: ResourceSource;\n  tokenizerSource: ResourceSource;\n}\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,r.jsxs)(n.p,{children:["Create an instance of SpeechToTextModule and use the ",(0,r.jsx)(n.code,{children:"load"})," method. It accepts an object with the following fields:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"isMultilingual"})})," - A boolean flag indicating whether the model supports multiple languages."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"encoderSource"})})," - A string that specifies the location of a ",(0,r.jsx)(n.code,{children:".pte"})," file for the encoder."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"decoderSource"})})," - A string that specifies the location of a ",(0,r.jsx)(n.code,{children:".pte"})," file for the decoder."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerSource"})})," - A string that specifies the location to the tokenizer for the model."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - (Optional) Function that will be called on download progress."]}),"\n",(0,r.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.code,{children:"transcribe"})," method. It accepts one argument, which is an array of numbers representing a waveform at 16kHz sampling rate. The method returns a promise, which can resolve either to an error or a string containing the output text."]}),"\n",(0,r.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,r.jsxs)(n.p,{children:["If you aim to obtain a transcription in other languages than English, use the multilingual version of whisper. To obtain the output text in your desired language, pass the ",(0,r.jsx)(n.code,{children:"DecodingOptions"})," object with the ",(0,r.jsx)(n.code,{children:"language"})," field set to your desired language code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY, (progress) => {\n  console.log(progress);\n});\n\nconst transcription = await model.transcribe(spanishAudio, { language: 'es' });\n"})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.h3,{id:"transcription",children:"Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\n// Load the model\nconst model = new SpeechToTextModule();\n\n// Download the audio file\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\n// Decode the audio data\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioDataSource(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\nconst audioArray = Array.from(audioBuffer);\n\n// Transcribe the audio\ntry {\n  const transcription = await model.transcribe(audioArray);\n  console.log(transcription);\n} catch (error) {\n  console.error('Error during audio transcription', error);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"streaming-transcription",children:"Streaming Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioManager, AudioRecorder } from 'react-native-audio-api';\n\n// Load the model\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\n// Configure audio session\nAudioManager.setAudioSessionOptions({\n  iosCategory: 'playAndRecord',\n  iosMode: 'spokenAudio',\n  iosOptions: ['allowBluetooth', 'defaultToSpeaker'],\n});\nAudioManager.requestRecordingPermissions();\n\n// Initialize audio recorder\nconst recorder = new AudioRecorder({\n  sampleRate: 16000,\n  bufferLengthInSamples: 1600,\n});\nrecorder.onAudioReady(async ({ buffer }) => {\n  const bufferArray = Array.from(buffer.getChannelData(0));\n  // Insert the audio into the streaming transcription\n  model.streamInsert(bufferArray);\n});\nrecorder.start();\n\n// Start streaming transcription\ntry {\n  let transcription = '';\n  for await (const { committed, nonCommitted } of model.stream()) {\n    console.log('Streaming transcription:', { committed, nonCommitted });\n    transcription += committed;\n  }\n  console.log('Final transcription:', transcription);\n} catch (error) {\n  console.error('Error during streaming transcription:', error);\n}\n\n// Stop streaming transcription\nmodel.streamStop();\nrecorder.stop();\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(6540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);
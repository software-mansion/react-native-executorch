"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[660],{5680:(e,n,t)=>{t.d(n,{xA:()=>d,yg:()=>m});var r=t(6540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,o=function(e,n){if(null==e)return{};var t,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var c=r.createContext({}),s=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},d=function(e){var n=s(e.components);return r.createElement(c.Provider,{value:n},e.children)},g="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),g=s(t),u=o,m=g["".concat(c,".").concat(u)]||g[u]||p[u]||a;return t?r.createElement(m,i(i({ref:n},d),{},{components:t})):r.createElement(m,i({ref:n},d))}));function m(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,i=new Array(a);i[0]=u;var l={};for(var c in n)hasOwnProperty.call(n,c)&&(l[c]=n[c]);l.originalType=e,l[g]="string"==typeof e?e:o,i[1]=l;for(var s=2;s<a;s++)i[s]=t[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},7792:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>s});var r=t(8168),o=(t(6540),t(5680));const a={title:"OCRModule",sidebar_position:6},i=void 0,l={unversionedId:"hookless-api/OCRModule",id:"hookless-api/OCRModule",title:"OCRModule",description:"Hookless implementation of the useOCR hook.",source:"@site/docs/hookless-api/OCRModule.md",sourceDirName:"hookless-api",slug:"/hookless-api/OCRModule",permalink:"/react-native-executorch/docs/hookless-api/OCRModule",draft:!1,editUrl:"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/hookless-api/OCRModule.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{title:"OCRModule",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"ObjectDetectionModule",permalink:"/react-native-executorch/docs/hookless-api/ObjectDetectionModule"},next:{title:"SpeechToTextModule",permalink:"/react-native-executorch/docs/hookless-api/SpeechToTextModule"}},c={},s=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Listening for download progress",id:"listening-for-download-progress",level:2},{value:"Running the model",id:"running-the-model",level:2}],d={toc:s},g="wrapper";function p(e){let{components:n,...t}=e;return(0,o.yg)(g,(0,r.A)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,o.yg)("p",null,"Hookless implementation of the ",(0,o.yg)("a",{parentName:"p",href:"/react-native-executorch/docs/computer-vision/useOCR"},"useOCR")," hook."),(0,o.yg)("h2",{id:"reference"},"Reference"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-typescript"},"import {\n  OCRModule,\n  CRAFT_800,\n  RECOGNIZER_EN_CRNN_512,\n  RECOGNIZER_EN_CRNN_256,\n  RECOGNIZER_EN_CRNN_128,\n} from 'react-native-executorch';\nconst imageUri = 'path/to/image.png';\n\n// Loading the model\nawait OCRModule.load({\n  detectorSource: CRAFT_800,\n  recognizerSources: {\n    recognizerLarge: RECOGNIZER_EN_CRNN_512,\n    recognizerMedium: RECOGNIZER_EN_CRNN_256,\n    recognizerSmall: RECOGNIZER_EN_CRNN_128,\n  },\n  language: 'en',\n});\n\n// Running the model\nconst ocrDetections = await OCRModule.forward(imageUri);\n")),(0,o.yg)("h3",{id:"methods"},"Methods"),(0,o.yg)("table",null,(0,o.yg)("thead",{parentName:"table"},(0,o.yg)("tr",{parentName:"thead"},(0,o.yg)("th",{parentName:"tr",align:null},"Method"),(0,o.yg)("th",{parentName:"tr",align:null},"Type"),(0,o.yg)("th",{parentName:"tr",align:null},"Description"))),(0,o.yg)("tbody",{parentName:"table"},(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"load")),(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"(detectorSource: string, recognizerSources: RecognizerSources, language: OCRLanguage): Promise<void>")),(0,o.yg)("td",{parentName:"tr",align:null},"Loads the detector and recognizers, which sources are represented by ",(0,o.yg)("inlineCode",{parentName:"td"},"RecognizerSources"),".")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"forward")),(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"(input: string): Promise<OCRDetections[]>")),(0,o.yg)("td",{parentName:"tr",align:null},"Executes the model's forward pass, where ",(0,o.yg)("inlineCode",{parentName:"td"},"input")," can be a fetchable resource or a Base64-encoded string.")),(0,o.yg)("tr",{parentName:"tbody"},(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"onDownloadProgress")),(0,o.yg)("td",{parentName:"tr",align:null},(0,o.yg)("inlineCode",{parentName:"td"},"(callback: (downloadProgress: number) => void): any")),(0,o.yg)("td",{parentName:"tr",align:null},"Subscribe to the download progress event.")))),(0,o.yg)("details",null,(0,o.yg)("summary",null,"Type definitions"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-typescript"},"interface RecognizerSources {\n  recognizerLarge: string | number;\n  recognizerMedium: string | number;\n  recognizerSmall: string | number;\n}\n\ntype OCRLanguage = 'en';\n\ninterface Point {\n  x: number;\n  y: number;\n}\n\ninterface OCRDetection {\n  bbox: Point[];\n  text: string;\n  score: number;\n}\n"))),(0,o.yg)("h2",{id:"loading-the-model"},"Loading the model"),(0,o.yg)("p",null,"To load the model, use the ",(0,o.yg)("inlineCode",{parentName:"p"},"load")," method. It accepts:"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("inlineCode",{parentName:"strong"},"detectorSource"))," - A string that specifies the location of the detector binary. For more information, take a look at ",(0,o.yg)("a",{parentName:"p",href:"/react-native-executorch/docs/fundamentals/loading-models"},"loading models")," section."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("inlineCode",{parentName:"strong"},"recognizerSources"))," - An object that specifies locations of the recognizers binary files. Each recognizer is composed of three models tailored to process images of varying widths."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"recognizerLarge")," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 512 pixels."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"recognizerMedium")," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 256 pixels."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"recognizerSmall")," - A string that specifies the location of the recognizer binary file which accepts input images with a width of 128 pixels.")),(0,o.yg)("p",null,"For more information, take a look at ",(0,o.yg)("a",{parentName:"p",href:"/react-native-executorch/docs/fundamentals/loading-models"},"loading models")," section."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},(0,o.yg)("inlineCode",{parentName:"strong"},"language"))," - A parameter that specifies the language of the text to be recognized by the OCR."),(0,o.yg)("p",null,"This method returns a promise, which can resolve to an error or void."),(0,o.yg)("h2",{id:"listening-for-download-progress"},"Listening for download progress"),(0,o.yg)("p",null,"To subscribe to the download progress event, you can use the ",(0,o.yg)("inlineCode",{parentName:"p"},"onDownloadProgress")," method. It accepts a callback function that will be called whenever the download progress changes."),(0,o.yg)("h2",{id:"running-the-model"},"Running the model"),(0,o.yg)("p",null,"To run the model, you can use the ",(0,o.yg)("inlineCode",{parentName:"p"},"forward")," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The method returns a promise, which can resolve either to an error or an array of ",(0,o.yg)("inlineCode",{parentName:"p"},"OCRDetection")," objects. Each object contains coordinates of the bounding box, the label of the detected object, and the confidence score."))}p.isMDXComponent=!0}}]);
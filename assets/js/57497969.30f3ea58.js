"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[769],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var o=t(6540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}},8576:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"typescript-api/natural-language-processing/TokenizerModule","title":"TokenizerModule","description":"TypeScript API implementation of the useTokenizer hook.","source":"@site/docs/03-typescript-api/01-natural-language-processing/TokenizerModule.md","sourceDirName":"03-typescript-api/01-natural-language-processing","slug":"/typescript-api/natural-language-processing/TokenizerModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/TokenizerModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-typescript-api/01-natural-language-processing/TokenizerModule.md","tags":[],"version":"current","frontMatter":{"title":"TokenizerModule"},"sidebar":"tutorialSidebar","previous":{"title":"TextEmbeddingsModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/TextEmbeddingsModule"},"next":{"title":"Computer Vision","permalink":"/react-native-executorch/docs/next/category/computer-vision-1"}}');var r=t(4848),i=t(8453);const s={title:"TokenizerModule"},c=void 0,d={},l=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3}];function a(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useTokenizer",children:"useTokenizer"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { TokenizerModule, ALL_MINILM_L6_V2 } from 'react-native-executorch';\n\n// Creating an instance\nconst tokenizerModule = new TokenizerModule();\n\n// Load the tokenizer\nawait tokenizerModule.load(ALL_MINILM_L6_V2);\nconsole.log('Tokenizer loaded');\n\n// Get tokenizers vocabulary size\nconst vocabSize = await tokenizerModule.getVocabSize();\nconsole.log('Vocabulary size:', vocabSize);\n\nconst text = 'Hello, world!';\n\n// Tokenize the text\nconst tokens = await tokenizerModule.encode(text);\nconsole.log('Token IDs:', tokens);\n\n// Decode the tokens back to text\nconst decoded = await tokenizerModule.decode(tokens);\nconsole.log('Decoded text:', decoded);\n\n// Get the token ID for a specific token\nconst tokenId = await tokenizerModule.tokenToId('hello');\nconsole.log('Token ID for \"Hello\":', tokenId);\n\n// Get the token for a specific ID\nconst token = await tokenizerModule.idToToken(tokenId);\nconsole.log('Token for ID:', token);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"load"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(tokenizer: { tokenizerSource: ResourceSource }, onDownloadProgressCallback?: (progress: number) => void): Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Loads the tokenizer from the specified source. ",(0,r.jsx)(n.code,{children:"tokenizerSource"})," is a string that points to the location of the tokenizer JSON file."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"encode"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: string): Promise<number[]>"})}),(0,r.jsx)(n.td,{children:"Converts a string into an array of token IDs."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"decode"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: number[]): Promise<string>"})}),(0,r.jsx)(n.td,{children:"Converts an array of token IDs into a string."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"getVocabSize"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(): Promise<number>"})}),(0,r.jsx)(n.td,{children:"Returns the size of the tokenizer's vocabulary."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"idToToken"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(tokenId: number): Promise<string>"})}),(0,r.jsx)(n.td,{children:"Returns the token associated to the ID."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"tokenToId"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(token: string): Promise<number>"})}),(0,r.jsx)(n.td,{children:"Returns the ID associated to the token."})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"type ResourceSource = string | number | object;\n"})})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[61472],{28453:(e,n,o)=>{o.d(n,{R:()=>c,x:()=>a});var t=o(96540);const r={},i=t.createContext(r);function c(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),t.createElement(i.Provider,{value:n},e.children)}},60097:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"typescript-api/natural-language-processing/SpeechToTextModule","title":"SpeechToTextModule","description":"TypeScript API implementation of the useSpeechToText hook.","source":"@site/docs/04-typescript-api/01-natural-language-processing/SpeechToTextModule.md","sourceDirName":"04-typescript-api/01-natural-language-processing","slug":"/typescript-api/natural-language-processing/SpeechToTextModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/SpeechToTextModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/04-typescript-api/01-natural-language-processing/SpeechToTextModule.md","tags":[],"version":"current","frontMatter":{"title":"SpeechToTextModule"},"sidebar":"tutorialSidebar","previous":{"title":"LLMModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/LLMModule"},"next":{"title":"TextEmbeddingsModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/TextEmbeddingsModule"}}');var r=o(74848),i=o(28453);const c={title:"SpeechToTextModule"},a=void 0,s={},l=[{value:"API Reference",id:"api-reference",level:2},{value:"High Level Overview",id:"high-level-overview",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Example",id:"example",level:2},{value:"Transcription",id:"transcription",level:3},{value:"Streaming Transcription",id:"streaming-transcription",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText",children:"useSpeechToText"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For detailed API Reference for ",(0,r.jsx)(n.code,{children:"SpeechToTextModule"})," see: ",(0,r.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:[(0,r.jsx)(n.code,{children:"SpeechToTextModule"})," API Reference"]}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For all speech to text models available out-of-the-box in React Native ExecuTorch see: ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---speech-to-text",children:"STT Models"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"high-level-overview",children:"High Level Overview"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\nawait model.transcribe(waveform);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.p,{children:["All methods of ",(0,r.jsx)(n.code,{children:"SpeechToTextModule"})," are explained in details here: ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:(0,r.jsx)(n.code,{children:"SpeechToTextModule API Reference"})})]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"committed"})," contains the latest part of the transcription that is finalized and will not change. To obtain the full transcription during streaming, concatenate all the ",(0,r.jsx)(n.code,{children:"committed"})," values yielded over time. Useful for displaying stable results during streaming."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"nonCommitted"})," contains the part of the transcription that is still being processed and may change. Useful for displaying live, partial results during streaming."]}),"\n"]})}),"\n",(0,r.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,r.jsxs)(n.p,{children:["Create an instance of ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:(0,r.jsx)(n.code,{children:"SpeechToTextModule"})})," and use the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#load",children:(0,r.jsx)(n.code,{children:"load"})})," method. It accepts an object with the following fields:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#model",children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#ismultilingual",children:(0,r.jsx)(n.code,{children:"isMultilingual"})})," - Flag indicating if model is multilingual."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#encodersource",children:(0,r.jsx)(n.code,{children:"encoderSource"})})," - The location of the used encoder."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#decodersource",children:(0,r.jsx)(n.code,{children:"decoderSource"})})," - The location of the used decoder."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#tokenizersource",children:(0,r.jsx)(n.code,{children:"tokenizerSource"})})," - The location of the used tokenizer."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#ondownloadprogresscallback",children:(0,r.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - Callback to track download progress."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#transcribe",children:(0,r.jsx)(n.code,{children:"transcribe"})})," method. It accepts one argument, which is an array of type ",(0,r.jsx)(n.code,{children:"Float32Array"})," representing a waveform at 16kHz sampling rate. The method returns a promise, which can resolve either to an error or a string containing the output text."]}),"\n",(0,r.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,r.jsxs)(n.p,{children:["If you aim to obtain a transcription in other languages than English, use the multilingual version of whisper. To obtain the output text in your desired language, pass the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions",children:(0,r.jsx)(n.code,{children:"DecodingOptions"})})," object with the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions#language",children:(0,r.jsx)(n.code,{children:"language"})})," field set to your desired language code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY, (progress) => {\n  console.log(progress);\n});\n\nconst transcription = await model.transcribe(spanishAudio, { language: 'es' });\n"})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.h3,{id:"transcription",children:"Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\n// Load the model\nconst model = new SpeechToTextModule();\n\n// Download the audio file\nconst { uri } = await FileSystem.downloadAsync(\n  'https://some-audio-url.com/file.mp3',\n  FileSystem.cacheDirectory + 'audio_file'\n);\n\n// Decode the audio data\nconst audioContext = new AudioContext({ sampleRate: 16000 });\nconst decodedAudioData = await audioContext.decodeAudioDataSource(uri);\nconst audioBuffer = decodedAudioData.getChannelData(0);\n\n// Transcribe the audio\ntry {\n  const transcription = await model.transcribe(audioBuffer);\n  console.log(transcription);\n} catch (error) {\n  console.error('Error during audio transcription', error);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"streaming-transcription",children:"Streaming Transcription"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioManager, AudioRecorder } from 'react-native-audio-api';\n\n// Load the model\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\n// Configure audio session\nAudioManager.setAudioSessionOptions({\n  iosCategory: 'playAndRecord',\n  iosMode: 'spokenAudio',\n  iosOptions: ['allowBluetooth', 'defaultToSpeaker'],\n});\nAudioManager.requestRecordingPermissions();\n\n// Initialize audio recorder\nconst recorder = new AudioRecorder({\n  sampleRate: 16000,\n  bufferLengthInSamples: 1600,\n});\nrecorder.onAudioReady(({ buffer }) => {\n  // Insert the audio into the streaming transcription\n  model.streamInsert(buffer.getChannelData(0));\n});\nrecorder.start();\n\n// Start streaming transcription\ntry {\n  let transcription = '';\n  for await (const { committed, nonCommitted } of model.stream()) {\n    console.log('Streaming transcription:', { committed, nonCommitted });\n    transcription += committed;\n  }\n  console.log('Final transcription:', transcription);\n} catch (error) {\n  console.error('Error during streaming transcription:', error);\n}\n\n// Stop streaming transcription\nmodel.streamStop();\nrecorder.stop();\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);
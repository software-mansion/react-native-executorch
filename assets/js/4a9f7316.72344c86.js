"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[61472],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var r=t(96540);const o={},i=r.createContext(o);function a(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(i.Provider,{value:n},e.children)}},60097:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"typescript-api/natural-language-processing/SpeechToTextModule","title":"SpeechToTextModule","description":"TypeScript API implementation of the useSpeechToText hook.","source":"@site/docs/04-typescript-api/01-natural-language-processing/SpeechToTextModule.md","sourceDirName":"04-typescript-api/01-natural-language-processing","slug":"/typescript-api/natural-language-processing/SpeechToTextModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/SpeechToTextModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/04-typescript-api/01-natural-language-processing/SpeechToTextModule.md","tags":[],"version":"current","frontMatter":{"title":"SpeechToTextModule"},"sidebar":"tutorialSidebar","previous":{"title":"LLMModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/LLMModule"},"next":{"title":"TextEmbeddingsModule","permalink":"/react-native-executorch/docs/next/typescript-api/natural-language-processing/TextEmbeddingsModule"}}');var o=t(74848),i=t(28453);const a={title:"SpeechToTextModule"},s=void 0,c={},l=[{value:"API Reference",id:"api-reference",level:2},{value:"High Level Overview",id:"high-level-overview",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Timestamps &amp; Transcription Stat Data",id:"timestamps--transcription-stat-data",level:3},{value:"Example",id:"example",level:2},{value:"Transcription",id:"transcription",level:3},{value:"Streaming Transcription",id:"streaming-transcription",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText",children:"useSpeechToText"})," hook."]}),"\n",(0,o.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["For detailed API Reference for ",(0,o.jsx)(n.code,{children:"SpeechToTextModule"})," see: ",(0,o.jsxs)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:[(0,o.jsx)(n.code,{children:"SpeechToTextModule"})," API Reference"]}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["For all speech to text models available out-of-the-box in React Native ExecuTorch see: ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/#models---speech-to-text",children:"STT Models"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"high-level-overview",children:"High Level Overview"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\n// Standard transcription (returns string)\nconst text = await model.transcribe(waveform);\n\n// Transcription with timestamps (returns Word[])\nconst textWithTimestamps = await model.transcribe(waveform, {\n  enableTimestamps: true,\n});\n"})}),"\n",(0,o.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,o.jsxs)(n.p,{children:["All methods of ",(0,o.jsx)(n.code,{children:"SpeechToTextModule"})," are explained in details here: ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:(0,o.jsx)(n.code,{children:"SpeechToTextModule API Reference"})})]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"committed"})," contains the latest part of the transcription that is finalized and will not change. To obtain the full transcription during streaming, concatenate all the ",(0,o.jsx)(n.code,{children:"committed"})," values yielded over time. Useful for displaying stable results during streaming."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"nonCommitted"})," contains the part of the transcription that is still being processed and may change. Useful for displaying live, partial results during streaming."]}),"\n"]})}),"\n",(0,o.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,o.jsxs)(n.p,{children:["Create an instance of ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule",children:(0,o.jsx)(n.code,{children:"SpeechToTextModule"})})," and use the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#load",children:(0,o.jsx)(n.code,{children:"load"})})," method. It accepts an object with the following fields:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#model",children:(0,o.jsx)(n.code,{children:"model"})})," - Object containing:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#ismultilingual",children:(0,o.jsx)(n.code,{children:"isMultilingual"})})," - Flag indicating if model is multilingual."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#encodersource",children:(0,o.jsx)(n.code,{children:"encoderSource"})})," - The location of the used encoder."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#decodersource",children:(0,o.jsx)(n.code,{children:"decoderSource"})})," - The location of the used decoder."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/SpeechToTextModelConfig#tokenizersource",children:(0,o.jsx)(n.code,{children:"tokenizerSource"})})," - The location of the used tokenizer."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#ondownloadprogresscallback",children:(0,o.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - Callback to track download progress."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,o.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,o.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,o.jsxs)(n.p,{children:["To run the model, you can use the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#transcribe",children:(0,o.jsx)(n.code,{children:"transcribe"})})," method. It accepts one argument, which is an array of type ",(0,o.jsx)(n.code,{children:"Float32Array"})," representing a waveform at 16kHz sampling rate. The method returns a promise, which can resolve either to an error or a string containing the output text."]}),"\n",(0,o.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,o.jsxs)(n.p,{children:["If you aim to obtain a transcription in other languages than English, use the multilingual version of whisper. To obtain the output text in your desired language, pass the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions",children:(0,o.jsx)(n.code,{children:"DecodingOptions"})})," object with the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/DecodingOptions#language",children:(0,o.jsx)(n.code,{children:"language"})})," field set to your desired language code."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule, WHISPER_TINY } from 'react-native-executorch';\n\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY, (progress) => {\n  console.log(progress);\n});\n\nconst transcription = await model.transcribe(spanishAudio, { language: 'es' });\n"})}),"\n",(0,o.jsx)(n.h3,{id:"timestamps--transcription-stat-data",children:"Timestamps & Transcription Stat Data"}),"\n",(0,o.jsxs)(n.p,{children:["You can obtain word-level timestamps and other useful parameters from transcription (",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#transcribe",children:(0,o.jsx)(n.code,{children:"transcribe"})})," and ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#stream",children:(0,o.jsx)(n.code,{children:"stream"})})," methods) by setting ",(0,o.jsx)(n.code,{children:"verbose: true"})," in the options. The result mimics the ",(0,o.jsx)(n.em,{children:"verbose_json"})," format from OpenAI Whisper API. For more information please read ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#transcribe",children:(0,o.jsx)(n.code,{children:"transcribe"})}),", ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/classes/SpeechToTextModule#stream",children:(0,o.jsx)(n.code,{children:"stream"})}),", and ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/TranscriptionResult",children:(0,o.jsx)(n.code,{children:"TranscriptionResult"})})," API References."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:'const transcription = await model.transcribe(audioBuffer, { verbose: true });\n// Example result\n//\n// transcription: {\n//   task: "transcription",\n//   text: "Example text for a ...",\n//   duration: 9.05,\n//   language: "en",\n//   segments: [\n//     {\n//       start: 0,\n//       end: 5.4,\n//       text: "Example text for",\n//       words: [\n//         {\n//            word: "Example",\n//            start: 0,\n//            end: 1.4\n//         },\n//         ...\n//       ]\n//       tokens: [1, 32, 45, ...],\n//       temperature: 0.0,\n//       avgLogprob: -1.235,\n//       compressionRatio: 1.632\n//     },\n//     ...\n//   ]\n// }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,o.jsx)(n.h3,{id:"transcription",children:"Transcription"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst transcribeAudio = async () => {\n  // Initialize with the model config\n  const model = new SpeechToTextModule();\n  await model.load(WHISPER_TINY_EN, (progress) => {\n    console.log(progress);\n  });\n\n  // Download the audio file\n  const { uri } = await FileSystem.downloadAsync(\n    'https://some-audio-url.com/file.mp3',\n    FileSystem.cacheDirectory + 'audio_file'\n  );\n\n  // Decode the audio data (Correct as per your previous code)\n  const audioContext = new AudioContext({ sampleRate: 16000 });\n  const decodedAudioData = await audioContext.decodeAudioData(uri);\n  const audioBuffer = decodedAudioData.getChannelData(0);\n\n  // Transcribe the audio\n  try {\n    // Option 1: Text only\n    const resultText = await model.transcribe(audioBuffer);\n    console.log('Text:', resultText.text); // .text is the standard property now\n\n    // Option 2: With timestamps (Use 'verbose' instead of 'enableTimestamps')\n    const resultVerbose = await model.transcribe(audioBuffer, {\n      verbose: true,\n    });\n\n    console.log('Full Text:', resultVerbose.text);\n    console.log('Segments:', resultVerbose.segments); // Contains start/end/more parameters\n  } catch (error) {\n    console.error('Error during audio transcription', error);\n  }\n};\n"})}),"\n",(0,o.jsx)(n.h3,{id:"streaming-transcription",children:"Streaming Transcription"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-tsx",children:"import { SpeechToTextModule, WHISPER_TINY_EN } from 'react-native-executorch';\nimport { AudioManager, AudioRecorder } from 'react-native-audio-api';\n\n// Load the model\nconst model = new SpeechToTextModule();\nawait model.load(WHISPER_TINY_EN, (progress) => {\n  console.log(progress);\n});\n\n// Configure audio session\nAudioManager.setAudioSessionOptions({\n  iosCategory: 'playAndRecord',\n  iosMode: 'spokenAudio',\n  iosOptions: ['allowBluetooth', 'defaultToSpeaker'],\n});\nawait AudioManager.requestRecordingPermissions();\n\n// Initialize audio recorder with FULL config in constructor\nconst recorder = new AudioRecorder({\n  sampleRate: 16000,\n  channelCount: 1,\n  bitsPerSample: 16,\n  bufferLengthInSamples: 16000, // e.g. 1 second buffer\n});\n\n// Pass ONLY the callback to onAudioReady\nrecorder.onAudioReady((chunk) => {\n  // Insert the audio into the streaming transcription\n  model.streamInsert(chunk.buffer.getChannelData(0));\n});\n\nawait recorder.start();\n\n// Start streaming transcription\ntry {\n  let finalTranscription = '';\n\n  // Use 'verbose' flag for timestamps/segments\n  const streamIter = model.stream({ verbose: true });\n\n  for await (const { committed, nonCommitted } of streamIter) {\n    // Note: committed/nonCommitted are objects { text, segments } now\n    console.log('Committed Text:', committed.text);\n    console.log('Live Text:', nonCommitted.text);\n\n    if (committed.text) {\n      finalTranscription += committed.text;\n    }\n  }\n  console.log('Final transcription:', finalTranscription);\n} catch (error) {\n  console.error('Error during streaming transcription:', error);\n}\n\n// Stop streaming transcription\nmodel.streamStop();\nrecorder.stop();\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);
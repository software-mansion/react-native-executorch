"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2219],{3564:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"hooks/computer-vision/useObjectDetection","title":"useObjectDetection","description":"Object detection is a computer vision technique that identifies and locates objects within images or video. It\u2019s commonly used in applications like image recognition, video surveillance or autonomous driving.","source":"@site/docs/02-hooks/02-computer-vision/useObjectDetection.md","sourceDirName":"02-hooks/02-computer-vision","slug":"/hooks/computer-vision/useObjectDetection","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useObjectDetection","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/02-hooks/02-computer-vision/useObjectDetection.md","tags":[],"version":"current","frontMatter":{"title":"useObjectDetection"},"sidebar":"tutorialSidebar","previous":{"title":"useOCR","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useOCR"},"next":{"title":"useStyleTransfer","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useStyleTransfer"}}');var i=t(4848),s=t(8453);const r={title:"useObjectDetection"},c=void 0,d={},l=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Detection object",id:"detection-object",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Object detection is a computer vision technique that identifies and locates objects within images or video. It\u2019s commonly used in applications like image recognition, video surveillance or autonomous driving.\n",(0,i.jsx)(n.code,{children:"useObjectDetection"})," is a hook that allows you to seamlessly integrate object detection into your React Native applications."]}),"\n",(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,i.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-ssdlite320-mobilenet-v3-large",children:"Hugging Face repository"}),". You can also use ",(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,i.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-tsx",children:"import {\n  useObjectDetection,\n  SSDLITE_320_MOBILENET_V3_LARGE,\n} from 'react-native-executorch';\n\nfunction App() {\n  const ssdlite = useObjectDetection({\n    modelSource: SSDLITE_320_MOBILENET_V3_LARGE, // alternatively, you can use require(...)\n  });\n\n  // ...\n  for (const detection of await ssdlite.forward('https://url-to-image.jpg')) {\n    console.log('Bounding box: ', detection.bbox);\n    console.log('Bounding label: ', detection.label);\n    console.log('Bounding score: ', detection.score);\n  }\n  // ...\n}\n"})}),"\n",(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"Type definitions"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface Bbox {\n  x1: number;\n  x2: number;\n  y1: number;\n  y2: number;\n}\n\ninterface Detection {\n  bbox: Bbox;\n  label: keyof typeof CocoLabel;\n  score: number;\n}\n"})})]}),"\n",(0,i.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"modelSource"})})," - A string that specifies the path to the model file. You can download the model from our ",(0,i.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-ssdlite320-mobilenet-v3-large/tree/main",children:"HuggingFace repository"}),".\nFor more information on that topic, you can check out the ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"Loading models"})," page."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,i.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,i.jsx)(n.p,{children:"The hook returns an object with the following properties:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Field"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"forward"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"(input: string, detectionThreshold: number = 0.7) => Promise<Detection[]>"})}),(0,i.jsxs)(n.td,{children:["A function that accepts an image (url, b64) and returns an array of ",(0,i.jsx)(n.code,{children:"Detection"})," objects. ",(0,i.jsx)(n.code,{children:"detectionThreshold"})," can be supplied to alter the sensitivity of the detection."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"error"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)("code",{children:"string | null"})}),(0,i.jsx)(n.td,{children:"Contains the error message if the model loading failed."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"isGenerating"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"boolean"})}),(0,i.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"isReady"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"boolean"})}),(0,i.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"downloadProgress"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"number"})}),(0,i.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,i.jsxs)(n.p,{children:["To run the model, you can use the ",(0,i.jsx)(n.code,{children:"forward"})," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The function returns an array of ",(0,i.jsx)(n.code,{children:"Detection"})," objects. Each object contains coordinates of the bounding box, the label of the detected object, and the confidence score. For more information, please refer to the reference or type definitions."]}),"\n",(0,i.jsx)(n.h2,{id:"detection-object",children:"Detection object"}),"\n",(0,i.jsx)(n.p,{children:"The detection object is specified as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface Bbox {\n  x1: number;\n  y1: number;\n  x2: number;\n  y2: number;\n}\n\ninterface Detection {\n  bbox: Bbox;\n  label: keyof typeof CocoLabels;\n  score: number;\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"bbox"})," property contains information about the bounding box of detected objects. It is represented as two points: one at the bottom-left corner of the bounding box (",(0,i.jsx)(n.code,{children:"x1"}),", ",(0,i.jsx)(n.code,{children:"y1"}),") and the other at the top-right corner (",(0,i.jsx)(n.code,{children:"x2"}),", ",(0,i.jsx)(n.code,{children:"y2"}),").\nThe ",(0,i.jsx)(n.code,{children:"label"})," property contains the name of the detected object, which corresponds to one of the ",(0,i.jsx)(n.code,{children:"CocoLabels"}),". The ",(0,i.jsx)(n.code,{children:"score"})," represents the confidence score of the detected object."]}),"\n",(0,i.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-tsx",children:"import {\n  useObjectDetection,\n  SSDLITE_320_MOBILENET_V3_LARGE,\n} from 'react-native-executorch';\n\nfunction App() {\n  const ssdlite = useObjectDetection({\n    modelSource: SSDLITE_320_MOBILENET_V3_LARGE,\n  });\n\n  const runModel = async () => {\n    const detections = await ssdlite.forward('https://url-to-image.jpg');\n\n    for (const detection of detections) {\n      console.log('Bounding box: ', detection.bbox);\n      console.log('Bounding label: ', detection.label);\n      console.log('Bounding score: ', detection.score);\n    }\n  };\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Number of classes"}),(0,i.jsx)(n.th,{children:"Class list"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://pytorch.org/vision/main/models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights",children:"SSDLite320 MobileNetV3 Large"})}),(0,i.jsx)(n.td,{children:"91"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/common/rnexecutorch/models/object_detection/Constants.h",children:"COCO"})})]})})]}),"\n",(0,i.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"SSDLITE_320_MOBILENET_V3_LARGE"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"13.9"})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"SSDLITE_320_MOBILENET_V3_LARGE"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"90"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"90"})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,i.jsx)(n.admonition,{title:"warning",type:"warning",children:(0,i.jsx)(n.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization."})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 13 Pro (XNNPACK) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"SSDLITE_320_MOBILENET_V3_LARGE"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"190"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"260"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"280"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"100"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"90"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>c});var o=t(6540);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);
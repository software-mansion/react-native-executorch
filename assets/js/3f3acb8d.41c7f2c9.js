"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[739],{57:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"llms/exporting-llama","title":"Exporting Llama","description":"In order to make the process of export as simple as possible for you, we created a script that runs a Docker container and exports the model.","source":"@site/docs/llms/exporting-llama.md","sourceDirName":"llms","slug":"/llms/exporting-llama","permalink":"/react-native-executorch/docs/llms/exporting-llama","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/llms/exporting-llama.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Exporting Llama","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"useLLM","permalink":"/react-native-executorch/docs/llms/useLLM"},"next":{"title":"Speech To Text","permalink":"/react-native-executorch/docs/category/speech-to-text"}}');var o=a(4848),l=a(8453);const i={title:"Exporting Llama",sidebar_position:2},r=void 0,s={},c=[{value:"Steps to export Llama",id:"steps-to-export-llama",level:2},{value:"1. Create an account",id:"1-create-an-account",level:3},{value:"2. Select a model",id:"2-select-a-model",level:3},{value:"3. Download files",id:"3-download-files",level:3},{value:"4. Rename the tokenizer file",id:"4-rename-the-tokenizer-file",level:3},{value:"5. Run the export script",id:"5-run-the-export-script",level:3}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:"In order to make the process of export as simple as possible for you, we created a script that runs a Docker container and exports the model."}),"\n",(0,o.jsx)(t.h2,{id:"steps-to-export-llama",children:"Steps to export Llama"}),"\n",(0,o.jsx)(t.h3,{id:"1-create-an-account",children:"1. Create an account"}),"\n",(0,o.jsxs)(t.p,{children:["Get a ",(0,o.jsx)(t.a,{href:"https://huggingface.co/",children:"HuggingFace"})," account. This will allow you to download needed files. You can also use the ",(0,o.jsx)(t.a,{href:"https://www.llama.com/llama-downloads/",children:"official Llama website"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"2-select-a-model",children:"2. Select a model"}),"\n",(0,o.jsx)(t.p,{children:"Pick the model that suits your needs. Before you download it, you'll need to accept a license. For best performance, we recommend using Spin-Quant or QLoRA versions of the model:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/tree/main/original",children:"Llama 3.2 3B"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/tree/main/original",children:"Llama 3.2 1B"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8/tree/main",children:"Llama 3.2 3B Spin-Quant"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8/tree/main",children:"Llama 3.2 1B Spin-Quant"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8/tree/main",children:"Llama 3.2 3B QLoRA"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8/tree/main",children:"Llama 3.2 1B QLoRA"})}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"3-download-files",children:"3. Download files"}),"\n",(0,o.jsxs)(t.p,{children:["Download the ",(0,o.jsx)(t.code,{children:"consolidated.00.pth"}),", ",(0,o.jsx)(t.code,{children:"params.json"})," and ",(0,o.jsx)(t.code,{children:"tokenizer.model"})," files. If you can't see them, make sure to check the ",(0,o.jsx)(t.code,{children:"original"})," directory."]}),"\n",(0,o.jsx)(t.h3,{id:"4-rename-the-tokenizer-file",children:"4. Rename the tokenizer file"}),"\n",(0,o.jsxs)(t.p,{children:["Rename the ",(0,o.jsx)(t.code,{children:"tokenizer.model"})," file to ",(0,o.jsx)(t.code,{children:"tokenizer.bin"})," as required by the library:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"mv tokenizer.model tokenizer.bin\n"})}),"\n",(0,o.jsx)(t.h3,{id:"5-run-the-export-script",children:"5. Run the export script"}),"\n",(0,o.jsxs)(t.p,{children:["Navigate to the ",(0,o.jsx)(t.code,{children:"llama_export"})," directory and run the following command:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",children:"./build_llama_binary.sh --model-path /path/to/consolidated.00.pth --params-path /path/to/params.json\n"})}),"\n",(0,o.jsxs)(t.p,{children:["The script will pull a Docker image from Docker Hub, and then run it to export the model. By default the output (llama3_2.pte file) will be saved in the ",(0,o.jsx)(t.code,{children:"llama-export/outputs"})," directory. However, you can override that behavior with the ",(0,o.jsx)(t.code,{children:"--output-path [path]"})," flag."]}),"\n",(0,o.jsx)(t.admonition,{type:"note",children:(0,o.jsx)(t.p,{children:"This Docker image was tested on MacOS with ARM chip. This might not work in other environments."})})]})}function h(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>r});var n=a(6540);const o={},l=n.createContext(o);function i(e){const t=n.useContext(l);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),n.createElement(l.Provider,{value:t},e.children)}}}]);
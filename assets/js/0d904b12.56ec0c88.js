"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2211],{1483:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/castle512-ce3f6cb4d1c34ca0703d57c8e164add9.png"},3221:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"hooks/computer-vision/useTextToImage","title":"useTextToImage","description":"Learn how to use image generation models in your React Native applications with React Native ExecuTorch\'s useTextToImage hook.","source":"@site/docs/02-hooks/02-computer-vision/useTextToImage.md","sourceDirName":"02-hooks/02-computer-vision","slug":"/hooks/computer-vision/useTextToImage","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useTextToImage","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/02-hooks/02-computer-vision/useTextToImage.md","tags":[],"version":"current","frontMatter":{"title":"useTextToImage","keywords":["image generation"],"description":"Learn how to use image generation models in your React Native applications with React Native ExecuTorch\'s useTextToImage hook."},"sidebar":"tutorialSidebar","previous":{"title":"useStyleTransfer","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useStyleTransfer"},"next":{"title":"useVerticalOCR","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useVerticalOCR"}}');var s=t(4848),r=t(8453);const d={title:"useTextToImage",keywords:["image generation"],description:"Learn how to use image generation models in your React Native applications with React Native ExecuTorch's useTextToImage hook."},o=void 0,c={},l=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Text-to-image is a process of generating images directly from a description in natural language by conditioning a model on the provided text input. Our implementation follows the Stable Diffusion pipeline, which applies the diffusion process in a lower-dimensional latent space to reduce memory requirements. The pipeline combines a text encoder to preprocess the prompt, a U-Net that iteratively denoises latent representations, and a VAE decoder to reconstruct the final image. React Native ExecuTorch offers a dedicated hook, ",(0,s.jsx)(n.code,{children:"useTextToImage"}),", for this task."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["It is recommended to use models provided by us which are available at our Hugging Face repository, you can also use ",(0,s.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { useTextToImage, BK_SDM_TINY_VPRED_256 } from 'react-native-executorch';\n\nconst model = useTextToImage({ model: BK_SDM_TINY_VPRED_256 });\n\nconst input = 'a castle';\n\ntry {\n  const image = await model.generate(input);\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"schedulerSource"})})," - A string that specifies the location of the scheduler config."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"tokenizerSource"})})," - A string that specifies the location of the tokenizer config."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"encoderSource"})})," - A string that specifies the location of the text encoder binary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"unetSource"})})," - A string that specifies the location of the U-Net binary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"decoderSource"})})," - A string that specifies the location of the VAE decoder binary."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,s.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,s.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Field"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"generate"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(input: string, imageSize?: number, numSteps?: number, seed?: number) => Promise<string>"})}),(0,s.jsxs)(n.td,{children:["Runs the model to generate an image described by ",(0,s.jsx)(n.code,{children:"input"}),", and conditioned by ",(0,s.jsx)(n.code,{children:"seed"}),", performing ",(0,s.jsx)(n.code,{children:"numSteps"})," inference steps. The resulting image, with dimensions ",(0,s.jsx)(n.code,{children:"imageSize"}),"\xd7",(0,s.jsx)(n.code,{children:"imageSize"})," pixels, is returned as a base64-encoded string."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"error"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)("code",{children:"string | null"})}),(0,s.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isGenerating"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isReady"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"downloadProgress"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"number"})}),(0,s.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"interrupt()"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"() => void"})}),(0,s.jsx)(n.td,{children:"Interrupts the current inference. The model is stopped in the nearest inference step."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,s.jsxs)(n.p,{children:["To run the model, you can use the ",(0,s.jsx)(n.code,{children:"forward"})," method. It accepts four arguments: a text prompt describing the requested image, a size of the image in pixels, a number of denoising steps, and an optional seed value, which enables reproducibility of the results."]}),"\n",(0,s.jsx)(n.p,{children:"The image size must be a multiple of 32 due to the architecture of the U-Net and VAE models. The seed should be a positive integer."}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Larger imageSize values require significantly more memory to run the model."})}),"\n",(0,s.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-tsx",children:"import { useTextToImage, BK_SDM_TINY_VPRED_256 } from 'react-native-executorch';\n\nfunction App() {\n  const model = useTextToImage({ model: BK_SDM_TINY_VPRED_256 });\n\n  //...\n  const input = 'a medieval castle by the sea shore';\n\n  const imageSize = 256;\n  const numSteps = 25;\n\n  try {\n    image = await model.generate(input, imageSize, numSteps);\n  } catch (error) {\n    console.error(error);\n  }\n  //...\n\n  return <Image source={{ uri: `data:image/png;base64,${image}` }} />;\n}\n"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.img,{alt:"Castle 256x256",src:t(7588).A+"",width:"320",height:"320"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.img,{alt:"Castle 512x512",src:t(1483).A+"",width:"320",height:"320"})})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Image of size 256\xd7256"}),(0,s.jsx)(n.td,{children:"Image of size 512\xd7512"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Parameters [B]"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/vivym/bk-sdm-tiny-vpred",children:"bk-sdm-tiny-vpred"})}),(0,s.jsx)(n.td,{children:"0.5"}),(0,s.jsx)(n.td,{children:"BK-SDM (Block-removed Knowledge-distilled Stable Diffusion Model) is a compressed version of Stable Diffusion v1.4 with several residual and attention blocks removed. The BK-SDM-Tiny is a v-prediction variant of the model, obtained through further block removal, built around a 0.33B-parameter U-Net."})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"The number following the underscore (_) indicates that the model supports generating image with dimensions ranging from 128 pixels up to that value. This setting doesn\u2019t affect the model\u2019s file size - it only determines how memory is allocated at runtime, based on the maximum allowed image size."})}),"\n",(0,s.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Text encoder (XNNPACK) [MB]"}),(0,s.jsx)(n.th,{children:"UNet (XNNPACK) [MB]"}),(0,s.jsx)(n.th,{children:"VAE decoder (XNNPACK) [MB]"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BK_SDM_TINY_VPRED_256"}),(0,s.jsx)(n.td,{children:"492"}),(0,s.jsx)(n.td,{children:"1290"}),(0,s.jsx)(n.td,{children:"198"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BK_SDM_TINY_VPRED_512"}),(0,s.jsx)(n.td,{children:"492"}),(0,s.jsx)(n.td,{children:"1290"}),(0,s.jsx)(n.td,{children:"198"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Android (XNNPACK) [MB]"}),(0,s.jsx)(n.th,{children:"iOS (XNNPACK) [MB]"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BK_SDM_TINY_VPRED_256"}),(0,s.jsx)(n.td,{children:"2900"}),(0,s.jsx)(n.td,{children:"2800"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BK_SDM_TINY_VPRED_512"}),(0,s.jsx)(n.td,{children:"6700"}),(0,s.jsx)(n.td,{children:"6560"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 14 Pro Max (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK)"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BK_SDM_TINY_VPRED_256"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"19100"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"25000"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"23100"})]})})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Text-to-image benchmark times are measured generating 256\xd7256 images in 10 inference steps."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},7588:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/castle256-f1bd9ddef581060ac70921b8a806ef5f.png"},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>o});var i=t(6540);const s={},r=i.createContext(s);function d(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
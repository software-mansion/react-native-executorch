"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5544],{6564:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"typescript-api/computer-vision/TextToImageModule","title":"TextToImageModule","description":"TypeScript API implementation of the useTextToImage hook.","source":"@site/docs/03-typescript-api/02-computer-vision/TextToImageModule.md","sourceDirName":"03-typescript-api/02-computer-vision","slug":"/typescript-api/computer-vision/TextToImageModule","permalink":"/react-native-executorch/docs/next/typescript-api/computer-vision/TextToImageModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/03-typescript-api/02-computer-vision/TextToImageModule.md","tags":[],"version":"current","frontMatter":{"title":"TextToImageModule"},"sidebar":"tutorialSidebar","previous":{"title":"StyleTransferModule","permalink":"/react-native-executorch/docs/next/typescript-api/computer-vision/StyleTransferModule"},"next":{"title":"VerticalOCRModule","permalink":"/react-native-executorch/docs/next/typescript-api/computer-vision/VerticalOCRModule"}}');var r=t(4848),i=t(8453);const s={title:"TextToImageModule"},d=void 0,c={},l=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Listening for inference steps",id:"listening-for-inference-steps",level:2},{value:"Deleting the model from memory",id:"deleting-the-model-from-memory",level:2}];function a(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/computer-vision/useTextToImage",children:"useTextToImage"})," hook."]}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import {\n  TextToImageModule,\n  BK_SDM_TINY_VPRED_256,\n} from 'react-native-executorch';\n\nconst input = 'a castle';\n\n// Creating an instance\nconst textToImageModule = new TextToImageModule();\n\n// Loading the model\nawait textToImageModule.load(BK_SDM_TINY_VPRED_256);\n\n// Running the model\nconst image = await textToImageModule.forward(input);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Method"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"constructor"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(inferenceCallback?: (stepIdx: number) => void)"})}),(0,r.jsx)(n.td,{children:"Creates a new instance of TextToImageModule with optional callback on inference step."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"load"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(model: {tokenizerSource: ResourceSource; schedulerSource: ResourceSource; encoderSource: ResourceSource; unetSource: ResourceSource; decoderSource: ResourceSource;}, onDownloadProgressCallback: (progress: number) => void): Promise<void>"})}),(0,r.jsx)(n.td,{children:"Loads the model."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"forward"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: string, imageSize: number, numSteps: number, seed?: number) => Promise<string>"})}),(0,r.jsxs)(n.td,{children:["Runs the model to generate an image described by ",(0,r.jsx)(n.code,{children:"input"}),", and conditioned by ",(0,r.jsx)(n.code,{children:"seed"}),", performing ",(0,r.jsx)(n.code,{children:"numSteps"})," inference steps. The resulting image, with dimensions ",(0,r.jsx)(n.code,{children:"imageSize"}),"\xd7",(0,r.jsx)(n.code,{children:"imageSize"})," pixels, is returned as a base64-encoded string."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"delete"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"() => void"})}),(0,r.jsx)(n.td,{children:"Deletes the model from memory. Note you cannot delete model while it's generating. You need to interrupt it first and make sure model stopped generation."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"interrupt"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"() => void"})}),(0,r.jsx)(n.td,{children:"Interrupts model generation. The model is stopped in the nearest step."})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"type ResourceSource = string | number | object;\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,r.jsxs)(n.p,{children:["To load the model, use the ",(0,r.jsx)(n.code,{children:"load"})," method. It accepts an object:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"schedulerSource"})})," - A string that specifies the location of the scheduler config."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerSource"})})," - A string that specifies the location of the tokenizer config."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"encoderSource"})})," - A string that specifies the location of the text encoder binary."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"unetSource"})})," - A string that specifies the location of the U-Net binary."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"decoderSource"})})," - A string that specifies the location of the VAE decoder binary."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"onDownloadProgressCallback"})})," - (Optional) Function called on download progress."]}),"\n",(0,r.jsx)(n.p,{children:"This method returns a promise, which can resolve to an error or void."}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.code,{children:"forward"})," method. It accepts four arguments: a text prompt describing the requested image, a size of the image in pixels, a number of denoising steps, and an optional seed value, which enables reproducibility of the results."]}),"\n",(0,r.jsx)(n.p,{children:"The image size must fall within the range from 128 to 512 unless specified differently, and be a multiple of 32 due to the architecture of the U-Net and VAE models."}),"\n",(0,r.jsx)(n.p,{children:"The seed value should be a positive integer."}),"\n",(0,r.jsx)(n.h2,{id:"listening-for-inference-steps",children:"Listening for inference steps"}),"\n",(0,r.jsxs)(n.p,{children:["To monitor the progress of image generation, you can pass an ",(0,r.jsx)(n.code,{children:"inferenceCallback"})," function to the constructor. The callback is invoked at each denoising step (for a total of ",(0,r.jsx)(n.code,{children:"numSteps + 1"})," times), yielding the current step index that can be used, for example, to display a progress bar."]}),"\n",(0,r.jsx)(n.h2,{id:"deleting-the-model-from-memory",children:"Deleting the model from memory"}),"\n",(0,r.jsxs)(n.p,{children:["To delete the model from memory, you can use the ",(0,r.jsx)(n.code,{children:"delete"})," method."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>d});var o=t(6540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);
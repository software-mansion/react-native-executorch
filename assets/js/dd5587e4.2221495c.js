"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[661],{2390:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"hooks/computer-vision/useImageEmbeddings","title":"useImageEmbeddings","description":"Learn how to use image embeddings models in your React Native applications with React Native ExecuTorch\'s useImageEmbeddings hook.","source":"@site/docs/02-hooks/02-computer-vision/useImageEmbeddings.md","sourceDirName":"02-hooks/02-computer-vision","slug":"/hooks/computer-vision/useImageEmbeddings","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useImageEmbeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/02-hooks/02-computer-vision/useImageEmbeddings.md","tags":[],"version":"current","frontMatter":{"title":"useImageEmbeddings","keywords":["image embedding","image embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai","clip"],"description":"Learn how to use image embeddings models in your React Native applications with React Native ExecuTorch\'s useImageEmbeddings hook."},"sidebar":"tutorialSidebar","previous":{"title":"useClassification","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useClassification"},"next":{"title":"useImageSegmentation","permalink":"/react-native-executorch/docs/next/hooks/computer-vision/useImageSegmentation"}}');var s=t(4848),r=t(8453);const d={title:"useImageEmbeddings",keywords:["image embedding","image embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai","clip"],description:"Learn how to use image embeddings models in your React Native applications with React Native ExecuTorch's useImageEmbeddings hook."},o=void 0,a={},c=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Image Embedding is the process of converting an image into a numerical representation. This representation can be used for tasks, such as classification, clustering and (using contrastive learning like e.g. CLIP model) image search."}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,s.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-clip-vit-base-patch32",children:"Hugging Face repository"}),". You can also use ",(0,s.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import {\n  useImageEmbeddings,\n  CLIP_VIT_BASE_PATCH32_IMAGE,\n} from 'react-native-executorch';\n\nconst model = useImageEmbeddings({ model: CLIP_VIT_BASE_PATCH32_IMAGE });\n\ntry {\n  const imageEmbedding = await model.forward('https://url-to-image.jpg');\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"modelSource"})})," - A string that specifies the location of the model binary."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,s.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,s.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Field"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"forward"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"(imageSource: string) => Promise<Float32Array>"})}),(0,s.jsxs)(n.td,{children:["Executes the model's forward pass, where ",(0,s.jsx)(n.code,{children:"imageSource"})," is a URI/URL to image that will be embedded."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"error"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)("code",{children:"string | null"})}),(0,s.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isGenerating"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"isReady"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"boolean"})}),(0,s.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"downloadProgress"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"number"})}),(0,s.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,s.jsxs)(n.p,{children:["To run the model, you can use the ",(0,s.jsx)(n.code,{children:"forward"})," method. It accepts one argument which is a URI/URL to an image you want to encode. The function returns a promise, which can resolve either to an error or an array of numbers representing the embedding."]}),"\n",(0,s.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const dotProduct = (a: Float32Array, b: Float32Array) =>\n  a.reduce((sum, val, i) => sum + val * b[i], 0);\n\nconst cosineSimilarity = (a: Float32Array, b: Float32Array) => {\n  const dot = dotProduct(a, b);\n  const normA = Math.sqrt(dotProduct(a, a));\n  const normB = Math.sqrt(dotProduct(b, b));\n  return dot / (normA * normB);\n};\n\ntry {\n  // we assume you've provided catImage and dogImage\n  const catImageEmbedding = await model.forward(catImage);\n  const dogImageEmbedding = await model.forward(dogImage);\n\n  const similarity = cosineSimilarity(catImageEmbedding, dogImageEmbedding);\n\n  console.log(`Cosine similarity: ${similarity}`);\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Language"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Image size"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Embedding Dimensions"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/openai/clip-vit-base-patch32",children:"clip-vit-base-patch32-image"})}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"224 x 224"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,s.jsxs)(n.td,{children:["CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. CLIP allows to embed images and text into the same vector space. This allows to find similar images as well as to implement image search. This is the image encoder part of the CLIP model. To embed text checkout ",(0,s.jsx)(n.a,{href:"/react-native-executorch/docs/next/hooks/natural-language-processing/useTextEmbeddings#supported-models",children:"clip-vit-base-patch32-text"}),"."]})]})})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Image size"})})," - the size of an image that the model takes as an input. Resize will happen automatically."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Embedding Dimensions"})})," - the size of the output embedding vector. This is the number of dimensions in the vector representation of the input image."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"For the supported models, the returned embedding vector is normalized, meaning that its length is equal to 1. This allows for easier comparison of vectors using cosine similarity, just calculate the dot product of two vectors to get the cosine similarity score."})}),"\n",(0,s.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,s.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CLIP_VIT_BASE_PATCH32_IMAGE"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"352"})]})})]}),"\n",(0,s.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CLIP_VIT_BASE_PATCH32_IMAGE"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"350"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"340"})]})})]}),"\n",(0,s.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,s.jsx)(n.admonition,{title:"warning",type:"warning",children:(0,s.jsx)(n.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization. Performance also heavily depends on image size, because resize is expansive operation, especially on low-end devices."})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 14 Pro Max (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CLIP_VIT_BASE_PATCH32_IMAGE"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"48"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"64"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"69"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"65"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"63"})]})})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Image embedding benchmark times are measured using 224\xd7224 pixel images, as required by the model. All input images, whether larger or smaller, are resized to 224\xd7224 before processing. Resizing is typically fast for small images but may be noticeably slower for very large images, which can increase total inference time."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>o});var i=t(6540);const s={},r=i.createContext(s);function d(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[97068],{28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var r=s(96540);const c={},t=r.createContext(c);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(c):e.components||c:a(e.components),r.createElement(t.Provider,{value:n},e.children)}},42772:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>i});const r=JSON.parse('{"id":"api-reference/classes/LLMModule","title":"Class: LLMModule","description":"Defined in10","source":"@site/docs/06-api-reference/classes/LLMModule.md","sourceDirName":"06-api-reference/classes","slug":"/api-reference/classes/LLMModule","permalink":"/react-native-executorch/docs/next/api-reference/classes/LLMModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/06-api-reference/classes/LLMModule.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Class: ImageSegmentationModule","permalink":"/react-native-executorch/docs/next/api-reference/classes/ImageSegmentationModule"},"next":{"title":"Class: OCRModule","permalink":"/react-native-executorch/docs/next/api-reference/classes/OCRModule"}}');var c=s(74848),t=s(28453);const a={},l="Class: LLMModule",o={},i=[{value:"Constructors",id:"constructors",level:2},{value:"Constructor",id:"constructor",level:3},{value:"Parameters",id:"parameters",level:4},{value:"optionalCallbacks",id:"optionalcallbacks",level:5},{value:"messageHistoryCallback?",id:"messagehistorycallback",level:6},{value:"tokenCallback?",id:"tokencallback",level:6},{value:"Returns",id:"returns",level:4},{value:"Methods",id:"methods",level:2},{value:"configure()",id:"configure",level:3},{value:"Parameters",id:"parameters-1",level:4},{value:"configuration",id:"configuration",level:5},{value:"Returns",id:"returns-1",level:4},{value:"delete()",id:"delete",level:3},{value:"Returns",id:"returns-2",level:4},{value:"deleteMessage()",id:"deletemessage",level:3},{value:"Parameters",id:"parameters-2",level:4},{value:"index",id:"index",level:5},{value:"Returns",id:"returns-3",level:4},{value:"forward()",id:"forward",level:3},{value:"Parameters",id:"parameters-3",level:4},{value:"input",id:"input",level:5},{value:"Returns",id:"returns-4",level:4},{value:"generate()",id:"generate",level:3},{value:"Parameters",id:"parameters-4",level:4},{value:"messages",id:"messages",level:5},{value:"tools?",id:"tools",level:5},{value:"Returns",id:"returns-5",level:4},{value:"getGeneratedTokenCount()",id:"getgeneratedtokencount",level:3},{value:"Returns",id:"returns-6",level:4},{value:"getPromptTokensCount()",id:"getprompttokenscount",level:3},{value:"Returns",id:"returns-7",level:4},{value:"getTotalTokensCount()",id:"gettotaltokenscount",level:3},{value:"Returns",id:"returns-8",level:4},{value:"interrupt()",id:"interrupt",level:3},{value:"Returns",id:"returns-9",level:4},{value:"load()",id:"load",level:3},{value:"Parameters",id:"parameters-5",level:4},{value:"model",id:"model",level:5},{value:"modelSource",id:"modelsource",level:6},{value:"tokenizerConfigSource",id:"tokenizerconfigsource",level:6},{value:"tokenizerSource",id:"tokenizersource",level:6},{value:"onDownloadProgressCallback",id:"ondownloadprogresscallback",level:5},{value:"Returns",id:"returns-10",level:4},{value:"sendMessage()",id:"sendmessage",level:3},{value:"Parameters",id:"parameters-6",level:4},{value:"message",id:"message",level:5},{value:"Returns",id:"returns-11",level:4},{value:"setTokenCallback()",id:"settokencallback",level:3},{value:"Parameters",id:"parameters-7",level:4},{value:"tokenCallback",id:"tokencallback-1",level:5},{value:"tokenCallback",id:"tokencallback-2",level:6},{value:"Returns",id:"returns-12",level:4}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",h6:"h6",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsx)(n.header,{children:(0,c.jsx)(n.h1,{id:"class-llmmodule",children:"Class: LLMModule"})}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L10",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:10"})]}),"\n",(0,c.jsx)(n.p,{children:"Module for managing a Large Language Model (LLM) instance."}),"\n",(0,c.jsx)(n.h2,{id:"constructors",children:"Constructors"}),"\n",(0,c.jsx)(n.h3,{id:"constructor",children:"Constructor"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"new LLMModule"}),"(",(0,c.jsx)(n.code,{children:"optionalCallbacks"}),"): ",(0,c.jsx)(n.code,{children:"LLMModule"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L19",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:19"})]}),"\n",(0,c.jsxs)(n.p,{children:["Creates a new instance of ",(0,c.jsx)(n.code,{children:"LLMModule"})," with optional callbacks."]}),"\n",(0,c.jsx)(n.h4,{id:"parameters",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"optionalcallbacks",children:"optionalCallbacks"}),"\n",(0,c.jsx)(n.p,{children:"Object containing optional callbacks."}),"\n",(0,c.jsx)(n.h6,{id:"messagehistorycallback",children:"messageHistoryCallback?"}),"\n",(0,c.jsxs)(n.p,{children:["(",(0,c.jsx)(n.code,{children:"messageHistory"}),") => ",(0,c.jsx)(n.code,{children:"void"})]}),"\n",(0,c.jsxs)(n.p,{children:["An optional function called on every finished message (",(0,c.jsx)(n.code,{children:"Message[]"}),").\nReturns the entire message history."]}),"\n",(0,c.jsx)(n.h6,{id:"tokencallback",children:"tokenCallback?"}),"\n",(0,c.jsxs)(n.p,{children:["(",(0,c.jsx)(n.code,{children:"token"}),") => ",(0,c.jsx)(n.code,{children:"void"})]}),"\n",(0,c.jsxs)(n.p,{children:["An optional function that will be called on every generated token (",(0,c.jsx)(n.code,{children:"string"}),") with that token as its only argument."]}),"\n",(0,c.jsx)(n.h4,{id:"returns",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"LLMModule"})}),"\n",(0,c.jsx)(n.p,{children:"A new LLMModule instance."}),"\n",(0,c.jsx)(n.h2,{id:"methods",children:"Methods"}),"\n",(0,c.jsx)(n.h3,{id:"configure",children:"configure()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"configure"}),"(",(0,c.jsx)(n.code,{children:"configuration"}),"): ",(0,c.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L81",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:81"})]}),"\n",(0,c.jsxs)(n.p,{children:["Configures chat and tool calling and generation settings.\nSee ",(0,c.jsx)(n.a,{href:"https://docs.swmansion.com/react-native-executorch/docs/hooks/natural-language-processing/useLLM#configuring-the-model",children:"Configuring the model"})," for details."]}),"\n",(0,c.jsx)(n.h4,{id:"parameters-1",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"configuration",children:"configuration"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/LLMConfig",children:(0,c.jsx)(n.code,{children:"LLMConfig"})})}),"\n",(0,c.jsxs)(n.p,{children:["Configuration object containing ",(0,c.jsx)(n.code,{children:"chatConfig"}),", ",(0,c.jsx)(n.code,{children:"toolsConfig"}),", and ",(0,c.jsx)(n.code,{children:"generationConfig"}),"."]}),"\n",(0,c.jsx)(n.h4,{id:"returns-1",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"void"})}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"delete",children:"delete()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"delete"}),"(): ",(0,c.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L174",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:174"})]}),"\n",(0,c.jsx)(n.p,{children:"Method to delete the model from memory.\nNote you cannot delete model while it's generating.\nYou need to interrupt it first and make sure model stopped generation."}),"\n",(0,c.jsx)(n.h4,{id:"returns-2",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"void"})}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"deletemessage",children:"deleteMessage()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"deleteMessage"}),"(",(0,c.jsx)(n.code,{children:"index"}),"): ",(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Message",children:(0,c.jsx)(n.code,{children:"Message"})}),"[]"]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L130",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:130"})]}),"\n",(0,c.jsxs)(n.p,{children:["Deletes all messages starting with message on ",(0,c.jsx)(n.code,{children:"index"})," position.\nAfter deletion it will call ",(0,c.jsx)(n.code,{children:"messageHistoryCallback()"})," containing new history.\nIt also returns it."]}),"\n",(0,c.jsx)(n.h4,{id:"parameters-2",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"index",children:"index"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"number"})}),"\n",(0,c.jsx)(n.p,{children:"The index of the message to delete from history."}),"\n",(0,c.jsx)(n.h4,{id:"returns-3",children:"Returns"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Message",children:(0,c.jsx)(n.code,{children:"Message"})}),"[]"]}),"\n",(0,c.jsxs)(n.ul,{children:["\n",(0,c.jsx)(n.li,{children:"Updated message history after deletion."}),"\n"]}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"forward",children:"forward()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"forward"}),"(",(0,c.jsx)(n.code,{children:"input"}),"): ",(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"string"}),">"]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L94",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:94"})]}),"\n",(0,c.jsxs)(n.p,{children:["Runs model inference with raw input string.\nYou need to provide entire conversation and prompt (in correct format and with special tokens!) in input string to this method.\nIt doesn't manage conversation context. It is intended for users that need access to the model itself without any wrapper.\nIf you want a simple chat with model the consider using ",(0,c.jsx)(n.code,{children:"sendMessage"})]}),"\n",(0,c.jsx)(n.h4,{id:"parameters-3",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"input",children:"input"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"string"})}),"\n",(0,c.jsx)(n.p,{children:"Raw input string containing the prompt and conversation history."}),"\n",(0,c.jsx)(n.h4,{id:"returns-4",children:"Returns"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"string"}),">"]}),"\n",(0,c.jsx)(n.p,{children:"The generated response as a string."}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"generate",children:"generate()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"generate"}),"(",(0,c.jsx)(n.code,{children:"messages"}),", ",(0,c.jsx)(n.code,{children:"tools?"}),"): ",(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"string"}),">"]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L105",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:105"})]}),"\n",(0,c.jsxs)(n.p,{children:["Runs model to complete chat passed in ",(0,c.jsx)(n.code,{children:"messages"})," argument. It doesn't manage conversation context."]}),"\n",(0,c.jsx)(n.h4,{id:"parameters-4",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"messages",children:"messages"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Message",children:(0,c.jsx)(n.code,{children:"Message"})}),"[]"]}),"\n",(0,c.jsx)(n.p,{children:"Array of messages representing the chat history."}),"\n",(0,c.jsx)(n.h5,{id:"tools",children:"tools?"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"Object"}),"[]"]}),"\n",(0,c.jsx)(n.p,{children:"Optional array of tools that can be used during generation."}),"\n",(0,c.jsx)(n.h4,{id:"returns-5",children:"Returns"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"string"}),">"]}),"\n",(0,c.jsx)(n.p,{children:"The generated response as a string."}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"getgeneratedtokencount",children:"getGeneratedTokenCount()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"getGeneratedTokenCount"}),"(): ",(0,c.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L147",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:147"})]}),"\n",(0,c.jsx)(n.p,{children:"Returns the number of tokens generated in the last response."}),"\n",(0,c.jsx)(n.h4,{id:"returns-6",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"number"})}),"\n",(0,c.jsx)(n.p,{children:"The count of generated tokens."}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"getprompttokenscount",children:"getPromptTokensCount()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"getPromptTokensCount"}),"(): ",(0,c.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L156",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:156"})]}),"\n",(0,c.jsx)(n.p,{children:"Returns the number of prompt tokens in the last message."}),"\n",(0,c.jsx)(n.h4,{id:"returns-7",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"number"})}),"\n",(0,c.jsx)(n.p,{children:"The count of prompt token."}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"gettotaltokenscount",children:"getTotalTokensCount()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"getTotalTokensCount"}),"(): ",(0,c.jsx)(n.code,{children:"number"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L165",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:165"})]}),"\n",(0,c.jsx)(n.p,{children:"Returns the number of total tokens from the previous generation. This is a sum of prompt tokens and generated tokens."}),"\n",(0,c.jsx)(n.h4,{id:"returns-8",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"number"})}),"\n",(0,c.jsx)(n.p,{children:"The count of prompt and generated tokens."}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"interrupt",children:"interrupt()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"interrupt"}),"(): ",(0,c.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L138",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:138"})]}),"\n",(0,c.jsx)(n.p,{children:"Interrupts model generation. It may return one more token after interrupt."}),"\n",(0,c.jsx)(n.h4,{id:"returns-9",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"void"})}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"load",children:"load()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"load"}),"(",(0,c.jsx)(n.code,{children:"model"}),", ",(0,c.jsx)(n.code,{children:"onDownloadProgressCallback"}),"): ",(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"void"}),">"]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L48",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:48"})]}),"\n",(0,c.jsx)(n.p,{children:"Loads the LLM model and tokenizer."}),"\n",(0,c.jsx)(n.h4,{id:"parameters-5",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"model",children:"model"}),"\n",(0,c.jsx)(n.p,{children:"Object containing model, tokenizer, and tokenizer config sources."}),"\n",(0,c.jsx)(n.h6,{id:"modelsource",children:"modelSource"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/type-aliases/ResourceSource",children:(0,c.jsx)(n.code,{children:"ResourceSource"})})}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"ResourceSource"})," that specifies the location of the model binary."]}),"\n",(0,c.jsx)(n.h6,{id:"tokenizerconfigsource",children:"tokenizerConfigSource"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/type-aliases/ResourceSource",children:(0,c.jsx)(n.code,{children:"ResourceSource"})})}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"ResourceSource"})," pointing to the JSON file which contains the tokenizer config."]}),"\n",(0,c.jsx)(n.h6,{id:"tokenizersource",children:"tokenizerSource"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/type-aliases/ResourceSource",children:(0,c.jsx)(n.code,{children:"ResourceSource"})})}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"ResourceSource"})," pointing to the JSON file which contains the tokenizer."]}),"\n",(0,c.jsx)(n.h5,{id:"ondownloadprogresscallback",children:"onDownloadProgressCallback"}),"\n",(0,c.jsxs)(n.p,{children:["(",(0,c.jsx)(n.code,{children:"progress"}),") => ",(0,c.jsx)(n.code,{children:"void"})]}),"\n",(0,c.jsx)(n.p,{children:"Optional callback to track download progress (value between 0 and 1)."}),"\n",(0,c.jsx)(n.h4,{id:"returns-10",children:"Returns"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.code,{children:"void"}),">"]}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"sendmessage",children:"sendMessage()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"sendMessage"}),"(",(0,c.jsx)(n.code,{children:"message"}),"): ",(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Message",children:(0,c.jsx)(n.code,{children:"Message"})}),"[]>"]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L117",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:117"})]}),"\n",(0,c.jsxs)(n.p,{children:["Method to add user message to conversation.\nAfter model responds it will call ",(0,c.jsx)(n.code,{children:"messageHistoryCallback()"})," containing both user message and model response.\nIt also returns them."]}),"\n",(0,c.jsx)(n.h4,{id:"parameters-6",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"message",children:"message"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"string"})}),"\n",(0,c.jsx)(n.p,{children:"The message string to send."}),"\n",(0,c.jsx)(n.h4,{id:"returns-11",children:"Returns"}),"\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.code,{children:"Promise"}),"<",(0,c.jsx)(n.a,{href:"/react-native-executorch/docs/next/api-reference/interfaces/Message",children:(0,c.jsx)(n.code,{children:"Message"})}),"[]>"]}),"\n",(0,c.jsxs)(n.ul,{children:["\n",(0,c.jsx)(n.li,{children:"Updated message history including the new user message and model response."}),"\n"]}),"\n",(0,c.jsx)(n.hr,{}),"\n",(0,c.jsx)(n.h3,{id:"settokencallback",children:"setTokenCallback()"}),"\n",(0,c.jsxs)(n.blockquote,{children:["\n",(0,c.jsxs)(n.p,{children:[(0,c.jsx)(n.strong,{children:"setTokenCallback"}),"(",(0,c.jsx)(n.code,{children:"tokenCallback"}),"): ",(0,c.jsx)(n.code,{children:"void"})]}),"\n"]}),"\n",(0,c.jsxs)(n.p,{children:["Defined in: ",(0,c.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/326d6344894d75625c600d5988666e215a32d466/packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts#L67",children:"packages/react-native-executorch/src/modules/natural_language_processing/LLMModule.ts:67"})]}),"\n",(0,c.jsx)(n.p,{children:"Sets new token callback invoked on every token batch."}),"\n",(0,c.jsx)(n.h4,{id:"parameters-7",children:"Parameters"}),"\n",(0,c.jsx)(n.h5,{id:"tokencallback-1",children:"tokenCallback"}),"\n",(0,c.jsx)(n.p,{children:"Callback function to handle new tokens."}),"\n",(0,c.jsx)(n.h6,{id:"tokencallback-2",children:"tokenCallback"}),"\n",(0,c.jsxs)(n.p,{children:["(",(0,c.jsx)(n.code,{children:"token"}),") => ",(0,c.jsx)(n.code,{children:"void"})]}),"\n",(0,c.jsx)(n.h4,{id:"returns-12",children:"Returns"}),"\n",(0,c.jsx)(n.p,{children:(0,c.jsx)(n.code,{children:"void"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,c.jsx)(n,{...e,children:(0,c.jsx)(d,{...e})}):d(e)}}}]);
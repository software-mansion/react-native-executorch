"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7667],{4728:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"natural-language-processing/useTextEmbeddings","title":"useTextEmbeddings","description":"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch\'s useTextEmbeddings hook.","source":"@site/versioned_docs/version-0.4.x/natural-language-processing/useTextEmbeddings.md","sourceDirName":"natural-language-processing","slug":"/natural-language-processing/useTextEmbeddings","permalink":"/react-native-executorch/docs/0.4.x/natural-language-processing/useTextEmbeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.4.x/natural-language-processing/useTextEmbeddings.md","tags":[],"version":"0.4.x","frontMatter":{"title":"useTextEmbeddings","keywords":["text embedding","text embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai"],"description":"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch\'s useTextEmbeddings hook."},"sidebar":"tutorialSidebar","previous":{"title":"useSpeechToText","permalink":"/react-native-executorch/docs/0.4.x/natural-language-processing/useSpeechToText"},"next":{"title":"useTokenizer","permalink":"/react-native-executorch/docs/0.4.x/natural-language-processing/useTokenizer"}}');var r=t(4848),i=t(8453);const d={title:"useTextEmbeddings",keywords:["text embedding","text embeddings","embeddings","react native","executorch","ai","machine learning","on-device","mobile ai"],description:"Learn how to use text embeddings models in your React Native applications with React Native ExecuTorch's useTextEmbeddings hook."},l=void 0,c={},o=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Text Embedding is the process of converting text into a numerical representation. This representation can be used for various natural language processing tasks, such as semantic search, text classification, and clustering."}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-all-MiniLM-L6-v2",children:"Hugging Face repository"}),". You can also use ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/release/0.4/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,r.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import {\n  useTextEmbeddings,\n  ALL_MINILM_L6_V2,\n  ALL_MINILM_L6_V2_TOKENIZER,\n} from 'react-native-executorch';\n\nconst model = useTextEmbeddings({\n  modelSource: ALL_MINILM_L6_V2,\n  tokenizerSource: ALL_MINILM_L6_V2_TOKENIZER,\n});\n\ntry {\n  const embedding = await model.forward('Hello World!');\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,r.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"modelSource"})}),"\nA string that specifies the location of the model binary. For more information, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/0.4.x/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerSource"})}),"\nA string that specifies the location of the tokenizer JSON file."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,r.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"forward"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(input: string) => Promise<number[]>"})}),(0,r.jsxs)(n.td,{children:["Executes the model's forward pass, where ",(0,r.jsx)(n.code,{children:"input"})," is a text that will be embedded."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"error"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"string | null"})}),(0,r.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isGenerating"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isReady"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"downloadProgress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"number"})}),(0,r.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,r.jsxs)(n.p,{children:["To run the model, you can use the ",(0,r.jsx)(n.code,{children:"forward"})," method. It accepts one argument, which is a string representing the text you want to embed. The function returns a promise, which can resolve either to an error or an array of numbers representing the embedding."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"The returned embedding vector is normalized, meaning that its length is equal to 1. This allows for easier comparison of vectors using cosine similarity, just calculate the dot product of two vectors to get the cosine similarity score."})}),"\n",(0,r.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import {\n  useTextEmbeddings,\n  ALL_MINILM_L6_V2,\n  ALL_MINILM_L6_V2_TOKENIZER,\n} from 'react-native-executorch';\n\nconst dotProduct = (a: number[], b: number[]) =>\n  a.reduce((sum, val, i) => sum + val * b[i], 0);\n\nfunction App() {\n  const model = useTextEmbeddings({\n    modelSource: ALL_MINILM_L6_V2,\n    tokenizerSource: ALL_MINILM_L6_V2_TOKENIZER,\n  });\n\n  ...\n\n  try {\n    const helloWorldEmbedding = await model.forward('Hello World!');\n    const goodMorningEmbedding = await model.forward('Good Morning!');\n\n    // The embeddings are normalized, so we can use dot product to calculate cosine similarity\n    const similarity = dotProduct(\n      helloWorldEmbedding,\n      goodMorningEmbedding\n    );\n\n    console.log(`Cosine similarity: ${similarity}`);\n  } catch (error) {\n    console.error(error);\n  }\n\n  ...\n}\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Language"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Max Tokens"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Embedding Dimensions"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"256"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"384"}),(0,r.jsx)(n.td,{children:"All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-mpnet-base-v2",children:"all-mpnet-base-v2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"384"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"768"}),(0,r.jsx)(n.td,{children:"All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1",children:"multi-qa-MiniLM-L6-cos-v1"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"511"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"384"}),(0,r.jsx)(n.td,{children:"This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1",children:"multi-qa-mpnet-base-dot-v1"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"English"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"512"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"768"}),(0,r.jsx)(n.td,{children:"This model was tuned for semantic search: Given a query/question, it can find relevant passages. It was trained on a large and diverse set of (question, answer) pairs."})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"Max Tokens"})})," - the maximum number of tokens that can be processed by the model. If the input text exceeds this limit, it will be truncated."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"Embedding Dimensions"})})," - the size of the output embedding vector. This is the number of dimensions in the vector representation of the input text."]}),"\n",(0,r.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MINILM_L6_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"91"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MPNET_BASE_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"438"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MINILM_L6_COS_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"91"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MPNET_BASE_DOT_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"438"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [MB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MINILM_L6_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"150"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"190"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MPNET_BASE_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"520"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"470"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MINILM_L6_COS_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"160"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"225"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MPNET_BASE_DOT_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"540"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"500"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,r.jsx)(n.admonition,{title:"warning",type:"warning",children:(0,r.jsx)(n.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization."})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 17 Pro (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MINILM_L6_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"50"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"58"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"84"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"58"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"58"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ALL_MPNET_BASE_V2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"352"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"428"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"879"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"483"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"517"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MINILM_L6_COS_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"133"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"161"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"269"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"151"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"155"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MULTI_QA_MPNET_BASE_DOT_V1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"502"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"796"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1216"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"915"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"713"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>l});var s=t(6540);const r={},i=s.createContext(r);function d(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);
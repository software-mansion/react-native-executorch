"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[329],{3041:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"typescript-api/SpeechToTextModule","title":"SpeechToTextModule","description":"TypeScript API implementation of the useSpeechToText hook.","source":"@site/docs/typescript-api/SpeechToTextModule.md","sourceDirName":"typescript-api","slug":"/typescript-api/SpeechToTextModule","permalink":"/react-native-executorch/docs/next/typescript-api/SpeechToTextModule","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/typescript-api/SpeechToTextModule.md","tags":[],"version":"current","frontMatter":{"title":"SpeechToTextModule"},"sidebar":"tutorialSidebar","previous":{"title":"LLMModule","permalink":"/react-native-executorch/docs/next/typescript-api/LLMModule"},"next":{"title":"TextEmbeddingsModule","permalink":"/react-native-executorch/docs/next/typescript-api/TextEmbeddingsModule"}}');var o=i(4848),r=i(8453);const a={title:"SpeechToTextModule"},s=void 0,d={},c=[{value:"Reference",id:"reference",level:2},{value:"Methods",id:"methods",level:3},{value:"Loading the model",id:"loading-the-model",level:2},{value:"Running the model",id:"running-the-model",level:2},{value:"Multilingual transcription",id:"multilingual-transcription",level:3},{value:"Obtaining the input",id:"obtaining-the-input",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["TypeScript API implementation of the ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/natural-language-processing/useSpeechToText",children:"useSpeechToText"})," hook."]}),"\n",(0,o.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextModule } from 'react-native-executorch';\nimport { AudioContext } from 'react-native-audio-api';\nimport * as FileSystem from 'expo-file-system';\n\nconst loadAudio = async (url: string) => {\n  const audioContext = new AudioContext({ sampleRate: 16e3 });\n  const audioBuffer = await FileSystem.downloadAsync(\n    url,\n    FileSystem.documentDirectory + '_tmp_transcribe_audio.mp3'\n  ).then(({ uri }) => {\n    return audioContext.decodeAudioDataSource(uri);\n  });\n  return audioBuffer?.getChannelData(0);\n};\n\nconst audioUrl = 'https://some-audio-url.com/file.mp3'; // URL with audio to transcribe\n\n// Loading the model\nconst onSequenceUpdate = (sequence) => {\n  console.log(sequence);\n};\nawait SpeechToTextModule.load('moonshine', onSequenceUpdate);\n\n// Loading the audio and running the model\nconst waveform = await loadAudio(audioUrl);\nconst transcribedText = await SpeechToTextModule.transcribe(waveform);\n"})}),"\n",(0,o.jsx)(n.h3,{id:"methods",children:"Methods"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Method"}),(0,o.jsx)(n.th,{children:"Type"}),(0,o.jsx)(n.th,{children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"load"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)("code",{children:"(modelName: 'whisper' &#124 'moonshine' &#124 'whisperMultilingual', transcribeCallback?: (sequence: string) => void, modelDownloadProgressCallback?: (downloadProgress: number) => void, encoderSource?: ResourceSource, decoderSource?: ResourceSource, tokenizerSource?: ResourceSource)"})}),(0,o.jsxs)(n.td,{children:["Loads the model specified with ",(0,o.jsx)(n.code,{children:"modelName"}),", where ",(0,o.jsx)(n.code,{children:"encoderSource"}),", ",(0,o.jsx)(n.code,{children:"decoderSource"}),", ",(0,o.jsx)(n.code,{children:"tokenizerSource"})," are strings specifying the location of the binaries for the models. ",(0,o.jsx)(n.code,{children:"modelDownloadProgressCallback"})," allows you to monitor the current progress of the model download, while ",(0,o.jsx)(n.code,{children:"transcribeCallback"})," is invoked with each generated token"]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"transcribe"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(waveform: number[], audioLanguage?: SpeechToTextLanguage): Promise<string>"})}),(0,o.jsx)(n.td,{children:"Starts a transcription process for a given input array, which should be a waveform at 16kHz. Resolves a promise with the output transcription when the model is finished. For multilingual models, you have to specify the audioLanguage flag, which is the language of the spoken language in the audio."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"streamingTranscribe"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(streamingAction: STREAMING_ACTION, waveform?: number[], audioLanguage?: SpeechToTextLanguage) => Promise<string>"})}),(0,o.jsxs)(n.td,{children:["This allows for running transcription process on-line, which means where the whole audio is not known beforehand i.e. when transcribing from a live microphone feed. ",(0,o.jsx)(n.code,{children:"streamingAction"})," defines the type of package sent to the model: ",(0,o.jsxs)("li",{children:[(0,o.jsx)(n.code,{children:"START"})," - initializes the process, allows for optional ",(0,o.jsx)(n.code,{children:"waveform"})," data"]}),(0,o.jsxs)("li",{children:[(0,o.jsx)(n.code,{children:"DATA"})," - this package should contain consecutive audio data chunks sampled in 16k Hz"]}),(0,o.jsxs)("li",{children:[(0,o.jsx)(n.code,{children:"STOP"})," - the last data chunk for this transcription, ends the transcription process and flushes internal buffers"]})," Each call returns most recent transcription. Returns error when called when module is in use (i.e. processing ",(0,o.jsx)(n.code,{children:"transcribe"})," call)"]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"encode"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(waveform: number[]) => Promise<null>"})}),(0,o.jsx)(n.td,{children:"Runs the encoding part of the model. It doesn't return the encodings. Instead, it stores the result internally, reducing data transfer overhead."})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"decode"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"(tokens: number[]) => Promise<number>"})}),(0,o.jsxs)(n.td,{children:["Runs the decoder of the model. Returns a single token representing a next token in the output. It uses internal cached encodings from the most recent ",(0,o.jsx)(n.code,{children:"encode"})," call, meaning that you have to call ",(0,o.jsx)(n.code,{children:"encode"})," prior to decoding."]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"configureStreaming"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)("code",{children:"(overlapSeconds?: number, windowSize?: number, streamingConfig?: 'fast' | 'balanced' | 'quality') => void"})}),(0,o.jsxs)(n.td,{children:["Configures options for the streaming algorithm: ",(0,o.jsxs)("ul",{children:[(0,o.jsxs)("li",{children:[(0,o.jsx)(n.code,{children:"overlapSeconds"})," determines how much adjacent audio chunks overlap (increasing it slows down transcription, decreases probability of weird wording at the chunks intersection, setting it larger than 3 seconds generally is discouraged), "]}),(0,o.jsxs)("li",{children:[(0,o.jsx)(n.code,{children:"windowSize"})," describes size of the audio chunks (increasing it speeds up the end to end transcription time, but increases latency for the first token to be returned),"]}),(0,o.jsxs)("li",{children:[" ",(0,o.jsx)(n.code,{children:"streamingConfig"})," predefined configs for ",(0,o.jsx)(n.code,{children:"windowSize"})," and ",(0,o.jsx)(n.code,{children:"overlapSeconds"})," values."]})]})," Keep ",(0,o.jsx)(n.code,{children:"windowSize + 2 * overlapSeconds <= 30"}),"."]})]})]})]}),"\n",(0,o.jsxs)(i,{children:[(0,o.jsx)("summary",{children:"Type definitions"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"type ResourceSource = string | number | object;\n\nenum STREAMING_ACTION {\n  START,\n  DATA,\n  STOP,\n}\n\nenum SpeechToTextLanguage {\n  Afrikaans = 'af',\n  Albanian = 'sq',\n  Arabic = 'ar',\n  Armenian = 'hy',\n  Azerbaijani = 'az',\n  Basque = 'eu',\n  Belarusian = 'be',\n  Bengali = 'bn',\n  Bosnian = 'bs',\n  Bulgarian = 'bg',\n  Burmese = 'my',\n  Catalan = 'ca',\n  Chinese = 'zh',\n  Croatian = 'hr',\n  Czech = 'cs',\n  Danish = 'da',\n  Dutch = 'nl',\n  Estonian = 'et',\n  English = 'en',\n  Finnish = 'fi',\n  French = 'fr',\n  Galician = 'gl',\n  Georgian = 'ka',\n  German = 'de',\n  Greek = 'el',\n  Gujarati = 'gu',\n  HaitianCreole = 'ht',\n  Hebrew = 'he',\n  Hindi = 'hi',\n  Hungarian = 'hu',\n  Icelandic = 'is',\n  Indonesian = 'id',\n  Italian = 'it',\n  Japanese = 'ja',\n  Kannada = 'kn',\n  Kazakh = 'kk',\n  Khmer = 'km',\n  Korean = 'ko',\n  Lao = 'lo',\n  Latvian = 'lv',\n  Lithuanian = 'lt',\n  Macedonian = 'mk',\n  Malagasy = 'mg',\n  Malay = 'ms',\n  Malayalam = 'ml',\n  Maltese = 'mt',\n  Marathi = 'mr',\n  Nepali = 'ne',\n  Norwegian = 'no',\n  Persian = 'fa',\n  Polish = 'pl',\n  Portuguese = 'pt',\n  Punjabi = 'pa',\n  Romanian = 'ro',\n  Russian = 'ru',\n  Serbian = 'sr',\n  Sinhala = 'si',\n  Slovak = 'sk',\n  Slovenian = 'sl',\n  Spanish = 'es',\n  Sundanese = 'su',\n  Swahili = 'sw',\n  Swedish = 'sv',\n  Tagalog = 'tl',\n  Tajik = 'tg',\n  Tamil = 'ta',\n  Telugu = 'te',\n  Thai = 'th',\n  Turkish = 'tr',\n  Ukrainian = 'uk',\n  Urdu = 'ur',\n  Uzbek = 'uz',\n  Vietnamese = 'vi',\n  Welsh = 'cy',\n  Yiddish = 'yi',\n}\n"})})]}),"\n",(0,o.jsx)(n.h2,{id:"loading-the-model",children:"Loading the model"}),"\n",(0,o.jsxs)(n.p,{children:["To load the model, use the ",(0,o.jsx)(n.code,{children:"load"})," method. The required argument is ",(0,o.jsx)(n.code,{children:"modelName"}),", which serves as an identifier for which model to use. It also accepts accepts optional arguments such as ",(0,o.jsx)(n.code,{children:"encoderSource"}),", ",(0,o.jsx)(n.code,{children:"decoderSource"}),", ",(0,o.jsx)(n.code,{children:"tokenizerSource"})," which are strings that specify the location of the binaries for the model. For more information, take a look at ",(0,o.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page. This method returns a promise, which can resolve to an error or void."]}),"\n",(0,o.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,o.jsxs)(n.p,{children:["To run the model, you can use the ",(0,o.jsx)(n.code,{children:"transcribe"})," method. It accepts one argument, which is an array of numbers representing a waveform at 16kHz sampling rate. The method returns a promise, which can resolve either to an error or a string containing the output text."]}),"\n",(0,o.jsx)(n.h3,{id:"multilingual-transcription",children:"Multilingual transcription"}),"\n",(0,o.jsxs)(n.p,{children:["If you aim to obtain a transcription in other languages than English, in v0.4.0 we introduced a new model - ",(0,o.jsx)(n.code,{children:"whisperMultilingual"}),", a multilingual version of Whisper. To obtain the output text in your desired language, make sure pass ",(0,o.jsx)(n.code,{children:"audioLanguage"})," to ",(0,o.jsx)(n.code,{children:"transcribe"}),". You should not pass this flag if you're using a non-multilingual model. For example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { SpeechToTextLanguage } from 'react-native-executorch';\n\n// Rest of your code...\nconst mySpanishAudio = 'https://some-audio-url.com/spanish-file.mp3';\nawait model.transcribe(mySpanishAudio, SpeechToTextLanguage.Spanish);\n// Rest of your code...\n"})}),"\n",(0,o.jsx)(n.h2,{id:"obtaining-the-input",children:"Obtaining the input"}),"\n",(0,o.jsxs)(n.p,{children:["You need to parse audio to waveform in 16kHz, you can do that in any way most suitable to you. In the snippet at the top of the page we provide an example using ",(0,o.jsx)(n.code,{children:"react-native-audio-api"}),". Once you have the waveform simply pass it as the only argument to ",(0,o.jsx)(n.code,{children:"transcribe"})," method."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);
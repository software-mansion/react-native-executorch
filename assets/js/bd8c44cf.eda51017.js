"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7729],{9706:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"0.1.x","label":"0.1.x","banner":"unmaintained","badge":true,"noIndex":false,"className":"docs-version-0.1.x","isLast":false,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Fundamentals","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Getting Started","href":"/react-native-executorch/docs/0.1.x/fundamentals/getting-started","docId":"fundamentals/getting-started","unlisted":false}],"href":"/react-native-executorch/docs/0.1.x/category/fundamentals"},{"type":"category","label":"Guides","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Running LLMs","href":"/react-native-executorch/docs/0.1.x/guides/running-llms","docId":"guides/running-llms","unlisted":false},{"type":"link","label":"Exporting Llama","href":"/react-native-executorch/docs/0.1.x/guides/exporting-llama","docId":"guides/exporting-llama","unlisted":false}],"href":"/react-native-executorch/docs/0.1.x/category/guides"}]},"docs":{"fundamentals/getting-started":{"id":"fundamentals/getting-started","title":"Getting Started","description":"What is ExecuTorch?","sidebar":"tutorialSidebar"},"guides/exporting-llama":{"id":"guides/exporting-llama","title":"Exporting Llama","description":"In order to make the process of export as simple as possible for you, we created a script that runs a Docker container and exports the model.","sidebar":"tutorialSidebar"},"guides/running-llms":{"id":"guides/running-llms","title":"Running LLMs","description":"React Native ExecuTorch supports Llama 3.2 models, including quantized versions. Before getting started, you\u2019ll need to obtain the .pte binary\u2014a serialized model\u2014and the tokenizer. There are various ways to accomplish this:","sidebar":"tutorialSidebar"}}}}')}}]);
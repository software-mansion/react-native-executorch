"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4058],{4605:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"hooks/computer-vision/useClassification","title":"useClassification","description":"Image classification is the process of assigning a label to an image that best describes its contents. For example, when given an image of a puppy, the image classifier should assign the puppy class to that image.","source":"@site/versioned_docs/version-0.6.0/02-hooks/02-computer-vision/useClassification.md","sourceDirName":"02-hooks/02-computer-vision","slug":"/hooks/computer-vision/useClassification","permalink":"/react-native-executorch/docs/hooks/computer-vision/useClassification","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/versioned_docs/version-0.6.0/02-hooks/02-computer-vision/useClassification.md","tags":[],"version":"0.6.0","frontMatter":{"title":"useClassification"},"sidebar":"tutorialSidebar","previous":{"title":"Computer Vision","permalink":"/react-native-executorch/docs/category/computer-vision"},"next":{"title":"useImageEmbeddings","permalink":"/react-native-executorch/docs/hooks/computer-vision/useImageEmbeddings"}}');var i=t(4848),r=t(8453);const o={title:"useClassification"},c=void 0,l={},d=[{value:"Reference",id:"reference",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Running the model",id:"running-the-model",level:2},{value:"Example",id:"example",level:2},{value:"Supported models",id:"supported-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Image classification is the process of assigning a label to an image that best describes its contents. For example, when given an image of a puppy, the image classifier should assign the puppy class to that image."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Usually, the class with the highest probability is the one that is assigned to an image. However, if there are multiple classes with comparatively high probabilities, this may indicate that the model is not confident in its prediction."})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["It is recommended to use models provided by us, which are available at our ",(0,i.jsx)(n.a,{href:"https://huggingface.co/collections/software-mansion/classification-68d0ea49b5c7de8a3cae1e68",children:"Hugging Face repository"}),". You can also use ",(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]})}),"\n",(0,i.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useClassification, EFFICIENTNET_V2_S } from 'react-native-executorch';\n\nconst model = useClassification({ model: EFFICIENTNET_V2_S });\n\nconst imageUri = 'file::///Users/.../cute_puppy.png';\n\ntry {\n  const classesWithProbabilities = await model.forward(imageUri);\n} catch (error) {\n  console.error(error);\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"model"})})," - Object containing the model source."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"modelSource"})})," - A string that specifies the location of the model binary."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,i.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,i.jsx)(n.a,{href:"/react-native-executorch/docs/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,i.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Field"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"forward"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"(imageSource: string) => Promise<{ [category: string]: number }>"})}),(0,i.jsxs)(n.td,{children:["Executes the model's forward pass, where ",(0,i.jsx)(n.code,{children:"imageSource"})," can be a fetchable resource or a Base64-encoded string."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"error"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)("code",{children:"string | null"})}),(0,i.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"isGenerating"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"boolean"})}),(0,i.jsx)(n.td,{children:"Indicates whether the model is currently processing an inference."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"isReady"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"boolean"})}),(0,i.jsx)(n.td,{children:"Indicates whether the model has successfully loaded and is ready for inference."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"downloadProgress"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"number"})}),(0,i.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1."})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"running-the-model",children:"Running the model"}),"\n",(0,i.jsxs)(n.p,{children:["To run the model, you can use the ",(0,i.jsx)(n.code,{children:"forward"})," method. It accepts one argument, which is the image. The image can be a remote URL, a local file URI, or a base64-encoded image. The function returns a promise, which can resolve either to an error or an object containing categories with their probabilities."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Images from external sources are stored in your application's temporary directory."})}),"\n",(0,i.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { useClassification, EFFICIENTNET_V2_S } from 'react-native-executorch';\n\nfunction App() {\n  const model = useClassification({ model: EFFICIENTNET_V2_S });\n\n  // ...\n  const imageUri = 'file:///Users/.../cute_puppy.png';\n\n  try {\n    const classesWithProbabilities = await model.forward(imageUri);\n\n    // Extract three classes with the highest probabilities\n    const topThreeClasses = Object.entries(classesWithProbabilities)\n      .sort(([, a], [, b]) => b - a)\n      .slice(0, 3)\n      .map(([label, score]) => ({ label, score }));\n  } catch (error) {\n    console.error(error);\n  }\n  // ...\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"supported-models",children:"Supported models"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Number of classes"}),(0,i.jsx)(n.th,{children:"Class list"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_v2_s.html",children:"efficientnet_v2_s"})}),(0,i.jsx)(n.td,{children:"1000"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/common/rnexecutorch/models/classification/Constants.h",children:"ImageNet1k_v1"})})]})})]}),"\n",(0,i.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [MB]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Core ML [MB]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"EFFICIENTNET_V2_S"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"85.6"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"43.9"})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [MB]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (Core ML) [MB]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"EFFICIENTNET_V2_S"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"230"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"87"})]})})]}),"\n",(0,i.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"Times presented in the tables are measured as consecutive runs of the model. Initial run times may be up to 2x longer due to model loading and initialization."})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 17 Pro (Core ML) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (Core ML) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (Core ML) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [ms]"}),(0,i.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [ms]"})]})}),(0,i.jsx)(n.tbody,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"EFFICIENTNET_V2_S"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"64"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"68"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"217"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"205"}),(0,i.jsx)(n.td,{style:{textAlign:"center"},children:"198"})]})})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);
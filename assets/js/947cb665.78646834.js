"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2769],{2938:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"hooks/natural-language-processing/useLLM","title":"useLLM","description":"Learn how to use LLMs in your React Native applications with React Native ExecuTorch\'s useLLM hook.","source":"@site/docs/02-hooks/01-natural-language-processing/useLLM.md","sourceDirName":"02-hooks/01-natural-language-processing","slug":"/hooks/natural-language-processing/useLLM","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useLLM","draft":false,"unlisted":false,"editUrl":"https://github.com/software-mansion/react-native-executorch/edit/main/docs/docs/02-hooks/01-natural-language-processing/useLLM.md","tags":[],"version":"current","frontMatter":{"title":"useLLM","keywords":["react native","react native ai","react native llm","react native qwen","react native llama","react native executorch","executorch","pytorch","on-device ai","mobile ai","llama 3","qwen","text generation","tool calling","function calling"],"description":"Learn how to use LLMs in your React Native applications with React Native ExecuTorch\'s useLLM hook."},"sidebar":"tutorialSidebar","previous":{"title":"Natural Language Processing","permalink":"/react-native-executorch/docs/next/category/natural-language-processing"},"next":{"title":"useSpeechToText","permalink":"/react-native-executorch/docs/next/hooks/natural-language-processing/useSpeechToText"}}');var r=t(4848),i=t(8453);const o={title:"useLLM",keywords:["react native","react native ai","react native llm","react native qwen","react native llama","react native executorch","executorch","pytorch","on-device ai","mobile ai","llama 3","qwen","text generation","tool calling","function calling"],description:"Learn how to use LLMs in your React Native applications with React Native ExecuTorch's useLLM hook."},l=void 0,c={},d=[{value:"Initializing",id:"initializing",level:2},{value:"Arguments",id:"arguments",level:3},{value:"Returns",id:"returns",level:3},{value:"Functional vs managed",id:"functional-vs-managed",level:2},{value:"Functional way",id:"functional-way",level:2},{value:"Simple generation",id:"simple-generation",level:3},{value:"Interrupting the model",id:"interrupting-the-model",level:3},{value:"Tool calling",id:"tool-calling",level:3},{value:"Managed LLM Chat",id:"managed-llm-chat",level:2},{value:"Configuring the model",id:"configuring-the-model",level:3},{value:"Sending a message",id:"sending-a-message",level:3},{value:"Accessing conversation history",id:"accessing-conversation-history",level:3},{value:"Tool calling example",id:"tool-calling-example",level:3},{value:"Structured output example",id:"structured-output-example",level:3},{value:"Available models",id:"available-models",level:2},{value:"Benchmarks",id:"benchmarks",level:2},{value:"Model size",id:"model-size",level:3},{value:"Memory usage",id:"memory-usage",level:3},{value:"Inference time",id:"inference-time",level:3}];function a(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["React Native ExecuTorch supports a variety of LLMs (checkout our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion",children:"HuggingFace repository"})," for model already converted to ExecuTorch format) including Llama 3.2. Before getting started, you\u2019ll need to obtain the .pte binary\u2014a serialized model, the tokenizer and tokenizer config JSON files. There are various ways to accomplish this:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For your convenience, it's best if you use models exported by us, you can get them from our ",(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion",children:"HuggingFace repository"}),". You can also use ",(0,r.jsx)(n.a,{href:"https://github.com/software-mansion/react-native-executorch/blob/main/packages/react-native-executorch/src/constants/modelUrls.ts",children:"constants"})," shipped with our library."]}),"\n",(0,r.jsxs)(n.li,{children:["Follow the official ",(0,r.jsx)(n.a,{href:"https://github.com/pytorch/executorch/blob/release/0.7/examples/demo-apps/android/LlamaDemo/docs/delegates/xnnpack_README.md",children:"tutorial"})," made by ExecuTorch team to build the model and tokenizer yourself."]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"danger",children:(0,r.jsx)(n.p,{children:"Lower-end devices might not be able to fit LLMs into memory. We recommend using quantized models to reduce the memory footprint."})}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["Given computational constraints, our architecture is designed to support only one instance of the model runner at the time. Consequently, this means you can have only one active component leveraging ",(0,r.jsx)(n.code,{children:"useLLM"})," concurrently."]})}),"\n",(0,r.jsx)(n.h2,{id:"initializing",children:"Initializing"}),"\n",(0,r.jsx)(n.p,{children:"In order to load a model into the app, you need to run the following code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { useLLM, LLAMA3_2_1B } from 'react-native-executorch';\n\nconst llm = useLLM({ model: LLAMA3_2_1B });\n"})}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsxs)(n.p,{children:["The code snippet above fetches the model from the specified URL, loads it into memory, and returns an object with various functions and properties for controlling the model. You can monitor the loading progress by checking the ",(0,r.jsx)(n.code,{children:"llm.downloadProgress"})," and ",(0,r.jsx)(n.code,{children:"llm.isReady"})," property, and if anything goes wrong, the ",(0,r.jsx)(n.code,{children:"llm.error"})," property will contain the error message."]}),"\n",(0,r.jsx)(n.h3,{id:"arguments",children:"Arguments"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"model"})})," - Object containing the model source, tokenizer source, and tokenizer config source."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"modelSource"})})," - ",(0,r.jsx)(n.code,{children:"ResourceSource"})," that specifies the location of the model binary."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerSource"})})," - ",(0,r.jsx)(n.code,{children:"ResourceSource"})," pointing to the JSON file which contains the tokenizer."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizerConfigSource"})})," - ",(0,r.jsx)(n.code,{children:"ResourceSource"})," pointing to the JSON file which contains the tokenizer config."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"preventLoad?"})})," - Boolean that can prevent automatic model loading (and downloading the data if you load it for the first time) after running the hook."]}),"\n",(0,r.jsxs)(n.p,{children:["For more information on loading resources, take a look at ",(0,r.jsx)(n.a,{href:"/react-native-executorch/docs/next/fundamentals/loading-models",children:"loading models"})," page."]}),"\n",(0,r.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"generate()"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(messages: Message[], tools?: LLMTool[]) => Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Runs model to complete chat passed in ",(0,r.jsx)(n.code,{children:"messages"})," argument. It doesn't manage conversation context."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"interrupt()"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"() => void"})}),(0,r.jsx)(n.td,{children:"Function to interrupt the current inference."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"response"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"string"})}),(0,r.jsx)(n.td,{children:"State of the generated response. This field is updated with each token generated by the model."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"token"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"string"})}),(0,r.jsx)(n.td,{children:"The most recently generated token."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isReady"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model is ready."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"isGenerating"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"boolean"})}),(0,r.jsx)(n.td,{children:"Indicates whether the model is currently generating a response."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"downloadProgress"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"number"})}),(0,r.jsx)(n.td,{children:"Represents the download progress as a value between 0 and 1, indicating the extent of the model file retrieval."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"error"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)("code",{children:"string | null"})}),(0,r.jsx)(n.td,{children:"Contains the error message if the model failed to load."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"configure"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"({ chatConfig?: Partial<ChatConfig>, toolsConfig?: ToolsConfig }) => void"})}),(0,r.jsxs)(n.td,{children:["Configures chat and tool calling. See more details in ",(0,r.jsx)(n.a,{href:"#configuring-the-model",children:"configuring the model"}),"."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"sendMessage"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(message: string) => Promise<void>"})}),(0,r.jsxs)(n.td,{children:["Function to add user message to conversation. After model responds, ",(0,r.jsx)(n.code,{children:"messageHistory"})," will be updated with both user message and model response."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"deleteMessage"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"(index: number) => void"})}),(0,r.jsxs)(n.td,{children:["Deletes all messages starting with message on ",(0,r.jsx)(n.code,{children:"index"})," position. After deletion ",(0,r.jsx)(n.code,{children:"messageHistory"})," will be updated."]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"messageHistory"})}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"Message[]"})}),(0,r.jsxs)(n.td,{children:["History containing all messages in conversation. This field is updated after model responds to ",(0,r.jsx)(n.code,{children:"sendMessage"}),"."]})]})]})]}),"\n",(0,r.jsxs)(t,{children:[(0,r.jsx)("summary",{children:"Type definitions"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"const useLLM: ({\n  model,\n  preventLoad,\n}: {\n  model: {\n    modelSource: ResourceSource;\n    tokenizerSource: ResourceSource;\n    tokenizerConfigSource: ResourceSource;\n  };\n  preventLoad?: boolean;\n}) => LLMType;\n\ninterface LLMType {\n  messageHistory: Message[];\n  response: string;\n  token: string;\n  isReady: boolean;\n  isGenerating: boolean;\n  downloadProgress: number;\n  error: string | null;\n  configure: ({\n    chatConfig,\n    toolsConfig,\n  }: {\n    chatConfig?: Partial<ChatConfig>;\n    toolsConfig?: ToolsConfig;\n  }) => void;\n  generate: (messages: Message[], tools?: LLMTool[]) => Promise<void>;\n  sendMessage: (message: string) => Promise<void>;\n  deleteMessage: (index: number) => void;\n  interrupt: () => void;\n}\n\ntype ResourceSource = string | number | object;\n\ntype MessageRole = 'user' | 'assistant' | 'system';\n\ninterface Message {\n  role: MessageRole;\n  content: string;\n}\ninterface ChatConfig {\n  initialMessageHistory: Message[];\n  contextWindowLength: number;\n  systemPrompt: string;\n}\n\n// tool calling\ninterface ToolsConfig {\n  tools: LLMTool[];\n  executeToolCallback: (call: ToolCall) => Promise<string | null>;\n  displayToolCalls?: boolean;\n}\n\ninterface ToolCall {\n  toolName: string;\n  arguments: Object;\n}\n\ntype LLMTool = Object;\n"})})]}),"\n",(0,r.jsx)(n.h2,{id:"functional-vs-managed",children:"Functional vs managed"}),"\n",(0,r.jsx)(n.p,{children:"You can use functions returned from this hooks in two manners:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Functional/pure - we will not keep any state for you. You'll need to keep conversation history and handle function calling yourself. Use ",(0,r.jsx)(n.code,{children:"generate"})," (and rarely ",(0,r.jsx)(n.code,{children:"forward"}),") and ",(0,r.jsx)(n.code,{children:"response"}),". Note that you don't need to run ",(0,r.jsx)(n.code,{children:"configure"})," to use those. Furthermore, it will not have any effect on those functions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Managed/stateful - we will manage conversation state. Tool calls will be parsed and called automatically after passing appropriate callbacks. See more at ",(0,r.jsx)(n.a,{href:"#managed-llm-chat",children:"managed LLM chat"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"functional-way",children:"Functional way"}),"\n",(0,r.jsx)(n.h3,{id:"simple-generation",children:"Simple generation"}),"\n",(0,r.jsxs)(n.p,{children:["To perform chat completion you can use the ",(0,r.jsx)(n.code,{children:"generate"})," function. There is no return value. Instead, the ",(0,r.jsx)(n.code,{children:"response"})," value is updated with each token."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"const llm = useLLM({ model: LLAMA3_2_1B });\n\nconst handleGenerate = () => {\n  const chat = [\n    { role: 'system', content: 'You are a helpful assistant' },\n    { role: 'user', content: 'Hi!' },\n    { role: 'assistant', content: 'Hi!, how can I help you?' },\n    { role: 'user', content: 'What is the meaning of life?' },\n  ];\n\n  // Chat completion\n  llm.generate(chat);\n};\n\nreturn (\n  <View>\n    <Button onPress={handleGenerate} title=\"Generate!\" />\n    <Text>{llm.response}</Text>\n  </View>\n);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"interrupting-the-model",children:"Interrupting the model"}),"\n",(0,r.jsxs)(n.p,{children:["Sometimes, you might want to stop the model while it\u2019s generating. To do this, you can use ",(0,r.jsx)(n.code,{children:"interrupt()"}),", which will halt the model and update the response one last time."]}),"\n",(0,r.jsxs)(n.p,{children:["There are also cases when you need to check if tokens are being generated, such as to conditionally render a stop button. We\u2019ve made this easy with the ",(0,r.jsx)(n.code,{children:"isGenerating"})," property."]}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["If you try to dismount the component using this hook while generation is still going on, it will result in crash.\nYou'll need to interrupt the model first and wait until ",(0,r.jsx)(n.code,{children:"isGenerating"})," is set to false."]})}),"\n",(0,r.jsx)(n.h3,{id:"tool-calling",children:"Tool calling"}),"\n",(0,r.jsx)(n.p,{children:"Sometimes text processing capabilities of LLMs are not enough. That's when you may want to introduce tool calling (also called function calling). It allows model to use external tools to perform its tasks. The tools may be any arbitrary function that you want your model to run. It may retrieve some data from 3rd party API. It may do an action inside an app like pressing buttons or filling forms, or it may use system APIs to interact with your phone (turning on flashlight, adding events to your calendar, changing volume etc.)."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"const TOOL_DEFINITIONS: LLMTool[] = [\n  {\n    name: 'get_weather',\n    description: 'Get/check weather in given location.',\n    parameters: {\n      type: 'dict',\n      properties: {\n        location: {\n          type: 'string',\n          description: 'Location where user wants to check weather',\n        },\n      },\n      required: ['location'],\n    },\n  },\n];\n\nconst llm = useLLM({ model: HAMMER2_1_1_5B });\n\nconst handleGenerate = () => {\n  const chat = [\n    {\n      role: 'system',\n      content: `You are a helpful assistant. Current time and date: ${new Date().toString()}`,\n    },\n    {\n      role: 'user',\n      content: `Hi, what's the weather like in Cracow right now?`,\n    },\n  ];\n\n  // Chat completion\n  llm.generate(chat, TOOL_DEFINITIONS);\n};\n\nuseEffect(() => {\n  // Parse response and call tools accordingly\n  // ...\n}, [llm.response]);\n\nreturn (\n  <View>\n    <Button onPress={handleGenerate} title=\"Generate!\" />\n    <Text>{llm.response}</Text>\n  </View>\n);\n"})}),"\n",(0,r.jsx)(n.h2,{id:"managed-llm-chat",children:"Managed LLM Chat"}),"\n",(0,r.jsx)(n.h3,{id:"configuring-the-model",children:"Configuring the model"}),"\n",(0,r.jsxs)(n.p,{children:["To configure model (i.e. change system prompt, load initial conversation history or manage tool calling) you can use\n",(0,r.jsx)(n.code,{children:"configure"})," function. It accepts object with following fields:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"chatConfig"})})," - Object configuring chat management, contains following properties:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"systemPrompt"})}),' - Often used to tell the model what is its purpose, for example - "Be a helpful translator".']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"initialMessageHistory"})})," - An array of ",(0,r.jsx)(n.code,{children:"Message"})," objects that represent the conversation history. This can be used to provide initial context to the model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"contextWindowLength"})})," - The number of messages from the current conversation that the model will use to generate a response. The higher the number, the more context the model will have. Keep in mind that using larger context windows will result in longer inference time and higher memory usage."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"toolsConfig"})})," - Object configuring options for enabling and managing tool use. ",(0,r.jsx)(n.strong,{children:"It will only have effect if your model's chat template support it"}),". Contains following properties:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tools"})})," - List of objects defining tools."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"executeToolCallback"})})," - Function that accepts ",(0,r.jsx)(n.code,{children:"ToolCall"}),", executes tool and returns the string to model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"displayToolCalls"})})," - If set to true, JSON tool calls will be displayed in chat. If false, only answers will be displayed."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sending-a-message",children:"Sending a message"}),"\n",(0,r.jsx)(n.p,{children:"In order to send a message to the model, one can use the following code:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"const llm = useLLM({ model: LLAMA3_2_1B });\n\nconst send = () => {\n  const message = 'Hi, who are you?';\n  llm.sendMessage(message);\n};\n\nreturn <Button onPress={send} title=\"Generate!\" />;\n"})}),"\n",(0,r.jsx)(n.h3,{id:"accessing-conversation-history",children:"Accessing conversation history"}),"\n",(0,r.jsxs)(n.p,{children:["Behind the scenes, tokens are generated one by one, and the ",(0,r.jsx)(n.code,{children:"response"})," property is updated with each token as it\u2019s created.\nIf you want to get entire conversation you can use ",(0,r.jsx)(n.code,{children:"messageHistory"})," field:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"return (\n  <View>\n    {llm.messageHistory.map((message) => (\n      <Text>{message.content}</Text>\n    ))}\n  </View>\n);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"tool-calling-example",children:"Tool calling example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"const TOOL_DEFINITIONS: LLMTool[] = [\n  {\n    name: 'get_weather',\n    description: 'Get/check weather in given location.',\n    parameters: {\n      type: 'dict',\n      properties: {\n        location: {\n          type: 'string',\n          description: 'Location where user wants to check weather',\n        },\n      },\n      required: ['location'],\n    },\n  },\n];\n\nconst llm = useLLM({ model: HAMMER2_1_1_5B });\n\nuseEffect(() => {\n  llm.configure({\n    chatConfig: {\n      systemPrompt: `You are helpful assistant. Current time and date: ${new Date().toString()}`,\n    },\n    toolsConfig: {\n      tools: TOOL_DEFINITIONS,\n      executeToolCallback: async (call) => {\n        if (call.toolName === 'get_weather') {\n          console.log('Checking weather!');\n          // perform call to weather API\n          // ...\n          const mockResults = 'Weather is great!';\n          return mockResults;\n        }\n        return null;\n      },\n      displayToolCalls: true,\n    },\n  });\n}, []);\n\nconst send = () => {\n  const message = `Hi, what's the weather like in Cracow right now?`;\n  llm.sendMessage(message);\n};\n\nreturn (\n  <View>\n    <Button onPress={send} title=\"Generate!\" />\n    <Text>{llm.response}</Text>\n  </View>\n);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"structured-output-example",children:"Structured output example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-tsx",children:"const responseSchema: Schema = {\n  properties: {\n    username: {\n      type: 'string',\n      description: 'Name of user, that is asking a question.',\n    },\n    question: {\n      type: 'string',\n      description: 'Question that user asks.',\n    },\n    bid: {\n      type: 'number',\n      description: 'Amount of money, that user offers.',\n    },\n    currency: {\n      type: 'string',\n      description: 'Currency of offer.',\n    },\n  },\n  required: ['username', 'bid'],\n  type: 'object',\n};\n\n// alternatively use Zod\nimport * as z from 'zod/v4';\nconst responseSchemaWithZod = z.object({\n  username: z\n    .string()\n    .meta({ description: 'Name of user, that is asking a question.' }),\n  question: z.optional(\n    z.string().meta({ description: 'Question that user asks.' })\n  ),\n  bid: z.number().meta({ description: 'Amount of money, that user offers.' }),\n  currency: z.optional(z.string().meta({ description: 'Currency of offer.' })),\n});\n\nconst llm = useLLM({ model: QWEN3_4B_QUANTIZED });\n\nuseEffect(() => {\n  const formattingInstructions = getStructuredOutputPrompt(responseSchema);\n  // alternatively pass schema defined with Zod\n  //  const formattingInstructions = getStructuredOutputPrompt(responseSchemaWithZod);\n\n  // Some extra prompting to improve quality of response.\n  const prompt = `Your goal is to parse user's messages and return them in JSON format. Don't respond to user. Simply return JSON with user's question parsed. \\n${formattingInstructions}\\n /no_think`;\n\n  configure({\n    chatConfig: {\n      systemPrompt: prompt,\n    },\n  });\n}, []);\n\nuseEffect(() => {\n  const lastMessage = llm.messageHistory.at(-1);\n  if (!llm.isGenerating && lastMessage?.role === 'assistant') {\n    try {\n      const formattedOutput = fixAndValidateStructuredOutput(\n        lastMessage.content,\n        responseSchemaWithZod\n      );\n      // Zod will allow you to correctly type output\n      const formattedOutputWithZod = fixAndValidateStructuredOutput(\n        lastMessage.content,\n        responseSchema\n      );\n      console.log('Formatted output:', formattedOutput, formattedOutputWithZod);\n    } catch (e) {\n      console.log(\n        \"Error parsing output and/or output doesn't match required schema!\",\n        e\n      );\n    }\n  }\n}, [llm.messageHistory, llm.isGenerating]);\n\nconst send = () => {\n  const message = `I'm John. Is this product damaged? I can give you $100 for this.`;\n  llm.sendMessage(message);\n};\n\nreturn (\n  <View>\n    <Button onPress={send} title=\"Generate!\" />\n    <Text>{llm.response}</Text>\n  </View>\n);\n"})}),"\n",(0,r.jsx)(n.p,{children:"The response should include JSON:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "username": "John",\n  "question": "Is this product damaged?",\n  "bid": 100,\n  "currency": "USD"\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"available-models",children:"Available models"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model Family"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Sizes"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Quantized"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-hammer-2.1",children:"Hammer 2.1"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"0.5B, 1.5B, 3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-qwen-2.5",children:"Qwen 2.5"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"0.5B, 1.5B, 3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-qwen-3",children:"Qwen 3"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"0.6B, 1.7B, 4B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-phi-4-mini",children:"Phi 4 Mini"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"4B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-smolLm-2",children:"SmolLM 2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"135M, 360M, 1.7B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/software-mansion/react-native-executorch-llama-3.2",children:"LLaMA 3.2"})}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1B, 3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u2705"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(n.h3,{id:"model-size",children:"Model size"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"XNNPACK [GB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2.47"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1.14"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1.18"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"6.43"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2.55"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2.65"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"memory-usage",children:"Memory usage"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Android (XNNPACK) [GB]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iOS (XNNPACK) [GB]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"3.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"3.1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"1.9"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"2.5"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"7.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"7.3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"3.7"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"3.8"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"4"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"4.1"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"inference-time",children:"Inference time"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 16 Pro (XNNPACK) [tokens/s]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone 13 Pro (XNNPACK) [tokens/s]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"iPhone SE 3 (XNNPACK) [tokens/s]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"Samsung Galaxy S24 (XNNPACK) [tokens/s]"}),(0,r.jsx)(n.th,{style:{textAlign:"center"},children:"OnePlus 12 (XNNPACK) [tokens/s]"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"16.1"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"11.4"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"15.6"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"19.3"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"40.6"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"16.7"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"16.5"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"40.3"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"48.2"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_1B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"31.8"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"11.4"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"11.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"37.3"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"44.4"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"7.1"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_SPINQUANT"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"17.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"8.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"16.2"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"19.4"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LLAMA3_2_3B_QLORA"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"14.5"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"\u274c"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"14.8"}),(0,r.jsx)(n.td,{style:{textAlign:"center"},children:"18.1"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"\u274c - Insufficient RAM."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);